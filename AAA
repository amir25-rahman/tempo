import polars as pl
from pathlib import Path
import gc
import shutil


def get_null_bucket(null_pct):
    if null_pct == 0:
        return "0"
    elif null_pct < 25:
        return "0-24%"
    elif null_pct < 50:
        return "25-49%"
    elif null_pct < 75:
        return "50-74%"
    elif null_pct < 100:
        return "75-99%"
    else:
        return "100%"


def profile_column(df, column_name, total_rows, prod_data_src, schema, table):
    # Normalize prod_data_src so the column is always string
    prod_data_src = "N/A" if prod_data_src is None else str(prod_data_src)

    series = df[column_name]

    null_count = series.null_count()
    non_null_count = total_rows - null_count
    null_pct = (null_count / total_rows * 100) if total_rows > 0 else 0.0

    # Early out if everything is null
    if non_null_count == 0:
        return {
            "prod_data_src": prod_data_src,
            "schema": schema,
            "table": table,
            "column": column_name,
            "total_rows": int(total_rows),
            "null_count": int(null_count),
            "null_pct": float(round(null_pct, 2)),
            "null_bucket": get_null_bucket(null_pct),
            "distinct_count": 0,
            "unique_count": 0,
            "unique_pct": 0.0,
            "duplicate_count": 0,
            "duplicate_pct": 0.0,
            "special_char_count": 0,
            "most_frequent_value": None,
            "top_5_dups": "No duplicates",
            "min_value": None,
            "max_value": None,
            "most_common_len": None,
            "median_len": None,
            "min_len": None,
            "max_len": None,
        }

    distinct_count = series.n_unique()

    is_numeric = series.dtype in [
        pl.Int8,
        pl.Int16,
        pl.Int32,
        pl.Int64,
        pl.UInt8,
        pl.UInt16,
        pl.UInt32,
        pl.UInt64,
        pl.Float32,
        pl.Float64,
    ]

    if is_numeric:
        try:
            min_value = series.min()
            max_value = series.max()
        except Exception:
            min_value = None
            max_value = None
    else:
        min_value = None
        max_value = None

    non_null_series = series.filter(series.is_not_null())
    str_series = non_null_series.cast(pl.Utf8)

    temp_df = pl.DataFrame(
        {
            "value": non_null_series,
            "str_value": str_series,
            "len": str_series.str.len_chars(),
        }
    )

    grouped = temp_df.group_by("value").agg(
        [
            pl.len().alias("count"),
            pl.col("str_value").first().alias("str_val"),
        ]
    )

    truly_unique_count = grouped.filter(pl.col("count") == 1).height
    unique_pct = (
        (truly_unique_count / non_null_count * 100) if non_null_count > 0 else 0.0
    )

    duplicate_rows = grouped.filter(pl.col("count") > 1)
    if duplicate_rows.height > 0:
        duplicate_count = duplicate_rows.select(pl.col("count").sum()).item()
        duplicate_pct = (
            (duplicate_count / non_null_count * 100) if non_null_count > 0 else 0.0
        )

        duplicate_sorted = duplicate_rows.sort("count", descending=True)
        top_5 = duplicate_sorted.head(5)
        result_parts = [
            f"{row[0]}({row[1]})"
            for row in top_5.select(["value", "count"]).iter_rows()
        ]
        if duplicate_rows.height > 5:
            result_parts.append(f"...{duplicate_rows.height - 5}+ more")
        top_5_dups = " | ".join(result_parts)
    else:
        duplicate_count = 0
        duplicate_pct = 0.0
        top_5_dups = "No duplicates"

    most_frequent_row = grouped.sort("count", descending=True).head(1)
    most_frequent_value = (
        most_frequent_row[0, 0] if most_frequent_row.height > 0 else None
    )

    special_char_count = (
        str_series.map_elements(
            lambda val: any(
                not c.isalnum() for c in (val[1:] if val.startswith("-") else val)
            ),
            return_dtype=pl.Boolean,
        ).sum()
    )

    lengths = temp_df["len"]
    len_value_counts = lengths.value_counts().sort("count", descending=True)
    most_common_len = len_value_counts[0, 0] if len_value_counts.height > 0 else None
    median_raw = lengths.median()
    median_len = int(median_raw) if median_raw is not None else None
    min_len = lengths.min()
    max_len = lengths.max()

    return {
        "prod_data_src": prod_data_src,
        "schema": schema,
        "table": table,
        "column": column_name,
        "total_rows": int(total_rows),
        "null_count": int(null_count),
        "null_pct": float(round(null_pct, 2)),
        "null_bucket": get_null_bucket(null_pct),
        "distinct_count": int(distinct_count),
        "unique_count": int(truly_unique_count),
        "unique_pct": float(round(unique_pct, 2)),
        "duplicate_count": int(duplicate_count),
        "duplicate_pct": float(round(duplicate_pct, 2)),
        "special_char_count": int(special_char_count),
        "most_frequent_value": (
            str(most_frequent_value) if most_frequent_value is not None else None
        ),
        "top_5_dups": top_5_dups,
        "min_value": str(min_value) if min_value is not None else None,
        "max_value": str(max_value) if max_value is not None else None,
        "most_common_len": int(most_common_len) if most_common_len is not None else None,
        "median_len": int(median_len) if median_len is not None else None,
        "min_len": int(min_len) if min_len is not None else None,
        "max_len": int(max_len) if max_len is not None else None,
    }


def parse_filename(filename):
    name = Path(filename).stem
    parts = name.split(".")

    if len(parts) >= 3:
        return parts[-2], parts[-1]
    elif len(parts) == 2:
        return parts[0], parts[1]
    else:
        return "unknown", parts[0]


def profiles_to_df(profiles):
    """
    Build a DataFrame from list of dicts with an explicit schema so
    weird Python type mixes do not confuse the builder.
    """
    schema = {
        "prod_data_src": pl.Utf8,
        "schema": pl.Utf8,
        "table": pl.Utf8,
        "column": pl.Utf8,
        "total_rows": pl.Int64,
        "null_count": pl.Int64,
        "null_pct": pl.Float64,
        "null_bucket": pl.Utf8,
        "distinct_count": pl.Int64,
        "unique_count": pl.Int64,
        "unique_pct": pl.Float64,
        "duplicate_count": pl.Int64,
        "duplicate_pct": pl.Float64,
        "special_char_count": pl.Int64,
        "most_frequent_value": pl.Utf8,
        "top_5_dups": pl.Utf8,
        "min_value": pl.Utf8,
        "max_value": pl.Utf8,
        "most_common_len": pl.Int64,
        "median_len": pl.Int64,
        "min_len": pl.Int64,
        "max_len": pl.Int64,
    }

    # from_dicts with schema will cast values into this shape
    return pl.from_dicts(profiles, schema=schema)


def profile_parquet_files(
    folder_path,
    output_file="data_profile_results.parquet",
    breakdown_column=None,
    return_results_df=False,
):
    folder_path = Path(folder_path)
    parquet_files = list(folder_path.glob("*.parquet"))

    if not parquet_files:
        print("No parquet files found in folder")
        return None

    temp_dir = folder_path / "_temp_profiles"

    # Hard reset temp dir to avoid mixing old incompatible temp files
    if temp_dir.exists():
        print(f"Removing existing temp directory: {temp_dir}")
        shutil.rmtree(temp_dir, ignore_errors=True)

    temp_dir.mkdir(exist_ok=True)
    print(f"Created temp directory: {temp_dir}")

    for file_path in parquet_files:
        try:
            print(f"\nReading {file_path.name}...")
            df = pl.read_parquet(file_path)
            schema, table = parse_filename(file_path.name)

            if breakdown_column is not None and breakdown_column not in df.columns:
                print(
                    f"Skipping {file_path.name}: "
                    f"breakdown column '{breakdown_column}' not found"
                )
                del df
                gc.collect()
                continue

            safe_file = file_path.name.replace(".", "_")

            if breakdown_column is None:
                profiles = []
                total_rows = df.height
                for column in df.columns:
                    try:
                        profile = profile_column(
                            df, column, total_rows, "N/A", schema, table
                        )
                        profiles.append(profile)
                    except Exception as e:
                        print(f"Error profiling column {column}: {e}")

                if profiles:
                    temp_file = temp_dir / f"{safe_file}__NA.parquet"
                    try:
                        profiles_df = profiles_to_df(profiles)
                        profiles_df.write_parquet(temp_file)
                        print(
                            f"Wrote {profiles_df.height} profiles to {temp_file.name}"
                        )
                    except Exception as e:
                        print(
                            f"Error building profile DataFrame for {file_path.name} "
                            f"(no breakdown): {e}"
                        )

                # free per file
                del profiles
                gc.collect()

            else:
                breakdown_values = df[breakdown_column].unique().to_list()
                columns_to_profile = [
                    col for col in df.columns if col != breakdown_column
                ]
                print(
                    f"Found {len(breakdown_values)} breakdown values, "
                    f"{len(columns_to_profile)} columns to profile"
                )

                for i, breakdown_value in enumerate(breakdown_values, 1):
                    profiles = []
                    subset = df.filter(pl.col(breakdown_column) == breakdown_value)
                    total_rows = subset.height

                    for column in columns_to_profile:
                        try:
                            profile = profile_column(
                                subset,
                                column,
                                total_rows,
                                breakdown_value,
                                schema,
                                table,
                            )
                            profiles.append(profile)
                        except Exception as e:
                            print(
                                f"Error profiling column {column} for "
                                f"{breakdown_column}={breakdown_value}: {e}"
                            )

                    if profiles:
                        safe_breakdown = (
                            str(breakdown_value)
                            .replace("/", "_")
                            .replace("\\", "_")
                            .replace(":", "_")
                        )
                        temp_file = temp_dir / f"{safe_file}__{safe_breakdown}.parquet"
                        try:
                            profiles_df = profiles_to_df(profiles)
                            profiles_df.write_parquet(temp_file)
                            print(
                                f"[{i}/{len(breakdown_values)}] "
                                f"Wrote {profiles_df.height} profiles to {temp_file.name}"
                            )
                        except Exception as e:
                            print(
                                f"Error building profile DataFrame for "
                                f"{file_path.name}, {breakdown_column}="
                                f"{breakdown_value}: {e}"
                            )

                    # clear memory after each prod_data_src group
                    del profiles
                    del subset
                    gc.collect()

            del df
            gc.collect()
            print(f"Freed memory for {file_path.name}")

        except Exception as e:
            print(f"Error processing {file_path.name}: {e}")

    # Final merge of all temp profile files
    temp_files = list(temp_dir.glob("*.parquet"))

    if not temp_files:
        print("No profiles generated")
        try:
            temp_dir.rmdir()
        except Exception:
            pass
        return None

    print("\n============================================================")
    print(f"Merging {len(temp_files)} temp files into {output_file} (streaming)...")

    column_order = [
        "prod_data_src",
        "schema",
        "table",
        "column",
        "total_rows",
        "null_count",
        "null_pct",
        "null_bucket",
        "distinct_count",
        "unique_count",
        "unique_pct",
        "duplicate_count",
        "duplicate_pct",
        "special_char_count",
        "most_frequent_value",
        "top_5_dups",
        "min_value",
        "max_value",
        "most_common_len",
        "median_len",
        "min_len",
        "max_len",
    ]

    temp_pattern = str(temp_dir / "*.parquet")

    # Stream to final parquet
    lf = pl.scan_parquet(temp_pattern).select(column_order)
    lf.sink_parquet(output_file)

    # Get row count in a streaming friendly way
    row_count = (
        pl.scan_parquet(temp_pattern)
        .select(pl.count().alias("count"))
        .collect()
        .item()
    )

    print("Cleaning up temp files...")
    for temp_file in temp_files:
        try:
            temp_file.unlink()
        except Exception as e:
            print(f"Could not delete temp file {temp_file}: {e}")

    try:
        temp_dir.rmdir()
    except Exception as e:
        print(f"Could not remove temp directory {temp_dir}: {e}")

    gc.collect()

    print("============================================================")
    print(f"Profiling complete. Results saved to: {output_file}")
    print(f"Total profiles: {row_count:,}")

    if return_results_df:
        print("Loading results DataFrame into memory (may be large)...")
        results_df = pl.read_parquet(output_file)
        return results_df

    return None


if __name__ == "__main__":
    folder_path = "C:/Users/NULL/Desktop/pdf codes/DD/v2"
    breakdown_column = "prod_data_src"

    results = profile_parquet_files(
        folder_path,
        output_file="data_profile_results.parquet",
        breakdown_column=breakdown_column,
        return_results_df=False,
    )










		##########


import polars as pl
from pathlib import Path
import gc


def get_null_bucket(null_pct):
    if null_pct == 0:
        return "0"
    elif null_pct < 25:
        return "0-24%"
    elif null_pct < 50:
        return "25-49%"
    elif null_pct < 75:
        return "50-74%"
    elif null_pct < 100:
        return "75-99%"
    else:
        return "100%"


def profile_column(df, column_name, total_rows, prod_data_src, schema, table):
    # Normalize prod_data_src so the column is always string
    prod_data_src = "N/A" if prod_data_src is None else str(prod_data_src)

    series = df[column_name]

    null_count = series.null_count()
    non_null_count = total_rows - null_count
    null_pct = (null_count / total_rows * 100) if total_rows > 0 else 0

    if non_null_count == 0:
        return {
            "prod_data_src": prod_data_src,
            "schema": schema,
            "table": table,
            "column": column_name,
            "total_rows": total_rows,
            "null_count": null_count,
            "null_pct": round(null_pct, 2),
            "null_bucket": get_null_bucket(null_pct),
            "distinct_count": 0,
            "unique_count": 0,
            "unique_pct": 0.0,
            "duplicate_count": 0,
            "duplicate_pct": 0.0,
            "special_char_count": 0,
            "most_frequent_value": None,
            "top_5_dups": "No duplicates",
            "min_value": None,
            "max_value": None,
            "most_common_len": None,
            "median_len": None,
            "min_len": None,
            "max_len": None,
        }

    distinct_count = series.n_unique()

    is_numeric = series.dtype in [
        pl.Int8,
        pl.Int16,
        pl.Int32,
        pl.Int64,
        pl.UInt8,
        pl.UInt16,
        pl.UInt32,
        pl.UInt64,
        pl.Float32,
        pl.Float64,
    ]

    if is_numeric:
        try:
            min_value = series.min()
            max_value = series.max()
        except Exception:
            min_value = None
            max_value = None
    else:
        min_value = None
        max_value = None

    non_null_series = series.filter(series.is_not_null())
    str_series = non_null_series.cast(pl.Utf8)

    temp_df = pl.DataFrame(
        {
            "value": non_null_series,
            "str_value": str_series,
            "len": str_series.str.len_chars(),
        }
    )

    grouped = temp_df.group_by("value").agg(
        [
            pl.len().alias("count"),
            pl.col("str_value").first().alias("str_val"),
        ]
    )

    truly_unique_count = grouped.filter(pl.col("count") == 1).height
    unique_pct = (truly_unique_count / non_null_count * 100) if non_null_count > 0 else 0.0

    duplicate_rows = grouped.filter(pl.col("count") > 1)
    if duplicate_rows.height > 0:
        duplicate_count = duplicate_rows.select(pl.col("count").sum()).item()
        duplicate_pct = (duplicate_count / non_null_count * 100) if non_null_count > 0 else 0.0

        duplicate_sorted = duplicate_rows.sort("count", descending=True)
        top_5 = duplicate_sorted.head(5)
        result_parts = [
            f"{row[0]}({row[1]})"
            for row in top_5.select(["value", "count"]).iter_rows()
        ]
        if duplicate_rows.height > 5:
            result_parts.append(f"...{duplicate_rows.height - 5}+ more")
        top_5_dups = " | ".join(result_parts)
    else:
        duplicate_count = 0
        duplicate_pct = 0.0
        top_5_dups = "No duplicates"

    most_frequent_row = grouped.sort("count", descending=True).head(1)
    most_frequent_value = (
        most_frequent_row[0, 0] if most_frequent_row.height > 0 else None
    )

    special_char_count = (
        str_series.map_elements(
            lambda val: any(
                not c.isalnum() for c in (val[1:] if val.startswith("-") else val)
            ),
            return_dtype=pl.Boolean,
        ).sum()
    )

    lengths = temp_df["len"]
    len_value_counts = lengths.value_counts().sort("count", descending=True)
    most_common_len = len_value_counts[0, 0] if len_value_counts.height > 0 else None
    median_raw = lengths.median()
    median_len = int(median_raw) if median_raw is not None else None
    min_len = lengths.min()
    max_len = lengths.max()

    return {
        "prod_data_src": prod_data_src,
        "schema": schema,
        "table": table,
        "column": column_name,
        "total_rows": total_rows,
        "null_count": null_count,
        "null_pct": round(null_pct, 2),
        "null_bucket": get_null_bucket(null_pct),
        "distinct_count": distinct_count,
        "unique_count": truly_unique_count,
        "unique_pct": round(unique_pct, 2),
        "duplicate_count": duplicate_count,
        "duplicate_pct": round(duplicate_pct, 2),
        "special_char_count": special_char_count,
        "most_frequent_value": str(most_frequent_value)
        if most_frequent_value is not None
        else None,
        "top_5_dups": top_5_dups,
        "min_value": str(min_value) if min_value is not None else None,
        "max_value": str(max_value) if max_value is not None else None,
        "most_common_len": most_common_len,
        "median_len": median_len,
        "min_len": min_len,
        "max_len": max_len,
    }


def parse_filename(filename):
    name = Path(filename).stem
    parts = name.split(".")

    if len(parts) >= 3:
        return parts[-2], parts[-1]
    elif len(parts) == 2:
        return parts[0], parts[1]
    else:
        return "unknown", parts[0]


def profile_parquet_files(
    folder_path,
    output_file="data_profile_results.parquet",
    breakdown_column=None,
    return_results_df=False,
):
    parquet_files = list(Path(folder_path).glob("*.parquet"))

    if not parquet_files:
        print("No parquet files found in folder")
        return None

    temp_dir = Path(folder_path) / "_temp_profiles"
    temp_dir.mkdir(exist_ok=True)
    print(f"Created temp directory: {temp_dir}")

    for file_path in parquet_files:
        try:
            print(f"\nReading {file_path.name}...")
            df = pl.read_parquet(file_path)
            schema, table = parse_filename(file_path.name)

            if breakdown_column is not None and breakdown_column not in df.columns:
                print(
                    f"Skipping {file_path.name}: breakdown column '{breakdown_column}' not found"
                )
                del df
                gc.collect()
                continue

            safe_file = file_path.name.replace(".", "_")

            if breakdown_column is None:
                profiles = []
                total_rows = df.height
                for column in df.columns:
                    try:
                        profile = profile_column(
                            df, column, total_rows, "N/A", schema, table
                        )
                        profiles.append(profile)
                    except Exception as e:
                        print(f"Error profiling column {column}: {e}")

                if profiles:
                    temp_file = temp_dir / f"{safe_file}__NA.parquet"
                    pl.DataFrame(profiles).write_parquet(temp_file)
                    print(f"Wrote {len(profiles)} profiles to {temp_file.name}")

                # free per file
                del profiles
                gc.collect()

            else:
                breakdown_values = df[breakdown_column].unique().to_list()
                columns_to_profile = [
                    col for col in df.columns if col != breakdown_column
                ]
                print(
                    f"Found {len(breakdown_values)} breakdown values, "
                    f"{len(columns_to_profile)} columns to profile"
                )

                for i, breakdown_value in enumerate(breakdown_values, 1):
                    profiles = []
                    subset = df.filter(pl.col(breakdown_column) == breakdown_value)
                    total_rows = subset.height

                    for column in columns_to_profile:
                        try:
                            profile = profile_column(
                                subset,
                                column,
                                total_rows,
                                breakdown_value,
                                schema,
                                table,
                            )
                            profiles.append(profile)
                        except Exception as e:
                            print(
                                f"Error profiling column {column} for {breakdown_value}: {e}"
                            )

                    if profiles:
                        safe_breakdown = (
                            str(breakdown_value)
                            .replace("/", "_")
                            .replace("\\", "_")
                            .replace(":", "_")
                        )
                        temp_file = temp_dir / f"{safe_file}__{safe_breakdown}.parquet"
                        pl.DataFrame(profiles).write_parquet(temp_file)
                        print(
                            f"[{i}/{len(breakdown_values)}] "
                            f"Wrote {len(profiles)} profiles to {temp_file.name}"
                        )

                    # clear memory after each prod_data_src group
                    del profiles
                    del subset
                    gc.collect()

            del df
            gc.collect()
            print(f"Freed memory for {file_path.name}")

        except Exception as e:
            print(f"Error processing {file_path.name}: {e}")

    # Final merge of all temp profile files
    temp_files = list(temp_dir.glob("*.parquet"))

    if not temp_files:
        print("No profiles generated")
        # cleanup temp dir even if empty
        try:
            temp_dir.rmdir()
        except Exception:
            pass
        return None

    print("\n============================================================")
    print(f"Merging {len(temp_files)} temp files into {output_file} (streaming)...")

    column_order = [
        "prod_data_src",
        "schema",
        "table",
        "column",
        "total_rows",
        "null_count",
        "null_pct",
        "null_bucket",
        "distinct_count",
        "unique_count",
        "unique_pct",
        "duplicate_count",
        "duplicate_pct",
        "special_char_count",
        "most_frequent_value",
        "top_5_dups",
        "min_value",
        "max_value",
        "most_common_len",
        "median_len",
        "min_len",
        "max_len",
    ]

    # Build a lazy scan over all temp parquet files and stream to final parquet
    temp_pattern = str(temp_dir / "*.parquet")
    lf = pl.scan_parquet(temp_pattern).select(column_order)
    lf.sink_parquet(output_file)

    # Get total row count from the same temp pattern with an aggregate only
    row_count = (
        pl.scan_parquet(temp_pattern)
        .select(pl.count().alias("count"))
        .collect()
        .item()
    )

    print("Cleaning up temp files...")
    for temp_file in temp_files:
        try:
            temp_file.unlink()
        except Exception as e:
            print(f"Could not delete temp file {temp_file}: {e}")

    try:
        temp_dir.rmdir()
    except Exception as e:
        print(f"Could not remove temp directory {temp_dir}: {e}")

    gc.collect()

    print("============================================================")
    print(f"Profiling complete. Results saved to: {output_file}")
    print(f"Total profiles: {row_count:,}")

    # By default do not load the huge result back into memory
    if return_results_df:
        print("Loading results DataFrame into memory (may be large)...")
        results_df = pl.read_parquet(output_file)
        return results_df

    return None


if __name__ == "__main__":
    folder_path = "C:/Users/NULL/Desktop/pdf codes/DD/v2"
    breakdown_column = "prod_data_src"

    # Keep return_results_df=False to avoid loading everything into RAM
    results = profile_parquet_files(
        folder_path,
        output_file="data_profile_results.parquet",
        breakdown_column=breakdown_column,
        return_results_df=False,
    )










#################

The error suggests there's an issue with the filtering operation. Here's the corrected version that handles potential indexing errors:

```python
import pandas as pd
import numpy as np

def fastest_dynamic_merge(d1, d2):
    # Standardize all columns to lowercase
    d1 = d1.copy()
    d2 = d2.copy()
    
    # Map d1 columns (adjust these to your actual column names)
    d1_cols = ['schema', 'table', 'sys', 'col']  # Replace with actual d1 column names
    d2_cols = ['schema', 'table_label', 'sys', 'col']  # Replace with actual d2 column names
    
    for col in d1_cols:
        if col in d1.columns:
            d1[col] = d1[col].astype(str).str.lower()
    
    for col in d2_cols:
        if col in d2.columns:
            d2[col] = d2[col].astype(str).str.lower()
    
    # Create lookup list for O(1) access
    d1_sys_list = d1[d1_cols[2]].tolist()  # sys column
    
    # Optimized matching using list comprehension with error handling
    matched_sys_list = []
    for row in d2[d2_cols[2]]:
        match = None
        for sys_val in d1_sys_list:
            if sys_val in str(row):
                match = sys_val
                break
        matched_sys_list.append(match)
    
    d2['matched_sys'] = matched_sys_list
    
    # Filter using boolean indexing to avoid indexing errors
    mask = d2['matched_sys'].notna()
    d2_matched = d2.loc[mask].copy()
    
    if len(d2_matched) == 0:
        # If no matches, return d1 with all 'N' indicators
        d1['match_indicator'] = 'N'
        return d1
    
    # Create primary keys using mapped columns
    d1['pk'] = d1[d1_cols[0]] + '|' + d1[d1_cols[1]] + '|' + d1[d1_cols[2]] + '|' + d1[d1_cols[3]]
    d2_matched['pk'] = d2_matched[d2_cols[0]] + '|' + d2_matched[d2_cols[1]] + '|' + d2_matched['matched_sys'] + '|' + d2_matched[d2_cols[3]]
    
    # Fast merge
    result = d1.merge(d2_matched, on='pk', how='left', suffixes=('', '_d2'))
    result['match_indicator'] = np.where(result[f'{d2_cols[2]}_d2'].notna(), 'Y', 'N')
    
    return result

# Execute
result = fastest_dynamic_merge(d1, d2)
```

The key changes:
1. Added column existence checks
2. Replaced `dropna()` with explicit boolean masking using `.loc[mask]`
3. Added a check for empty matches
4. Used explicit loop instead of list comprehension for better error handling

Update the column names in `d1_cols` and `d2_cols` to match your actual dataframe column names.​​​​​​​​​​​​​​​​















```python
def fastest_dynamic_merge(d1, d2):
    # Standardize all columns to lowercase
    d1 = d1.copy()
    d2 = d2.copy()
    
    # Map d1 columns (adjust these to your actual column names)
    d1_cols = ['schema', 'table', 'sys', 'col']  # Replace with actual d1 column names
    d2_cols = ['schema', 'table_label', 'sys', 'col']  # Replace with actual d2 column names
    
    for col in d1_cols:
        d1[col] = d1[col].astype(str).str.lower()
    
    for col in d2_cols:
        d2[col] = d2[col].astype(str).str.lower()
    
    # Create lookup list for O(1) access
    d1_sys_list = d1[d1_cols[2]].tolist()  # sys column
    
    # Optimized matching using list comprehension
    d2['matched_sys'] = [
        next((sys_val for sys_val in d1_sys_list if sys_val in str(row)), None)
        for row in d2[d2_cols[2]]  # sys column
    ]
    
    # Filter and create keys
    d2_matched = d2.dropna(subset=['matched_sys'])
    
    # Create primary keys using mapped columns
    d1['pk'] = d1[d1_cols[0]] + '|' + d1[d1_cols[1]] + '|' + d1[d1_cols[2]] + '|' + d1[d1_cols[3]]
    d2_matched['pk'] = d2_matched[d2_cols[0]] + '|' + d2_matched[d2_cols[1]] + '|' + d2_matched['matched_sys'] + '|' + d2_matched[d2_cols[3]]
    
    # Fast merge
    result = d1.merge(d2_matched, on='pk', how='left', suffixes=('', '_d2'))
    result['match_indicator'] = np.where(result[f'{d2_cols[2]}_d2'].notna(), 'Y', 'N')
    
    return result

# Execute - just update the column name lists above
result = fastest_dynamic_merge(d1, d2)
```

Just replace the column names in `d1_cols` and `d2_cols` lists with your actual column names. For example:
- `d1_cols = ['schema_name', 'table_name', 'system', 'column_name']`
- `d2_cols = ['schema_id', 'table_label', 'system_code', 'col_name']`









Showkou = 
VAR SelectedSSN = SELECTEDVALUE('Combined Analysis'[SoR Name])
VAR RelatedTable = RELATED('map'[table])

RETURN
IF (
    ISBLANK(SelectedSSN) || ISBLANK(RelatedTable),
    0,
    CALCULATE (
        COUNTROWS(Sheet1),
        FILTER (
            Sheet1,
            CONTAINSSTRING(Sheet1[filter criteria], SelectedSSN)
            && Sheet1[table] = RelatedTable
        )
    )
)






Showkou = 
VAR SelectedSSN = SELECTEDVALUE('Combined Analysis'[SoR Name])
VAR SelectedTable = SELECTEDVALUE('Combined Analysis'[map[table]])

RETURN
IF (
    ISBLANK(SelectedSSN) || ISBLANK(SelectedTable),
    0,
    CALCULATE (
        COUNTROWS(Sheet1),
        FILTER (
            Sheet1,
            CONTAINSSTRING(Sheet1[filter criteria], SelectedSSN)
            && Sheet1[table] = SelectedTable
        )
    )
)







ShowRow =
IF (
    NOT ISBLANK(Sheet1[Column1]) &&
    CALCULATE (
        COUNTROWS (
            FILTER (
                Sheet2,
                CONTAINSSTRING(Sheet2[Column1], Sheet1[Column1]) &&
                Sheet2[Table Name] IN VALUES(Map[Table Name])
            )
        )
    ) > 0,
    1,
    0
)





ShowRow =
VAR CurrentRowValue = Sheet1[Column1]
RETURN
IF (
    NOT ISBLANK(CurrentRowValue) &&
    CALCULATE(
        COUNTROWS(
            FILTER(Sheet2, CONTAINSSTRING(Sheet2[Column1], CurrentRowValue))
        )
    ) > 0,
    1,
    0
)






ShowRow = 
VAR SelectedSysName = SELECTEDVALUE(Sheet1[Column1])
RETURN
IF (
    NOT ISBLANK(SelectedSysName) &&
    CONTAINSSTRING(MAX(Sheet2[Column1]), SelectedSysName),
    1,
    0
)


import re

text = "T2_1dbg_7"
match = re.search(r'^[^_]*_((?:.*?))(?=_[0-9])', text)
if match:
    print(match.group(1))  # Output: 1dbg




special_char_pattern = r'[^a-zA-Z0-9\s.,:;!?@#\$%\^\&\*\(\)_\-\+=]'


import pandas as pd
import numpy as np
from openpyxl import Workbook
from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
from openpyxl.utils import get_column_letter
import warnings
import re
from datetime import datetime

warnings.filterwarnings('ignore')

class SimpleDatasetAnalyzer:
    def __init__(self, data_path):
        """Simple dataset analyzer"""
        if isinstance(data_path, str):
            self.df = pd.read_csv(data_path)
        else:
            self.df = data_path
        print(f"Loaded dataset: {len(self.df)} rows, {len(self.df.columns)} columns")

    def analyze_and_export(self, output_path='dataset_summary.xlsx'):
        """Create a simple one-page Excel summary"""
        print("Analyzing dataset...")

        wb = Workbook()
        ws = wb.active
        ws.title = "Dataset Summary"

        header_font = Font(bold=True, color='FFFFFF', size=12)
        header_fill = PatternFill(start_color='0F243E', end_color='0F243E', fill_type='solid')
        subheader_font = Font(bold=True, size=11)
        border = Border(left=Side(style='thin'), right=Side(style='thin'),
                        top=Side(style='thin'), bottom=Side(style='thin'))

        ws['A1'] = "DATASET SUMMARY REPORT"
        ws['A1'].font = Font(bold=True, size=16)
        ws.merge_cells('A1:I1')
        ws['A1'].alignment = Alignment(horizontal='center')

        current_row = 3

        # BASIC INFO
        ws[f'A{current_row}'] = "BASIC INFORMATION"
        ws[f'A{current_row}'].font = subheader_font
        current_row += 1

        non_null_columns = self.df.columns[self.df.notnull().all()].tolist()
        most_missing_col = self.df.isnull().sum().idxmax()
        most_missing_count = self.df[most_missing_col].isnull().sum()

        basic_info = [
            ['Metric', 'Value'],
            ['Total Rows', f"{len(self.df):,}"],
            ['Total Columns', len(self.df.columns)],
            ['Duplicate Rows', f"{self.df.duplicated().sum():,}"],
            ['Complete Rows (No Nulls)', f"{self.df.dropna().shape[0]:,}"],
            ['Columns with No Nulls', len(non_null_columns)],
            ['Column with Most Missing Values', f"{most_missing_col} ({most_missing_count:,} missing)"]
        ]

        for i, (metric, value) in enumerate(basic_info):
            ws[f'A{current_row}'] = metric
            ws[f'B{current_row}'] = value
            if i == 0:
                ws[f'A{current_row}'].font = header_font
                ws[f'B{current_row}'].font = header_font
                ws[f'A{current_row}'].fill = header_fill
                ws[f'B{current_row}'].fill = header_fill
            ws[f'A{current_row}'].border = border
            ws[f'B{current_row}'].border = border
            current_row += 1

        current_row += 1

        # COLUMN ANALYSIS
        ws[f'A{current_row}'] = "COLUMN ANALYSIS"
        ws[f'A{current_row}'].font = subheader_font
        current_row += 1

        special_char_pattern = re.compile(r'[^\w\s]')

        column_data = [['Column', 'Data Type', 'Null Count', 'Null %', 'Unique %', 'Duplicated %', 'Special Char Count', 'Sample Value']]

        for col in self.df.columns:
            col_series = self.df[col]
            null_count = col_series.isnull().sum()
            null_pct = (null_count / len(self.df) * 100)
            non_null_series = col_series.dropna()
            total_vals = len(non_null_series)
            unique_vals = non_null_series.nunique()
            duplicated_vals = total_vals - unique_vals
            unique_pct = (unique_vals / total_vals * 100) if total_vals > 0 else 0
            duplicated_pct = (duplicated_vals / total_vals * 100) if total_vals > 0 else 0
            special_char_count = col_series.astype(str).apply(lambda x: bool(special_char_pattern.search(x))).sum()
            sample_val = non_null_series.iloc[0] if not non_null_series.empty else 'N/A'
            if isinstance(sample_val, str) and len(str(sample_val)) > 20:
                sample_val = str(sample_val)[:20] + "..."
            column_data.append([
                col,
                str(col_series.dtype),
                f"{null_count:,}",
                f"{null_pct:.1f}%",
                f"{unique_pct:.1f}%",
                f"{duplicated_pct:.1f}%",
                f"{special_char_count:,}",
                str(sample_val)
            ])

        for i, row_data in enumerate(column_data):
            for j, value in enumerate(row_data):
                cell = ws.cell(row=current_row, column=j+1, value=value)
                if i == 0:
                    cell.font = header_font
                    cell.fill = header_fill
                cell.border = border
            current_row += 1

        wb.save(output_path)
        print(f"Summary saved to: {output_path}")





















date_cols = [col for col in self.df.columns 
             if re.search(r'(^|_)(date|dt|created|updated|modified)($|_)', col.lower())]






WeekLabel =
VAR CurrentDate = 'YourTable'[Date]
VAR StartDate = CurrentDate - WEEKDAY(CurrentDate, 2) + 1
VAR EndDate = StartDate + 6
RETURN FORMAT(StartDate, "mmm d") & " - " & FORMAT(EndDate, "mmm d")

info()
nunique()
isnull().sum()
duplicated().sum()
describe()


import pandas as pd
import numpy as np
from openpyxl import Workbook
from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
from openpyxl.utils import get_column_letter
import warnings
import re
from datetime import datetime

warnings.filterwarnings('ignore')

class SimpleDatasetAnalyzer:
    def __init__(self, data_path):
        """Simple dataset analyzer"""
        if isinstance(data_path, str):
            self.df = pd.read_csv(data_path)
        else:
            self.df = data_path
        print(f"Loaded dataset: {len(self.df)} rows, {len(self.df.columns)} columns")

    def analyze_and_export(self, output_path='dataset_summary.xlsx'):
        """Create a simple one-page Excel summary"""
        print("Analyzing dataset...")

        wb = Workbook()
        ws = wb.active
        ws.title = "Dataset Summary"

        header_font = Font(bold=True, color='FFFFFF', size=12)
        header_fill = PatternFill(start_color='0F243E', end_color='0F243E', fill_type='solid')
        subheader_font = Font(bold=True, size=11)
        border = Border(left=Side(style='thin'), right=Side(style='thin'),
                        top=Side(style='thin'), bottom=Side(style='thin'))

        ws['A1'] = "DATASET SUMMARY REPORT"
        ws['A1'].font = Font(bold=True, size=16)
        ws.merge_cells('A1:E1')
        ws['A1'].alignment = Alignment(horizontal='center')

        current_row = 3

        # BASIC INFO
        ws[f'A{current_row}'] = "BASIC INFORMATION"
        ws[f'A{current_row}'].font = subheader_font
        current_row += 1

        non_null_columns = self.df.columns[self.df.notnull().all()].tolist()
        most_missing_col = self.df.isnull().sum().idxmax()
        most_missing_count = self.df[most_missing_col].isnull().sum()

        basic_info = [
            ['Metric', 'Value'],
            ['Total Rows', f"{len(self.df):,}"],
            ['Total Columns', len(self.df.columns)],
            ['Duplicate Rows', f"{self.df.duplicated().sum():,}"],
            ['Complete Rows (No Nulls)', f"{self.df.dropna().shape[0]:,}"],
            ['Columns with No Nulls', len(non_null_columns)],
            ['Column with Most Missing Values', f"{most_missing_col} ({most_missing_count:,} missing)"]
        ]

        for i, (metric, value) in enumerate(basic_info):
            ws[f'A{current_row}'] = metric
            ws[f'B{current_row}'] = value
            if i == 0:
                ws[f'A{current_row}'].font = header_font
                ws[f'B{current_row}'].font = header_font
                ws[f'A{current_row}'].fill = header_fill
                ws[f'B{current_row}'].fill = header_fill
            ws[f'A{current_row}'].border = border
            ws[f'B{current_row}'].border = border
            current_row += 1

        current_row += 1

        # COLUMN ANALYSIS
        ws[f'A{current_row}'] = "COLUMN ANALYSIS"
        ws[f'A{current_row}'].font = subheader_font
        current_row += 1

        # Add "Duplicated Values" to header
        column_data = [['Column', 'Data Type', 'Null Count', 'Null %', 'Unique Values', 'Duplicated Values', 'Sample Value']]

        for col in self.df.columns:
            null_count = self.df[col].isnull().sum()
            null_pct = (null_count / len(self.df) * 100)
            unique_count = self.df[col].nunique()
            duplicated_values_count = len(self.df[col]) - unique_count  # duplicated count
            sample_val = self.df[col].dropna().iloc[0] if not self.df[col].dropna().empty else 'N/A'
            if isinstance(sample_val, str) and len(str(sample_val)) > 20:
                sample_val = str(sample_val)[:20] + "..."
            column_data.append([
                col,
                str(self.df[col].dtype),
                f"{null_count:,}",
                f"{null_pct:.1f}%",
                f"{unique_count:,}",
                f"{duplicated_values_count:,}",
                str(sample_val)
            ])

        for i, row_data in enumerate(column_data):
            for j, value in enumerate(row_data):
                cell = ws.cell(row=current_row, column=j+1, value=value)
                if i == 0:
                    cell.font = header_font
                    cell.fill = header_fill
                cell.border = border
            current_row += 1

        current_row += 1

        # ID FIELD ANALYSIS
        id_cols = [col for col in self.df.columns if re.search(r'\b(id|identifier|account|user|client)\b', col.lower()) or 'id' in col.lower()]

        if id_cols:
            ws[f'A{current_row}'] = "ID FIELD ANALYSIS"
            ws[f'A{current_row}'].font = subheader_font
            current_row += 1

            id_data = [['Column', 'Total Values', 'Most Common Len', 'Common Len Count', 'Min Len', 'Max Len', 'Duplicated IDs']]

            for col in id_cols:
                col_str = self.df[col].astype(str)
                lengths = col_str.str.len()
                most_common_len = lengths.mode().iloc[0] if not lengths.mode().empty else 'N/A'
                common_len_count = (lengths == most_common_len).sum() if most_common_len != 'N/A' else 'N/A'
                duplicated_count = self.df[col].duplicated().sum()

                id_data.append([
                    col,
                    f"{len(self.df):,}",
                    most_common_len,
                    f"{common_len_count:,}" if common_len_count != 'N/A' else 'N/A',
                    lengths.min(),
                    lengths.max(),
                    f"{duplicated_count:,}"
                ])

            for i, row_data in enumerate(id_data):
                for j, value in enumerate(row_data):
                    cell = ws.cell(row=current_row, column=j+1, value=value)
                    if i == 0:
                        cell.font = header_font
                        cell.fill = header_fill
                    cell.border = border
                current_row += 1

            current_row += 1

        # DATE FIELD ANALYSIS
        date_cols = [col for col in self.df.columns if re.search(r'\b(date|dt|created|updated|modified)\b', col.lower())]

        if date_cols:
            ws[f'A{current_row}'] = "DATE FIELD ANALYSIS"
            ws[f'A{current_row}'].font = subheader_font
            current_row += 1

            date_data = [['Column', 'Valid Dates', 'Earliest Date', 'Latest Date', 'Date Range (Days)', 'Most Common Format', 'Invalid Dates']]

            for col in date_cols:
                # Try to parse dates
                try:
                    # Parse dates and extract date component only
                    parsed_dates = pd.to_datetime(self.df[col], errors='coerce').dt.date
                    valid_dates = pd.Series(parsed_dates).dropna()
                    
                    if len(valid_dates) > 0:
                        earliest = str(valid_dates.min())
                        latest = str(valid_dates.max())
                        date_range = (valid_dates.max() - valid_dates.min()).days
                        valid_count = len(valid_dates)
                        invalid_count = len(self.df[col].dropna()) - valid_count
                        
                        # Try to identify the most common date format
                        sample_values = self.df[col].dropna().head(10).astype(str)
                        format_guess = self._guess_date_format(sample_values)
                        
                    else:
                        earliest = 'No valid dates'
                        latest = 'No valid dates'
                        date_range = 'N/A'
                        valid_count = 0
                        invalid_count = len(self.df[col].dropna())
                        format_guess = 'Unknown'

                except Exception:
                    # If parsing completely fails
                    valid_count = 0
                    invalid_count = len(self.df[col].dropna())
                    earliest = 'Parse failed'
                    latest = 'Parse failed'
                    date_range = 'N/A'
                    format_guess = 'Unknown'

                date_data.append([
                    col,
                    f"{valid_count:,}",
                    earliest,
                    latest,
                    f"{date_range:,}" if isinstance(date_range, int) else date_range,
                    format_guess,
                    f"{invalid_count:,}"
                ])

            for i, row_data in enumerate(date_data):
                for j, value in enumerate(row_data):
                    cell = ws.cell(row=current_row, column=j+1, value=value)
                    if i == 0:
                        cell.font = header_font
                        cell.fill = header_fill
                    cell.border = border
                current_row += 1

            current_row += 1

        # NUMERICAL SUMMARY (Exclude ID-like columns)
        numeric_cols = [col for col in self.df.select_dtypes(include=[np.number]).columns if col not in id_cols]
        if len(numeric_cols) > 0:
            ws[f'A{current_row}'] = "NUMERICAL COLUMNS SUMMARY"
            ws[f'A{current_row}'].font = subheader_font
            current_row += 1

            stats_data = [['Column', 'Min', 'Max', 'Mean', 'Std Dev', 'Zeros']]
            for col in numeric_cols:
                col_data = self.df[col].dropna()
                if len(col_data) > 0:
                    stats_data.append([
                        col,
                        f"{col_data.min():.2f}",
                        f"{col_data.max():.2f}",
                        f"{col_data.mean():.2f}",
                        f"{col_data.std():.2f}",
                        f"{(col_data == 0).sum():,}"
                    ])

            for i, row_data in enumerate(stats_data):
                for j, value in enumerate(row_data):
                    cell = ws.cell(row=current_row, column=j+1, value=value)
                    if i == 0:
                        cell.font = header_font
                        cell.fill = header_fill
                    cell.border = border
                current_row += 1

            current_row += 1
        

        # DATA QUALITY SCORE
        ws[f'A{current_row}'] = "DATA QUALITY SCORE"
        ws[f'A{current_row}'].font = subheader_font
        current_row += 1

        total_cells = len(self.df) * len(self.df.columns)
        missing_cells = self.df.isnull().sum().sum()
        completeness = ((total_cells - missing_cells) / total_cells) * 100
        uniqueness = 100 - (self.df.duplicated().sum() / len(self.df) * 100)

        quality_data = [
            ['Quality Metric', 'Score', 'Status'],
            ['Data Completeness(Null Adj)', f"{completeness:.1f}%", 'Good' if completeness > 90 else 'Fair' if completeness > 70 else 'Poor'],
            ['Data Uniqueness(Dup Adj)', f"{uniqueness:.1f}%", 'Good' if uniqueness > 95 else 'Fair' if uniqueness > 80 else 'Poor']
        ]

        for i, row_data in enumerate(quality_data):
            for j, value in enumerate(row_data):
                cell = ws.cell(row=current_row, column=j+1, value=value)
                if i == 0:
                    cell.font = header_font
                    cell.fill = header_fill
                elif j == 2 and i > 0:
                    if value == 'Good':
                        cell.fill = PatternFill(start_color='90EE90', end_color='90EE90', fill_type='solid')
                    elif value == 'Fair':
                        cell.fill = PatternFill(start_color='FFD700', end_color='FFD700', fill_type='solid')
                    else:
                        cell.fill = PatternFill(start_color='FFB6C1', end_color='FFB6C1', fill_type='solid')
                cell.border = border
            current_row += 1

        for col_num in range(1, 8):  # Updated to 8 columns to accommodate date analysis
            column_letter = get_column_letter(col_num)
            max_length = 0
            for row in ws.iter_rows(min_col=col_num, max_col=col_num):
                for cell in row:
                    if cell.value:
                        max_length = max(max_length, len(str(cell.value)))
            ws.column_dimensions[column_letter].width = min(max_length + 2, 25)

        wb.save(output_path)
        print(f"Summary saved to: {output_path}")

    def _guess_date_format(self, sample_values):
        """Helper method to guess the most common date format"""
        formats = []
        
        for value in sample_values:
            value_str = str(value).strip()
            
            # Common date format patterns (excluding time)
            if re.match(r'\d{4}-\d{2}-\d{2}', value_str):
                formats.append('YYYY-MM-DD')
            elif re.match(r'\d{2}/\d{2}/\d{4}', value_str):
                formats.append('MM/DD/YYYY')
            elif re.match(r'\d{2}-\d{2}-\d{4}', value_str):
                formats.append('MM-DD-YYYY')
            elif re.match(r'\d{4}/\d{2}/\d{2}', value_str):
                formats.append('YYYY/MM/DD')
            elif re.match(r'\d{1,2}/\d{1,2}/\d{4}', value_str):
                formats.append('M/D/YYYY')
            elif re.match(r'\d{2}\.\d{2}\.\d{4}', value_str):
                formats.append('DD.MM.YYYY')
            elif re.match(r'\d{8}', value_str):
                formats.append('YYYYMMDD')
            else:
                formats.append('Mixed/Unknown')
        
        # Return most common format
        if formats:
            return max(set(formats), key=formats.count)
        else:
            return 'Unknown'



if __name__ == "__main__":
    # Change path to your CSV file location
    analyzer = SimpleDatasetAnalyzer('intro_stage_raw.csv')
    analyzer.analyze_and_export('mydata.xlsx')
