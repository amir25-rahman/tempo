import polars as pl
from pathlib import Path
import gc
import shutil


def get_null_bucket(null_pct):
    if null_pct == 0:
        return "0"
    elif null_pct < 25:
        return "0-24%"
    elif null_pct < 50:
        return "25-49%"
    elif null_pct < 75:
        return "50-74%"
    elif null_pct < 100:
        return "75-99%"
    else:
        return "100%"


def profile_series(series, total_rows, prod_data_src, schema, table, column_name, 
                   prod_typ=None, prod_subtp=None, is_prod_subtp_level=False):
    """Extended to include prod_typ and prod_subtp for hierarchical analysis
    is_prod_subtp_level: When True, populate prod_subtp_* length columns instead of regular length columns
    """
    # Normalize prod_data_src so the column is always string
    prod_data_src = "N/A" if prod_data_src is None else str(prod_data_src)
    prod_typ = "N/A" if prod_typ is None else str(prod_typ)
    prod_subtp = "N/A" if prod_subtp is None else str(prod_subtp)

    null_count = series.null_count()
    non_null_count = total_rows - null_count
    null_pct = (null_count / total_rows * 100) if total_rows > 0 else 0.0

    # Early out if everything is null
    if non_null_count == 0:
        result = {
            "prod_data_src": prod_data_src,
            "prod_typ": prod_typ,
            "prod_subtp": prod_subtp,
            "schema": schema,
            "table": table,
            "column": column_name,
            "total_rows": int(total_rows),
            "null_count": int(null_count),
            "null_pct": float(round(null_pct, 2)),
            "null_bucket": get_null_bucket(null_pct),
            "distinct_count": 0,
            "unique_count": 0,
            "unique_pct": 0.0,
            "duplicate_count": 0,
            "duplicate_pct": 0.0,
            "special_char_count": 0,
            "most_frequent_value": None,
            "top_5_dups": "No duplicates",
            "min_value": None,
            "max_value": None,
            "most_common_len": None,
            "median_len": None,
            "min_len": None,
            "max_len": None,
            "prod_subtp_most_common_len": None,
            "prod_subtp_median_len": None,
            "prod_subtp_min_len": None,
            "prod_subtp_max_len": None,
        }
        return result

    # Distinct is on the full series including nulls
    distinct_count = series.n_unique()

    is_numeric = series.dtype in [
        pl.Int8, pl.Int16, pl.Int32, pl.Int64,
        pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64,
        pl.Float32, pl.Float64,
    ]

    if is_numeric:
        try:
            min_value = series.min()
            max_value = series.max()
        except Exception:
            min_value = None
            max_value = None
    else:
        min_value = None
        max_value = None

    # Work only on non null values for string based stats
    non_null_series = series.drop_nulls()
    if non_null_series.len() == 0:
        non_null_count = 0
    else:
        non_null_count = non_null_series.len()

    # String representation for grouping, length, special char
    str_series = non_null_series.cast(pl.Utf8)

    # Group by value on the string representation
    grouped = str_series.to_frame("val").group_by("val").agg(
        pl.len().alias("count")
    )

    truly_unique_count = grouped.filter(pl.col("count") == 1).height
    unique_pct = (
        (truly_unique_count / non_null_count * 100) if non_null_count > 0 else 0.0
    )

    duplicate_rows = grouped.filter(pl.col("count") > 1)
    if duplicate_rows.height > 0:
        duplicate_count = duplicate_rows.select(pl.col("count").sum()).item()
        duplicate_pct = (
            (duplicate_count / non_null_count * 100) if non_null_count > 0 else 0.0
        )

        duplicate_sorted = duplicate_rows.sort("count", descending=True)
        top_5 = duplicate_sorted.head(5)
        result_parts = [
            f"{row[0]}({row[1]})"
            for row in top_5.select(["val", "count"]).iter_rows()
        ]
        if duplicate_rows.height > 5:
            result_parts.append(f"...{duplicate_rows.height - 5}+ more")
        top_5_dups = " | ".join(result_parts)
    else:
        duplicate_count = 0
        duplicate_pct = 0.0
        top_5_dups = "No duplicates"

    most_frequent_row = grouped.sort("count", descending=True).head(1)
    most_frequent_value = (
        most_frequent_row[0, 0] if most_frequent_row.height > 0 else None
    )

    # Special character count
    special_char_count = (
        str_series.map_elements(
            lambda val: any(
                not c.isalnum() for c in (val[1:] if val.startswith("-") else val)
            ),
            return_dtype=pl.Boolean,
        ).sum()
    )

    # Length based stats
    lengths = str_series.str.len_chars()
    len_value_counts = lengths.value_counts().sort("count", descending=True)
    most_common_len_val = len_value_counts[0, 0] if len_value_counts.height > 0 else None
    median_raw = lengths.median()
    median_len_val = int(median_raw) if median_raw is not None else None
    min_len_val = lengths.min()
    max_len_val = lengths.max()

    # Assign to appropriate column set based on level
    if is_prod_subtp_level:
        # This is prod_subtp level analysis - populate prod_subtp_* columns
        most_common_len = None
        median_len = None
        min_len = None
        max_len = None
        prod_subtp_most_common_len = int(most_common_len_val) if most_common_len_val is not None else None
        prod_subtp_median_len = int(median_len_val) if median_len_val is not None else None
        prod_subtp_min_len = int(min_len_val) if min_len_val is not None else None
        prod_subtp_max_len = int(max_len_val) if max_len_val is not None else None
    else:
        # This is regular level analysis - populate regular length columns
        most_common_len = int(most_common_len_val) if most_common_len_val is not None else None
        median_len = int(median_len_val) if median_len_val is not None else None
        min_len = int(min_len_val) if min_len_val is not None else None
        max_len = int(max_len_val) if max_len_val is not None else None
        prod_subtp_most_common_len = None
        prod_subtp_median_len = None
        prod_subtp_min_len = None
        prod_subtp_max_len = None

    # Free some intermediate objects explicitly
    del grouped
    del duplicate_rows
    del lengths
    gc.collect()

    return {
        "prod_data_src": prod_data_src,
        "prod_typ": prod_typ,
        "prod_subtp": prod_subtp,
        "schema": schema,
        "table": table,
        "column": column_name,
        "total_rows": int(total_rows),
        "null_count": int(null_count),
        "null_pct": float(round(null_pct, 2)),
        "null_bucket": get_null_bucket(null_pct),
        "distinct_count": int(distinct_count),
        "unique_count": int(truly_unique_count),
        "unique_pct": float(round(unique_pct, 2)),
        "duplicate_count": int(duplicate_count),
        "duplicate_pct": float(round(duplicate_pct, 2)),
        "special_char_count": int(special_char_count),
        "most_frequent_value": (
            str(most_frequent_value) if most_frequent_value is not None else None
        ),
        "top_5_dups": top_5_dups,
        "min_value": str(min_value) if min_value is not None else None,
        "max_value": str(max_value) if max_value is not None else None,
        "most_common_len": most_common_len,
        "median_len": median_len,
        "min_len": min_len,
        "max_len": max_len,
        "prod_subtp_most_common_len": prod_subtp_most_common_len,
        "prod_subtp_median_len": prod_subtp_median_len,
        "prod_subtp_min_len": prod_subtp_min_len,
        "prod_subtp_max_len": prod_subtp_max_len,
    }


def parse_filename(filename):
    name = Path(filename).stem
    parts = name.split(".")

    if len(parts) >= 3:
        return parts[-2], parts[-1]
    elif len(parts) == 2:
        return parts[0], parts[1]
    else:
        return "unknown", parts[0]


def profiles_to_df(profiles):
    """Extended schema to include prod_typ, prod_subtp, and prod_subtp_* length columns"""
    schema = {
        "prod_data_src": pl.Utf8,
        "prod_typ": pl.Utf8,
        "prod_subtp": pl.Utf8,
        "schema": pl.Utf8,
        "table": pl.Utf8,
        "column": pl.Utf8,
        "total_rows": pl.Int64,
        "null_count": pl.Int64,
        "null_pct": pl.Float64,
        "null_bucket": pl.Utf8,
        "distinct_count": pl.Int64,
        "unique_count": pl.Int64,
        "unique_pct": pl.Float64,
        "duplicate_count": pl.Int64,
        "duplicate_pct": pl.Float64,
        "special_char_count": pl.Int64,
        "most_frequent_value": pl.Utf8,
        "top_5_dups": pl.Utf8,
        "min_value": pl.Utf8,
        "max_value": pl.Utf8,
        "most_common_len": pl.Int64,
        "median_len": pl.Int64,
        "min_len": pl.Int64,
        "max_len": pl.Int64,
        "prod_subtp_most_common_len": pl.Int64,
        "prod_subtp_median_len": pl.Int64,
        "prod_subtp_min_len": pl.Int64,
        "prod_subtp_max_len": pl.Int64,
    }
    return pl.from_dicts(profiles, schema=schema)


def profile_parquet_files(
    folder_path,
    output_file="data_profile_results.parquet",
    breakdown_column=None,
    return_results_df=False,
):
    folder_path = Path(folder_path)
    parquet_files = list(folder_path.glob("*.parquet"))

    if not parquet_files:
        print("No parquet files found in folder")
        return None

    temp_dir = folder_path / "_temp_profiles"

    # Reset temp dir to avoid mixing old data
    if temp_dir.exists():
        print(f"Removing existing temp directory: {temp_dir}")
        shutil.rmtree(temp_dir, ignore_errors=True)

    temp_dir.mkdir(exist_ok=True)
    print(f"Created temp directory: {temp_dir}")

    for file_path in parquet_files:
        try:
            print(f"\nReading {file_path.name}...")
            df = pl.read_parquet(file_path)
            schema, table = parse_filename(file_path.name)

            if breakdown_column is not None and breakdown_column not in df.columns:
                print(
                    f"Skipping {file_path.name}: "
                    f"breakdown column '{breakdown_column}' not found"
                )
                del df
                gc.collect()
                continue

            safe_file = file_path.name.replace(".", "_")

            # Check for hierarchy columns
            has_prod_typ = "prod_typ" in df.columns
            has_prod_subtp = "prod_subtp" in df.columns
            
            # Identify _id columns
            id_columns = [col for col in df.columns if "_id" in col.lower()]
            non_id_columns = [col for col in df.columns if "_id" not in col.lower()]

            if breakdown_column is None:
                profiles = []
                total_rows = df.height
                for column in df.columns:
                    try:
                        series = df[column]
                        profile = profile_series(
                            series, total_rows, "N/A", schema, table, column
                        )
                        profiles.append(profile)
                    except Exception as e:
                        print(f"Error profiling column {column}: {e}")

                if profiles:
                    temp_file = temp_dir / f"{safe_file}__NA.parquet"
                    try:
                        profiles_df = profiles_to_df(profiles)
                        profiles_df.write_parquet(temp_file)
                        print(f"Wrote {profiles_df.height} profiles to {temp_file.name}")
                    except Exception as e:
                        print(f"Error building profile DataFrame for {file_path.name}: {e}")

                del profiles
                gc.collect()

            else:
                breakdown_values = df[breakdown_column].unique().to_list()
                columns_to_profile = [col for col in df.columns if col != breakdown_column]
                
                print(f"Found {len(breakdown_values)} breakdown values")
                print(f"ID columns: {len(id_columns)}, Non-ID columns: {len(non_id_columns)}")
                print(f"Has prod_typ: {has_prod_typ}, Has prod_subtp: {has_prod_subtp}")

                for i, breakdown_value in enumerate(breakdown_values, 1):
                    mask = df[breakdown_column] == breakdown_value
                    total_rows = int(mask.sum())

                    if total_rows == 0:
                        del mask
                        gc.collect()
                        continue

                    profiles = []

                    # Profile non-ID columns at prod_data_src level only
                    for column in non_id_columns:
                        if column == breakdown_column:
                            continue
                        try:
                            series = df[column].filter(mask)
                            # Non-ID columns: regular length analysis at prod_data_src level
                            profile = profile_series(
                                series, total_rows, breakdown_value, schema, table, column,
                                is_prod_subtp_level=False
                            )
                            profiles.append(profile)
                        except Exception as e:
                            print(f"Error profiling non-ID column {column}: {e}")

                    # Handle ID columns with hierarchical breakdown
                    if id_columns:
                        if has_prod_typ or has_prod_subtp:
                            # Hierarchical breakdown available
                            print(f"  [{i}/{len(breakdown_values)}] Processing hierarchical breakdown for ID columns...")
                            
                            # First pass: Get distinct prod_data_src level stats for ID columns (regular length columns)
                            for column in id_columns:
                                try:
                                    series = df[column].filter(mask)
                                    profile = profile_series(
                                        series, total_rows, breakdown_value, schema, table, column,
                                        is_prod_subtp_level=False  # Regular length analysis at prod_data_src level
                                    )
                                    profiles.append(profile)
                                except Exception as e:
                                    print(f"Error profiling ID column {column} at prod_data_src level: {e}")
                            
                            # Second pass: Get unique prod_typ values for this prod_data_src
                            if has_prod_typ:
                                prod_typ_values = df.filter(mask)[breakdown_column, "prod_typ"].unique()["prod_typ"].to_list()
                            else:
                                prod_typ_values = [None]
                            
                            for prod_typ_val in prod_typ_values:
                                # Create mask for this prod_typ
                                if has_prod_typ and prod_typ_val is not None:
                                    typ_mask = mask & (df["prod_typ"] == prod_typ_val)
                                else:
                                    typ_mask = mask
                                
                                typ_total_rows = int(typ_mask.sum())
                                if typ_total_rows == 0:
                                    continue
                                
                                # Get unique prod_subtp values for this prod_typ
                                if has_prod_subtp:
                                    if has_prod_typ and prod_typ_val is not None:
                                        prod_subtp_values = df.filter(typ_mask)[breakdown_column, "prod_typ", "prod_subtp"].unique()["prod_subtp"].to_list()
                                    else:
                                        prod_subtp_values = df.filter(typ_mask)[breakdown_column, "prod_subtp"].unique()["prod_subtp"].to_list()
                                else:
                                    prod_subtp_values = [None]
                                
                                for prod_subtp_val in prod_subtp_values:
                                    # Create mask for this prod_subtp
                                    if has_prod_subtp and prod_subtp_val is not None:
                                        subtp_mask = typ_mask & (df["prod_subtp"] == prod_subtp_val)
                                    else:
                                        subtp_mask = typ_mask
                                    
                                    subtp_total_rows = int(subtp_mask.sum())
                                    if subtp_total_rows == 0:
                                        continue
                                    
                                    # Profile all ID columns at this level
                                    # Use prod_subtp_* length columns at this finest grain
                                    for column in id_columns:
                                        try:
                                            series = df[column].filter(subtp_mask)
                                            profile = profile_series(
                                                series, subtp_total_rows, breakdown_value, 
                                                schema, table, column,
                                                prod_typ=prod_typ_val, prod_subtp=prod_subtp_val,
                                                is_prod_subtp_level=True  # Use prod_subtp_* length columns
                                            )
                                            profiles.append(profile)
                                        except Exception as e:
                                            print(f"Error profiling ID column {column} at prod_subtp level: {e}")
                                    
                                    del subtp_mask
                                    gc.collect()
                                
                                del typ_mask
                                gc.collect()
                        else:
                            # No hierarchy columns, use default analysis for ID columns (regular length columns only)
                            print(f"  [{i}/{len(breakdown_values)}] No hierarchy columns found, using default analysis for ID columns...")
                            for column in id_columns:
                                try:
                                    series = df[column].filter(mask)
                                    profile = profile_series(
                                        series, total_rows, breakdown_value, schema, table, column,
                                        is_prod_subtp_level=False  # Regular length columns when no hierarchy
                                    )
                                    profiles.append(profile)
                                except Exception as e:
                                    print(f"Error profiling ID column {column}: {e}")

                    # Write profiles for this breakdown value
                    if profiles:
                        safe_breakdown = (
                            str(breakdown_value)
                            .replace("/", "_")
                            .replace("\\", "_")
                            .replace(":", "_")
                        )
                        temp_file = temp_dir / f"{safe_file}__{safe_breakdown}.parquet"
                        try:
                            profiles_df = profiles_to_df(profiles)
                            profiles_df.write_parquet(temp_file)
                            print(f"  [{i}/{len(breakdown_values)}] Wrote {profiles_df.height} profiles to {temp_file.name}")
                        except Exception as e:
                            print(f"Error building profile DataFrame: {e}")

                    del profiles
                    del mask
                    gc.collect()

            del df
            gc.collect()
            print(f"Freed memory for {file_path.name}")

        except Exception as e:
            print(f"Error processing {file_path.name}: {e}")

    # Final merge of all temp profile files
    temp_files = list(temp_dir.glob("*.parquet"))

    if not temp_files:
        print("No profiles generated")
        try:
            temp_dir.rmdir()
        except Exception:
            pass
        return None

    print("\n============================================================")
    print(f"Merging {len(temp_files)} temp files into {output_file} (streaming)...")

    column_order = [
        "prod_data_src",
        "prod_typ",
        "prod_subtp",
        "schema",
        "table",
        "column",
        "total_rows",
        "null_count",
        "null_pct",
        "null_bucket",
        "distinct_count",
        "unique_count",
        "unique_pct",
        "duplicate_count",
        "duplicate_pct",
        "special_char_count",
        "most_frequent_value",
        "top_5_dups",
        "min_value",
        "max_value",
        "most_common_len",
        "median_len",
        "min_len",
        "max_len",
        "prod_subtp_most_common_len",
        "prod_subtp_median_len",
        "prod_subtp_min_len",
        "prod_subtp_max_len",
    ]

    temp_pattern = str(temp_dir / "*.parquet")

    # Stream to final parquet
    lf = pl.scan_parquet(temp_pattern).select(column_order)
    lf.sink_parquet(output_file)

    # Count rows in a streaming friendly way
    row_count = (
        pl.scan_parquet(temp_pattern)
        .select(pl.count().alias("count"))
        .collect()
        .item()
    )

    print("Cleaning up temp files...")
    for temp_file in temp_files:
        try:
            temp_file.unlink()
        except Exception as e:
            print(f"Could not delete temp file {temp_file}: {e}")

    try:
        temp_dir.rmdir()
    except Exception as e:
        print(f"Could not remove temp directory {temp_dir}: {e}")

    gc.collect()

    print("============================================================")
    print(f"Profiling complete. Results saved to: {output_file}")
    print(f"Total profiles: {row_count:,}")

    if return_results_df:
        print("Loading results DataFrame into memory (may be large)...")
        results_df = pl.read_parquet(output_file)
        return results_df

    return None


if __name__ == "__main__":
    folder_path = "C:/Users/NULL/Desktop/pdf codes/DD/v2"
    breakdown_column = "prod_data_src"

    results = profile_parquet_files(
        folder_path,
        output_file="data_profile_results.parquet",
        breakdown_column=breakdown_column,
        return_results_df=False,
    )
