
import polars as pl
from pathlib import Path
import gc
import shutil
import re
from typing import Dict, Any

def compute_date_metrics(col_name: str, series: pl.Series) -> Dict[str, Any]:
out = {
“earliest_date”: None,
“latest_date”: None,
“date_range_days”: None
}

```
if not re.search(r"(dt|date)", col_name, flags=re.IGNORECASE):
    return out

err_msg = "Format Error"

try:
    s = series.drop_nulls().cast(str)
    if s.len() == 0:
        return {
            "earliest_date": err_msg,
            "latest_date": err_msg,
            "date_range_days": None
        }
    
    dt = s.str.strptime(pl.Datetime, strict=False)
    dt_valid = dt.drop_nulls()
    
    if dt_valid.len() == 0:
        return {
            "earliest_date": err_msg,
            "latest_date": err_msg,
            "date_range_days": None
        }
    
    min_ts = dt_valid.min()
    max_ts = dt_valid.max()
    
    earliest_date = min_ts.date().strftime("%Y-%m-%d")
    latest_date = max_ts.date().strftime("%Y-%m-%d")
    
    date_range_days = (max_ts.date() - min_ts.date()).days
    
    return {
        "earliest_date": earliest_date,
        "latest_date": latest_date,
        "date_range_days": date_range_days
    }
    
except Exception:
    return {
        "earliest_date": err_msg,
        "latest_date": err_msg,
        "date_range_days": None
    }
```

def get_null_bucket(null_pct):
if null_pct == 0:
return “0”
elif null_pct < 25:
return “0-24%”
elif null_pct < 50:
return “25-49%”
elif null_pct < 75:
return “50-74%”
elif null_pct < 100:
return “75-99%”
else:
return “100%”

def profile_series(series, total_rows, prod_data_src, schema, table, column_name):
prod_data_src = “N/A” if prod_data_src is None else str(prod_data_src)

```
null_count = series.null_count()
non_null_count = total_rows - null_count
null_pct = (null_count / total_rows * 100) if total_rows > 0 else 0.0

if non_null_count == 0:
    return {
        "prod_data_src": prod_data_src,
        "schema": schema,
        "table": table,
        "column": column_name,
        "total_rows": int(total_rows),
        "null_count": int(null_count),
        "null_pct": float(round(null_pct, 2)),
        "null_bucket": get_null_bucket(null_pct),
        "distinct_count": 0,
        "unique_count": 0,
        "unique_pct": 0.0,
        "duplicate_count": 0,
        "duplicate_pct": 0.0,
        "special_char_count": 0,
        "most_frequent_value": None,
        "top_5_dups": "No duplicates",
        "min_value": None,
        "max_value": None,
        "most_common_len": None,
        "median_len": None,
        "min_len": None,
        "max_len": None,
        "earliest_date": None,
        "latest_date": None,
        "date_range_days": None,
    }

distinct_count = series.n_unique()

is_numeric = series.dtype in [
    pl.Int8, pl.Int16, pl.Int32, pl.Int64,
    pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64,
    pl.Float32, pl.Float64,
]

if is_numeric:
    try:
        min_value = series.min()
        max_value = series.max()
    except Exception:
        min_value = None
        max_value = None
else:
    min_value = None
    max_value = None

non_null_series = series.drop_nulls()
if non_null_series.len() == 0:
    non_null_count = 0
else:
    non_null_count = non_null_series.len()

str_series = non_null_series.cast(pl.Utf8)

grouped = str_series.to_frame("val").group_by("val").agg(
    pl.len().alias("count")
)

truly_unique_count = grouped.filter(pl.col("count") == 1).height
unique_pct = (
    (truly_unique_count / non_null_count * 100) if non_null_count > 0 else 0.0
)

duplicate_rows = grouped.filter(pl.col("count") > 1)
if duplicate_rows.height > 0:
    duplicate_count = duplicate_rows.select(pl.col("count").sum()).item()
    duplicate_pct = (
        (duplicate_count / non_null_count * 100) if non_null_count > 0 else 0.0
    )

    duplicate_sorted = duplicate_rows.sort("count", descending=True)
    top_5 = duplicate_sorted.head(5)
    result_parts = [
        f"{row[0]}({row[1]})"
        for row in top_5.select(["val", "count"]).iter_rows()
    ]
    if duplicate_rows.height > 5:
        result_parts.append(f"...{duplicate_rows.height - 5}+ more")
    top_5_dups = " | ".join(result_parts)
else:
    duplicate_count = 0
    duplicate_pct = 0.0
    top_5_dups = "No duplicates"

most_frequent_row = grouped.sort("count", descending=True).head(1)
most_frequent_value = (
    most_frequent_row[0, 0] if most_frequent_row.height > 0 else None
)

special_char_count = (
    str_series.map_elements(
        lambda val: any(
            not c.isalnum() for c in (val[1:] if val.startswith("-") else val)
        ),
        return_dtype=pl.Boolean,
    ).sum()
)

lengths = str_series.str.len_chars()
len_value_counts = lengths.value_counts().sort("count", descending=True)
most_common_len = len_value_counts[0, 0] if len_value_counts.height > 0 else None
median_raw = lengths.median()
median_len = int(median_raw) if median_raw is not None else None
min_len = lengths.min()
max_len = lengths.max()

date_metrics = compute_date_metrics(column_name, series)

del grouped
del duplicate_rows
del lengths
gc.collect()

return {
    "prod_data_src": prod_data_src,
    "schema": schema,
    "table": table,
    "column": column_name,
    "total_rows": int(total_rows),
    "null_count": int(null_count),
    "null_pct": float(round(null_pct, 2)),
    "null_bucket": get_null_bucket(null_pct),
    "distinct_count": int(distinct_count),
    "unique_count": int(truly_unique_count),
    "unique_pct": float(round(unique_pct, 2)),
    "duplicate_count": int(duplicate_count),
    "duplicate_pct": float(round(duplicate_pct, 2)),
    "special_char_count": int(special_char_count),
    "most_frequent_value": (
        str(most_frequent_value) if most_frequent_value is not None else None
    ),
    "top_5_dups": top_5_dups,
    "min_value": str(min_value) if min_value is not None else None,
    "max_value": str(max_value) if max_value is not None else None,
    "most_common_len": int(most_common_len) if most_common_len is not None else None,
    "median_len": int(median_len) if median_len is not None else None,
    "min_len": int(min_len) if min_len is not None else None,
    "max_len": int(max_len) if max_len is not None else None,
    "earliest_date": date_metrics["earliest_date"],
    "latest_date": date_metrics["latest_date"],
    "date_range_days": date_metrics["date_range_days"],
}
```

def parse_filename(filename):
name = Path(filename).stem
parts = name.split(”.”)

```
if len(parts) >= 3:
    return parts[-2], parts[-1]
elif len(parts) == 2:
    return parts[0], parts[1]
else:
    return "unknown", parts[0]
```

def profiles_to_df(profiles):
schema = {
“prod_data_src”: pl.Utf8,
“schema”: pl.Utf8,
“table”: pl.Utf8,
“column”: pl.Utf8,
“total_rows”: pl.Int64,
“null_count”: pl.Int64,
“null_pct”: pl.Float64,
“null_bucket”: pl.Utf8,
“distinct_count”: pl.Int64,
“unique_count”: pl.Int64,
“unique_pct”: pl.Float64,
“duplicate_count”: pl.Int64,
“duplicate_pct”: pl.Float64,
“special_char_count”: pl.Int64,
“most_frequent_value”: pl.Utf8,
“top_5_dups”: pl.Utf8,
“min_value”: pl.Utf8,
“max_value”: pl.Utf8,
“most_common_len”: pl.Int64,
“median_len”: pl.Int64,
“min_len”: pl.Int64,
“max_len”: pl.Int64,
“earliest_date”: pl.Utf8,
“latest_date”: pl.Utf8,
“date_range_days”: pl.Int64,
}
return pl.from_dicts(profiles, schema=schema)

def profile_parquet_files(
folder_path,
output_file=“data_profile_results.parquet”,
breakdown_column=None,
return_results_df=False,
):
folder_path = Path(folder_path)
parquet_files = list(folder_path.glob(”*.parquet”))

```
if not parquet_files:
    print("No parquet files found in folder")
    return None

temp_dir = folder_path / "_temp_profiles"

if temp_dir.exists():
    print(f"Removing existing temp directory: {temp_dir}")
    shutil.rmtree(temp_dir, ignore_errors=True)

temp_dir.mkdir(exist_ok=True)
print(f"Created temp directory: {temp_dir}")

for file_path in parquet_files:
    try:
        print(f"\nReading {file_path.name}...")
        df = pl.read_parquet(file_path)
        schema, table = parse_filename(file_path.name)

        if breakdown_column is not None and breakdown_column not in df.columns:
            print(
                f"Skipping {file_path.name}: "
                f"breakdown column '{breakdown_column}' not found"
            )
            del df
            gc.collect()
            continue

        safe_file = file_path.name.replace(".", "_")

        if breakdown_column is None:
            profiles = []
            total_rows = df.height
            for column in df.columns:
                try:
                    series = df[column]
                    profile = profile_series(
                        series,
                        total_rows,
                        "N/A",
                        schema,
                        table,
                        column,
                    )
                    profiles.append(profile)
                except Exception as e:
                    print(f"Error profiling column {column}: {e}")

            if profiles:
                temp_file = temp_dir / f"{safe_file}__NA.parquet"
                try:
                    profiles_df = profiles_to_df(profiles)
                    profiles_df.write_parquet(temp_file)
                    print(
                        f"Wrote {profiles_df.height} profiles to {temp_file.name}"
                    )
                except Exception as e:
                    print(
                        f"Error building profile DataFrame for {file_path.name} "
                        f"(no breakdown): {e}"
                    )

            del profiles
            gc.collect()

        else:
            breakdown_values = df[breakdown_column].unique().to_list()
            columns_to_profile = [
                col for col in df.columns if col != breakdown_column
            ]
            print(
                f"Found {len(breakdown_values)} breakdown values, "
                f"{len(columns_to_profile)} columns to profile"
            )

            for i, breakdown_value in enumerate(breakdown_values, 1):
                mask = df[breakdown_column] == breakdown_value
                total_rows = int(mask.sum())

                if total_rows == 0:
                    del mask
                    gc.collect()
                    continue

                profiles = []

                for column in columns_to_profile:
                    try:
                        series = df[column].filter(mask)
                        profile = profile_series(
                            series,
                            total_rows,
                            breakdown_value,
                            schema,
                            table,
                            column,
                        )
                        profiles.append(profile)
                    except Exception as e:
                        print(
                            f"Error profiling column {column} for "
                            f"{breakdown_column}={breakdown_value}: {e}"
                        )

                if profiles:
                    safe_breakdown = (
                        str(breakdown_value)
                        .replace("/", "_")
                        .replace("\\", "_")
                        .replace(":", "_")
                    )
                    temp_file = temp_dir / f"{safe_file}__{safe_breakdown}.parquet"
                    try:
                        profiles_df = profiles_to_df(profiles)
                        profiles_df.write_parquet(temp_file)
                        print(
                            f"[{i}/{len(breakdown_values)}] "
                            f"Wrote {profiles_df.height} profiles to {temp_file.name}"
                        )
                    except Exception as e:
                        print(
                            f"Error building profile DataFrame for "
                            f"{file_path.name}, {breakdown_column}="
                            f"{breakdown_value}: {e}"
                        )

                del profiles
                del mask
                gc.collect()

        del df
        gc.collect()
        print(f"Freed memory for {file_path.name}")

    except Exception as e:
        print(f"Error processing {file_path.name}: {e}")

temp_files = list(temp_dir.glob("*.parquet"))

if not temp_files:
    print("No profiles generated")
    try:
        temp_dir.rmdir()
    except Exception:
        pass
    return None

print("\n============================================================")
print(f"Merging {len(temp_files)} temp files into {output_file} (streaming)...")

column_order = [
    "prod_data_src",
    "schema",
    "table",
    "column",
    "total_rows",
    "null_count",
    "null_pct",
    "null_bucket",
    "distinct_count",
    "unique_count",
    "unique_pct",
    "duplicate_count",
    "duplicate_pct",
    "special_char_count",
    "most_frequent_value",
    "top_5_dups",
    "min_value",
    "max_value",
    "most_common_len",
    "median_len",
    "min_len",
    "max_len",
    "earliest_date",
    "latest_date",
    "date_range_days",
]

temp_pattern = str(temp_dir / "*.parquet")

lf = pl.scan_parquet(temp_pattern).select(column_order)
lf.sink_parquet(output_file)

row_count = (
    pl.scan_parquet(temp_pattern)
    .select(pl.count().alias("count"))
    .collect()
    .item()
)

print("Cleaning up temp files...")
for temp_file in temp_files:
    try:
        temp_file.unlink()
    except Exception as e:
        print(f"Could not delete temp file {temp_file}: {e}")

try:
    temp_dir.rmdir()
except Exception as e:
    print(f"Could not remove temp directory {temp_dir}: {e}")

gc.collect()

print("============================================================")
print(f"Profiling complete. Results saved to: {output_file}")
print(f"Total profiles: {row_count:,}")

if return_results_df:
    print("Loading results DataFrame into memory (may be large)...")
    results_df = pl.read_parquet(output_file)
    return results_df

return None
```

if **name** == “**main**”:
folder_path = “C:/Users/NULL/Desktop/pdf codes/DD/v2”
breakdown_column = “prod_data_src”

```
results = profile_parquet_files(
    folder_path,
    output_file="data_profile_results.parquet",
    breakdown_column=breakdown_column,
    return_results_df=False,
)
```











######################



import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from datetime import timedelta

# Create the dataframe
data = {
    'ID': [111, 222, 333, 444, 555, 666, 777, 888, 999, 999],
    'NAME': ['Bob', 'Bob', 'Bob', 'Bob', 'Joe', 'Joe', 'Joe', 'Joe', 'Mike', 'Mike'],
    'DUE': ['1/1/24', '1/1/24', '1/1/24', '1/5/24', '3/6/24', '3/6/24', '3/6/24', '4/5/24', '9/6/24', '9/6/24'],
    'Type': ['AA', 'AA', 'BB', 'BB', 'BB', 'BB', 'AA', 'BB', 'BB', 'CC']
}

df = pd.DataFrame(data)

# Convert 'DUE' to datetime
df['DUE'] = pd.to_datetime(df['DUE'], format='%m/%d/%y')

# Sort the dataframe by DUE date and then by NAME
df = df.sort_values(['DUE', 'NAME'])

# Define colors for task types
color_map = {'AA': '#007681', 'BB': '#6ECEB2', 'CC': '#FDAA2F'}

# Create the horizontal bar chart
fig, ax = plt.subplots(figsize=(12, 8), facecolor='white')
fig.patch.set_facecolor('white')

# Set a common start date (let's use the earliest due date as the start)
start_date = df['DUE'].min()
end_date = df['DUE'].max()

y_ticks = []
for i, (index, row) in enumerate(df.iterrows()):
    duration = (row['DUE'] - start_date).days + 1  # +1 to make sure even same-day tasks have visible bars
    ax.barh(i, duration, left=0, height=0.6, color=color_map[row['Type']], alpha=0.8)
    y_ticks.append(f"{row['NAME']} - {row['Type']} (ID: {row['ID']})")
    
    # Add text label for ID
    ax.text(duration / 2, i, f"{row['ID']}", va='center', ha='center', color='white', fontweight='bold', fontsize=8)

# Customize the plot
ax.set_yticks(range(len(y_ticks)))
ax.set_yticklabels(y_ticks, fontsize=8)
ax.set_title('Task Duration from Start to Due Date', fontsize=14, fontweight='bold', color='#012169')

# Format x-axis to show dates
max_days = (end_date - start_date).days + 1
ax.set_xlim(0, max_days)

# Set up the date formatter
date_formatter = mdates.DateFormatter("%m/%d")  # Month/Day format
date_locator = mdates.WeekdayLocator(byweekday=mdates.MO)  # Locate Mondays

# Set x-axis ticks to show dates
ax.xaxis.set_major_locator(date_locator)
ax.xaxis.set_major_formatter(date_formatter)

# Set x-axis label
ax.set_xlabel('Due Date', fontsize=12, fontweight='bold', color='#012169')

# Remove spines
for spine in ax.spines.values():
    spine.set_visible(False)

# Add gridlines
ax.xaxis.grid(True, linestyle=':', alpha=0.7, color='#CCCCCC')

# Rotate and align the tick labels so they look better
fig.autofmt_xdate(rotation=45, ha='right')

# Add legend
legend_elements = [plt.Rectangle((0,0),1,1, facecolor=color, edgecolor='none', alpha=0.8) for color in color_map.values()]
ax.legend(legend_elements, color_map.keys(), loc='lower right', title='Task Type')

plt.tight_layout()
plt.show()
