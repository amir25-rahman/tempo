
Private Sub Worksheet_Calculate()
    Dim r As Range
    For Each r In Range("F2:F100")   ' adjust range
        Range("D" & r.Row & ":E" & r.Row).Interior.Color = _
            r.DisplayFormat.Interior.Color
    Next r
End Sub





Private Sub Worksheet_Calculate()
    Dim r As Range
    For Each r In Range("F2:F100")  ' adjust range
        Range("D" & r.Row & ":E" & r.Row).Interior.Color = _
            r.DisplayFormat.Interior.Color
    Next r
End Sub





Unique DQ Elements _True :=
COALESCE(
    SUMX(
        VALUES(
            merged_df[SoR Name],
            merged_df[Schema],
            merged_df[Table],
            merged_df[Attribute]
        ),
        CALCULATE(
            DISTINCTCOUNT( merged_df[pde_name] ),
            FILTER(
                merged_df,
                merged_df[filter_criteria] <> BLANK()
            )
        )
    ),
    0
)






import polars as pl
from pathlib import Path
import gc
import shutil
import sys


def trim_memory():
    """
    Ask the OS to trim the process working set so you can
    actually see memory go down in Task Manager / top.

    On Windows: uses EmptyWorkingSet.
    On Linux: uses malloc_trim(0) if available.
    On other platforms: no-op.
    """
    try:
        if sys.platform == "win32":
            import ctypes

            ctypes.windll.psapi.EmptyWorkingSet(
                ctypes.windll.kernel32.GetCurrentProcess()
            )
        elif sys.platform.startswith("linux"):
            import ctypes

            libc = ctypes.CDLL("libc.so.6")
            libc.malloc_trim(0)
        # macOS and others: nothing easy and safe by default
    except Exception:
        # If trimming fails, do not crash the job
        pass


def get_null_bucket(null_pct):
    if null_pct == 0:
        return "0"
    elif null_pct < 25:
        return "0-24%"
    elif null_pct < 50:
        return "25-49%"
    elif null_pct < 75:
        return "50-74%"
    elif null_pct < 100:
        return "75-99%"
    else:
        return "100%"


def profile_series(series, total_rows, prod_data_src, schema, table, column_name):
    # Normalize prod_data_src so the column is always string
    prod_data_src = "N/A" if prod_data_src is None else str(prod_data_src)

    null_count = series.null_count()
    non_null_count = total_rows - null_count
    null_pct = (null_count / total_rows * 100) if total_rows > 0 else 0.0

    # Early out if everything is null
    if non_null_count == 0:
        return {
            "prod_data_src": prod_data_src,
            "schema": schema,
            "table": table,
            "column": column_name,
            "total_rows": int(total_rows),
            "null_count": int(null_count),
            "null_pct": float(round(null_pct, 2)),
            "null_bucket": get_null_bucket(null_pct),
            "distinct_count": 0,
            "unique_count": 0,
            "unique_pct": 0.0,
            "duplicate_count": 0,
            "duplicate_pct": 0.0,
            "special_char_count": 0,
            "most_frequent_value": None,
            "top_5_dups": "No duplicates",
            "min_value": None,
            "max_value": None,
            "most_common_len": None,
            "median_len": None,
            "min_len": None,
            "max_len": None,
        }

    # Distinct is on the full series including nulls
    distinct_count = series.n_unique()

    is_numeric = series.dtype in [
        pl.Int8,
        pl.Int16,
        pl.Int32,
        pl.Int64,
        pl.UInt8,
        pl.UInt16,
        pl.UInt32,
        pl.UInt64,
        pl.Float32,
        pl.Float64,
    ]

    if is_numeric:
        try:
            min_value = series.min()
            max_value = series.max()
        except Exception:
            min_value = None
            max_value = None
    else:
        min_value = None
        max_value = None

    # Work only on non null values for string based stats
    non_null_series = series.drop_nulls()
    if non_null_series.len() == 0:
        non_null_count = 0
    else:
        non_null_count = non_null_series.len()

    # String representation for grouping, length, special char
    str_series = non_null_series.cast(pl.Utf8)

    # Group by value on the string representation
    grouped = str_series.to_frame("val").group_by("val").agg(
        pl.len().alias("count")
    )

    truly_unique_count = grouped.filter(pl.col("count") == 1).height
    unique_pct = (
        (truly_unique_count / non_null_count * 100) if non_null_count > 0 else 0.0
    )

    duplicate_rows = grouped.filter(pl.col("count") > 1)
    if duplicate_rows.height > 0:
        duplicate_count = duplicate_rows.select(pl.col("count").sum()).item()
        duplicate_pct = (
            (duplicate_count / non_null_count * 100) if non_null_count > 0 else 0.0
        )

        duplicate_sorted = duplicate_rows.sort("count", descending=True)
        top_5 = duplicate_sorted.head(5)
        # val is column 0, count is column 1
        result_parts = [
            f"{row[0]}({row[1]})"
            for row in top_5.select(["val", "count"]).iter_rows()
        ]
        if duplicate_rows.height > 5:
            result_parts.append(f"...{duplicate_rows.height - 5}+ more")
        top_5_dups = " | ".join(result_parts)
    else:
        duplicate_count = 0
        duplicate_pct = 0.0
        top_5_dups = "No duplicates"

    most_frequent_row = grouped.sort("count", descending=True).head(1)
    most_frequent_value = (
        most_frequent_row[0, 0] if most_frequent_row.height > 0 else None
    )

    # Special character count
    special_char_count = (
        str_series.map_elements(
            lambda val: any(
                not c.isalnum() for c in (val[1:] if val.startswith("-") else val)
            ),
            return_dtype=pl.Boolean,
        ).sum()
    )

    # Length based stats
    lengths = str_series.str.len_chars()
    len_value_counts = lengths.value_counts().sort("count", descending=True)
    most_common_len = len_value_counts[0, 0] if len_value_counts.height > 0 else None
    median_raw = lengths.median()
    median_len = int(median_raw) if median_raw is not None else None
    min_len = lengths.min()
    max_len = lengths.max()

    # Free intermediate objects explicitly
    del grouped
    del duplicate_rows
    del lengths
    gc.collect()

    return {
        "prod_data_src": prod_data_src,
        "schema": schema,
        "table": table,
        "column": column_name,
        "total_rows": int(total_rows),
        "null_count": int(null_count),
        "null_pct": float(round(null_pct, 2)),
        "null_bucket": get_null_bucket(null_pct),
        "distinct_count": int(distinct_count),
        "unique_count": int(truly_unique_count),
        "unique_pct": float(round(unique_pct, 2)),
        "duplicate_count": int(duplicate_count),
        "duplicate_pct": float(round(duplicate_pct, 2)),
        "special_char_count": int(special_char_count),
        "most_frequent_value": (
            str(most_frequent_value) if most_frequent_value is not None else None
        ),
        "top_5_dups": top_5_dups,
        "min_value": str(min_value) if min_value is not None else None,
        "max_value": str(max_value) if max_value is not None else None,
        "most_common_len": int(most_common_len) if most_common_len is not None else None,
        "median_len": int(median_len) if median_len is not None else None,
        "min_len": int(min_len) if min_len is not None else None,
        "max_len": int(max_len) if max_len is not None else None,
    }


def parse_filename(filename):
    name = Path(filename).stem
    parts = name.split(".")

    if len(parts) >= 3:
        return parts[-2], parts[-1]
    elif len(parts) == 2:
        return parts[0], parts[1]
    else:
        return "unknown", parts[0]


def profiles_to_df(profiles):
    # Explicit schema to keep types stable and avoid builder errors
    schema = {
        "prod_data_src": pl.Utf8,
        "schema": pl.Utf8,
        "table": pl.Utf8,
        "column": pl.Utf8,
        "total_rows": pl.Int64,
        "null_count": pl.Int64,
        "null_pct": pl.Float64,
        "null_bucket": pl.Utf8,
        "distinct_count": pl.Int64,
        "unique_count": pl.Int64,
        "unique_pct": pl.Float64,
        "duplicate_count": pl.Int64,
        "duplicate_pct": pl.Float64,
        "special_char_count": pl.Int64,
        "most_frequent_value": pl.Utf8,
        "top_5_dups": pl.Utf8,
        "min_value": pl.Utf8,
        "max_value": pl.Utf8,
        "most_common_len": pl.Int64,
        "median_len": pl.Int64,
        "min_len": pl.Int64,
        "max_len": pl.Int64,
    }
    return pl.from_dicts(profiles, schema=schema)


def profile_parquet_files(
    folder_path,
    output_file="data_profile_results.parquet",
    breakdown_column=None,
    return_results_df=False,
):
    folder_path = Path(folder_path)
    parquet_files = list(folder_path.glob("*.parquet"))

    if not parquet_files:
        print("No parquet files found in folder")
        return None

    temp_dir = folder_path / "_temp_profiles"

    # Reset temp dir to avoid mixing old data
    if temp_dir.exists():
        print(f"Removing existing temp directory: {temp_dir}")
        shutil.rmtree(temp_dir, ignore_errors=True)

    temp_dir.mkdir(exist_ok=True)
    print(f"Created temp directory: {temp_dir}")

    for file_path in parquet_files:
        try:
            print(f"\nProcessing {file_path.name}...")
            schema, table = parse_filename(file_path.name)
            safe_file = file_path.name.replace(".", "_")

            # Get list of columns once, with a tiny read
            df_head = pl.read_parquet(file_path, n_rows=1)
            all_columns = df_head.columns
            del df_head
            gc.collect()

            if breakdown_column is not None and breakdown_column not in all_columns:
                print(
                    f"Skipping {file_path.name}: "
                    f"breakdown column '{breakdown_column}' not found"
                )
                continue

            if breakdown_column is None:
                # No breakdown - just profile each column over whole file
                profiles = []
                for column in all_columns:
                    print(f"  Profiling column {column} (no breakdown)")
                    df_col = pl.read_parquet(file_path, columns=[column])
                    total_rows = df_col.height
                    series = df_col[column]

                    profile = profile_series(
                        series,
                        total_rows,
                        "N/A",
                        schema,
                        table,
                        column,
                    )
                    profiles.append(profile)

                    del series
                    del df_col
                    gc.collect()

                if profiles:
                    temp_file = temp_dir / f"{safe_file}__NA.parquet"
                    try:
                        profiles_df = profiles_to_df(profiles)
                        profiles_df.write_parquet(temp_file)
                        print(
                            f"Wrote {profiles_df.height} profiles to {temp_file.name}"
                        )
                    finally:
                        del profiles_df
                        gc.collect()

                del profiles
                gc.collect()
                trim_memory()

            else:
                # With breakdown per prod_data_src (or other breakdown column)
                # First read only the breakdown column to get the distinct values
                print(f"  Reading breakdown column '{breakdown_column}'")
                df_break = pl.read_parquet(file_path, columns=[breakdown_column])
                breakdown_values = df_break[breakdown_column].unique().to_list()
                del df_break
                gc.collect()
                trim_memory()

                columns_to_profile = [c for c in all_columns if c != breakdown_column]
                print(
                    f"  Found {len(breakdown_values)} breakdown values, "
                    f"{len(columns_to_profile)} columns to profile"
                )

                for i, breakdown_value in enumerate(breakdown_values, 1):
                    print(f"  Breakdown {i}/{len(breakdown_values)}: {breakdown_value}")
                    profiles = []

                    for column in columns_to_profile:
                        # Read only breakdown_column and this column
                        df_col = pl.read_parquet(
                            file_path,
                            columns=[breakdown_column, column],
                        )

                        mask = df_col[breakdown_column] == breakdown_value
                        total_rows = int(mask.sum())

                        if total_rows > 0:
                            series = df_col[column].filter(mask)

                            profile = profile_series(
                                series,
                                total_rows,
                                breakdown_value,
                                schema,
                                table,
                                column,
                            )
                            profiles.append(profile)

                            del series

                        del mask
                        del df_col
                        gc.collect()

                    if profiles:
                        safe_breakdown = (
                            str(breakdown_value)
                            .replace("/", "_")
                            .replace("\\", "_")
                            .replace(":", "_")
                        )
                        temp_file = temp_dir / f"{safe_file}__{safe_breakdown}.parquet"
                        try:
                            profiles_df = profiles_to_df(profiles)
                            profiles_df.write_parquet(temp_file)
                            print(
                                f"    Wrote {profiles_df.height} profiles "
                                f"to {temp_file.name}"
                            )
                        finally:
                            del profiles_df
                            gc.collect()

                    # clear after each prod_data_src group, then trim working set
                    del profiles
                    gc.collect()
                    trim_memory()

            print(f"Freed memory for {file_path.name}")
            gc.collect()
            trim_memory()

        except Exception as e:
            print(f"Error processing {file_path.name}: {e}")
            gc.collect()
            trim_memory()

    # Final merge of all temp profile files
    temp_files = list(temp_dir.glob("*.parquet"))

    if not temp_files:
        print("No profiles generated")
        try:
            temp_dir.rmdir()
        except Exception:
            pass
        return None

    print("\n============================================================")
    print(f"Merging {len(temp_files)} temp files into {output_file} (streaming)...")

    column_order = [
        "prod_data_src",
        "schema",
        "table",
        "column",
        "total_rows",
        "null_count",
        "null_pct",
        "null_bucket",
        "distinct_count",
        "unique_count",
        "unique_pct",
        "duplicate_count",
        "duplicate_pct",
        "special_char_count",
        "most_frequent_value",
        "top_5_dups",
        "min_value",
        "max_value",
        "most_common_len",
        "median_len",
        "min_len",
        "max_len",
    ]

    temp_pattern = str(temp_dir / "*.parquet")

    # Stream to final parquet
    lf = pl.scan_parquet(temp_pattern).select(column_order)
    lf.sink_parquet(output_file)

    # Count rows in a streaming friendly way
    row_count = (
        pl.scan_parquet(temp_pattern)
        .select(pl.count().alias("count"))
        .collect()
        .item()
    )

    print("Cleaning up temp files...")
    for temp_file in temp_files:
        try:
            temp_file.unlink()
        except Exception as e:
            print(f"Could not delete temp file {temp_file}: {e}")

    try:
        temp_dir.rmdir()
    except Exception as e:
        print(f"Could not remove temp directory {temp_dir}: {e}")

    gc.collect()
    trim_memory()

    print("============================================================")
    print(f"Profiling complete. Results saved to: {output_file}")
    print(f"Total profiles: {row_count:,}")

    if return_results_df:
        print("Loading results DataFrame into memory (may be large)...")
        results_df = pl.read_parquet(output_file)
        return results_df

    return None


if __name__ == "__main__":
    folder_path = "C:/Users/NULL/Desktop/pdf codes/DD/v2"
    breakdown_column = "prod_data_src"

    results = profile_parquet_files(
        folder_path,
        output_file="data_profile_results.parquet",
        breakdown_column=breakdown_column,
        return_results_df=False,
    )











################################

import polars as pl
import os
import re
from pathlib import Path
from collections import Counter
from tqdm import tqdm

def get_null_bucket(null_pct):
    if null_pct == 0:
        return "0"
    elif null_pct < 25:
        return "0-24%"
    elif null_pct < 50:
        return "25-49%"
    elif null_pct < 75:
        return "50-74%"
    elif null_pct < 100:
        return "75-99%"
    else:
        return "100%"

def count_special_chars(series):
    non_null = series.filter(series.is_not_null())
    
    if non_null.len() == 0:
        return 0
    
    str_series = non_null.cast(pl.Utf8)
    
    def has_special_char(val):
        check_str = val[1:] if val.startswith('-') else val
        return any(not c.isalnum() for c in check_str)
    
    count = str_series.map_elements(has_special_char, return_dtype=pl.Boolean).sum()
    return count

def get_top_5_duplicates(series):
    value_counts = series.filter(series.is_not_null()).value_counts().sort("count", descending=True)
    duplicates = value_counts.filter(pl.col("count") > 1)
    
    if duplicates.height == 0:
        return "No duplicates"
    
    top_5 = duplicates.head(5)
    result_parts = []
    for row in top_5.iter_rows():
        val, count = row
        result_parts.append(f"{val}({count})")
    
    if duplicates.height > 5:
        additional = duplicates.height - 5
        result_parts.append(f"...{additional}+ more")
    
    return " | ".join(result_parts)

def get_length_stats(series):
    non_null = series.filter(series.is_not_null())
    
    if non_null.len() == 0:
        return None, None, None, None
    
    str_series = non_null.cast(pl.Utf8)
    lengths = str_series.str.len_chars()
    
    len_counts = lengths.value_counts().sort("count", descending=True)
    most_common_len = len_counts[0, 0] if len_counts.height > 0 else None
    
    median_len = lengths.median()
    min_len = lengths.min()
    max_len = lengths.max()
    
    return most_common_len, median_len, min_len, max_len

def profile_column(df, column_name, total_rows, prod_data_src, schema, table):
    series = df[column_name]
    
    non_null_series = series.filter(series.is_not_null())
    non_null_count = non_null_series.len()
    
    null_count = series.null_count()
    null_pct = (null_count / total_rows * 100) if total_rows > 0 else 0
    null_bucket = get_null_bucket(null_pct)
    
    value_counts = None
    if non_null_count > 0:
        value_counts = non_null_series.value_counts().sort("count", descending=True)
    
    if value_counts is not None and value_counts.height > 0:
        distinct_count = value_counts.height
        truly_unique_count = value_counts.filter(pl.col("count") == 1).height
        unique_pct = (truly_unique_count / non_null_count * 100)
        
        duplicate_values = value_counts.filter(pl.col("count") > 1)
        duplicate_count = duplicate_values.select(pl.col("count").sum()).item() if duplicate_values.height > 0 else 0
        duplicate_pct = (duplicate_count / non_null_count * 100)
        
        most_frequent_value = value_counts[0, 0]
        
        if duplicate_values.height == 0:
            top_5_dups = "No duplicates"
        else:
            top_5 = duplicate_values.head(5)
            result_parts = []
            for row in top_5.iter_rows():
                val, count = row
                result_parts.append(f"{val}({count})")
            if duplicate_values.height > 5:
                additional = duplicate_values.height - 5
                result_parts.append(f"...{additional}+ more")
            top_5_dups = " | ".join(result_parts)
    else:
        distinct_count = 0
        truly_unique_count = 0
        unique_pct = 0
        duplicate_count = 0
        duplicate_pct = 0
        most_frequent_value = None
        top_5_dups = "No duplicates"
    
    try:
        min_value = non_null_series.min() if non_null_count > 0 else None
        max_value = non_null_series.max() if non_null_count > 0 else None
    except:
        min_value = None
        max_value = None
    
    str_series = non_null_series.cast(pl.Utf8) if non_null_count > 0 else None
    
    if str_series is not None:
        def has_special_char(val):
            check_str = val[1:] if val.startswith('-') else val
            return any(not c.isalnum() for c in check_str)
        special_char_count = str_series.map_elements(has_special_char, return_dtype=pl.Boolean).sum()
    else:
        special_char_count = 0
    
    if str_series is not None and str_series.len() > 0:
        lengths = str_series.str.len_chars()
        len_counts = lengths.value_counts().sort("count", descending=True)
        most_common_len = len_counts[0, 0] if len_counts.height > 0 else None
        median_len = lengths.median()
        min_len = lengths.min()
        max_len = lengths.max()
    else:
        most_common_len = None
        median_len = None
        min_len = None
        max_len = None
    
    top_10_values = None
    if '_id' in column_name.lower() and value_counts is not None and value_counts.height > 0:
        top_10 = value_counts.head(10)
        top_10_parts = []
        for row in top_10.iter_rows():
            val, count = row
            top_10_parts.append(f"{val}({count})")
        top_10_values = " | ".join(top_10_parts)
    
    return {
        'prod_data_src': prod_data_src,
        'schema': schema,
        'table': table,
        'column': column_name,
        'total_rows': total_rows,
        'null_count': null_count,
        'null_pct': round(null_pct, 2),
        'null_bucket': null_bucket,
        'distinct_count': distinct_count,
        'unique_count': truly_unique_count,
        'unique_pct': round(unique_pct, 2),
        'duplicate_count': duplicate_count,
        'duplicate_pct': round(duplicate_pct, 2),
        'special_char_count': special_char_count,
        'most_frequent_value': str(most_frequent_value) if most_frequent_value is not None else None,
        'top_5_dups': top_5_dups,
        'min_value': str(min_value) if min_value is not None else None,
        'max_value': str(max_value) if max_value is not None else None,
        'most_common_len': most_common_len,
        'median_len': median_len,
        'min_len': min_len,
        'max_len': max_len,
        'top_10_values': top_10_values
    }

def parse_filename(filename):
    name = Path(filename).stem
    parts = name.split('.')
    
    if len(parts) >= 3:
        return parts[-2], parts[-1]
    elif len(parts) == 2:
        return parts[0], parts[1]
    else:
        return 'unknown', parts[0]

def profile_parquet_files(folder_path, output_file='data_profile_results.parquet', breakdown_column=None):
    all_profiles = []
    
    parquet_files = list(Path(folder_path).glob('*.parquet'))
    
    if not parquet_files:
        print(f"No .parquet files found in {folder_path}")
        return
    
    print(f"Found {len(parquet_files)} parquet files to process")
    if breakdown_column:
        print(f"Breaking down analysis by column: '{breakdown_column}'")
    else:
        print("Analyzing entire dataset (no breakdown)")
    
    for file_path in tqdm(parquet_files, desc="Processing files", unit="file"):
        print(f"\nProcessing: {file_path.name}")
        
        try:
            df = pl.read_parquet(file_path)
            
            schema, table = parse_filename(file_path.name)
            
            if breakdown_column and breakdown_column not in df.columns:
                print(f"  Warning: breakdown column '{breakdown_column}' not found in {file_path.name}")
                print(f"  Processing entire file as single group")
                breakdown_column = None
            
            if breakdown_column is None:
                total_rows = df.height
                columns_to_profile = df.columns
                
                for column in tqdm(columns_to_profile, desc=f"  Profiling columns", unit="col", leave=False):
                    profile = profile_column(
                        df, 
                        column, 
                        total_rows, 
                        'N/A',
                        schema,
                        table
                    )
                    all_profiles.append(profile)
                
                print(f"  Profiled {len(columns_to_profile)} columns ({total_rows:,} rows)")
            else:
                breakdown_values = df[breakdown_column].unique().to_list()
                print(f"  Found {len(breakdown_values)} unique '{breakdown_column}' values")
                
                columns_to_profile = [col for col in df.columns if col != breakdown_column]
                
                for breakdown_value in tqdm(breakdown_values, desc=f"  Processing breakdown values", unit="value", leave=False):
                    subset = df.filter(pl.col(breakdown_column) == breakdown_value)
                    total_rows = subset.height
                    
                    for column in columns_to_profile:
                        profile = profile_column(
                            subset, 
                            column, 
                            total_rows, 
                            breakdown_value,
                            schema,
                            table
                        )
                        all_profiles.append(profile)
                    
                    print(f"  Profiled {breakdown_column}='{breakdown_value}': {len(columns_to_profile)} columns ({total_rows:,} rows)")
        
        except Exception as e:
            print(f"  Error processing {file_path.name}: {str(e)}")
            import traceback
            traceback.print_exc()
            continue
    
    results_df = pl.DataFrame(all_profiles)
    
    column_order = [
        'prod_data_src', 'schema', 'table', 'column', 'total_rows',
        'null_count', 'null_pct', 'null_bucket', 
        'distinct_count', 'unique_count', 'unique_pct', 
        'duplicate_count', 'duplicate_pct',
        'special_char_count', 'most_frequent_value', 'top_5_dups',
        'min_value', 'max_value',
        'most_common_len', 'median_len', 'min_len', 'max_len',
        'top_10_values'
    ]
    
    results_df = results_df.select(column_order)
    
    results_df.write_parquet(output_file)
    print(f"\n{'='*60}")
    print(f"Profiling complete!")
    print(f"Results saved to: {output_file}")
    print(f"Total profiles generated: {len(all_profiles):,}")
    print(f"{'='*60}")
    
    return results_df

if __name__ == "__main__":
    folder_path = "C:/Users/NULL/Desktop/pdf codes/DD/v2"
    breakdown_column = 'prod_data_src'
    
    results = profile_parquet_files(
        folder_path, 
        output_file='data_profile_results.parquet',
        breakdown_column=breakdown_column
    )



####################################


import polars as pl
import os
import re
from pathlib import Path
from collections import Counter
from tqdm import tqdm

def get_null_bucket(null_pct):
“”“Categorize null percentage into buckets”””
if null_pct == 0:
return “0”
elif null_pct < 25:
return “0-24%”
elif null_pct < 50:
return “25-49%”
elif null_pct < 75:
return “50-74%”
elif null_pct < 100:
return “75-99%”
else:
return “100%”

def count_special_chars(series):
“”“Count rows that contain special characters (excluding alphanumeric and handling negatives)”””
# Filter nulls first, then convert to string
non_null = series.filter(series.is_not_null())

```
if non_null.len() == 0:
    return 0

# Convert to string after filtering nulls
str_series = non_null.cast(pl.Utf8)

# Count rows where special characters exist (not counting leading minus for negatives)
def has_special_char(val):
    # Remove leading minus if it exists
    check_str = val[1:] if val.startswith('-') else val
    # Check if any character is not alphanumeric
    return any(not c.isalnum() for c in check_str)

count = str_series.map_elements(has_special_char, return_dtype=pl.Boolean).sum()
return count
```

def get_top_5_duplicates(series):
“”“Get top 5 duplicate values with counts in value(count) format”””
# Get value counts, excluding nulls
value_counts = series.filter(series.is_not_null()).value_counts().sort(“count”, descending=True)

```
# Only consider values that appear more than once
duplicates = value_counts.filter(pl.col("count") > 1)

if duplicates.height == 0:
    return "No duplicates"

# Get top 5
top_5 = duplicates.head(5)

# Format result as value(count)
result_parts = []
for row in top_5.iter_rows():
    val, count = row
    result_parts.append(f"{val}({count})")

# Show how many additional duplicate groups exist
if duplicates.height > 5:
    additional = duplicates.height - 5
    result_parts.append(f"...{additional}+ more")

return " | ".join(result_parts)
```

def get_length_stats(series):
“”“Calculate length statistics for the actual values in a column”””
# Filter nulls first
non_null = series.filter(series.is_not_null())

```
if non_null.len() == 0:
    return None, None, None, None

# Convert to string AFTER filtering nulls to get character length
str_series = non_null.cast(pl.Utf8)
lengths = str_series.str.len_chars()

# Most common length
len_counts = lengths.value_counts().sort("count", descending=True)
most_common_len = len_counts[0, 0] if len_counts.height > 0 else None

# Stats
median_len = lengths.median()
min_len = lengths.min()
max_len = lengths.max()

return most_common_len, median_len, min_len, max_len
```

def profile_column(df, column_name, total_rows, prod_data_src, schema, table):
“”“Profile a single column with all required metrics”””

```
series = df[column_name]

# Single pass: get non-null series once and reuse
non_null_series = series.filter(series.is_not_null())
non_null_count = non_null_series.len()

# Null metrics
null_count = series.null_count()
null_pct = (null_count / total_rows * 100) if total_rows > 0 else 0
null_bucket = get_null_bucket(null_pct)

# Calculate value_counts ONCE and reuse for all metrics
value_counts = None
if non_null_count > 0:
    value_counts = non_null_series.value_counts().sort("count", descending=True)

# Unique/duplicate metrics - all from single value_counts
if value_counts is not None and value_counts.height > 0:
    distinct_count = value_counts.height
    truly_unique_count = value_counts.filter(pl.col("count") == 1).height
    unique_pct = (truly_unique_count / non_null_count * 100)
    
    # Duplicates
    duplicate_values = value_counts.filter(pl.col("count") > 1)
    duplicate_count = duplicate_values.select(pl.col("count").sum()).item() if duplicate_values.height > 0 else 0
    duplicate_pct = (duplicate_count / non_null_count * 100)
    
    # Most frequent value (already sorted)
    most_frequent_value = value_counts[0, 0]
    
    # Top 5 duplicates (reuse duplicate_values from above)
    if duplicate_values.height == 0:
        top_5_dups = "No duplicates"
    else:
        top_5 = duplicate_values.head(5)
        result_parts = []
        for row in top_5.iter_rows():
            val, count = row
            result_parts.append(f"{val}({count})")
        if duplicate_values.height > 5:
            additional = duplicate_values.height - 5
            result_parts.append(f"...{additional}+ more")
        top_5_dups = " | ".join(result_parts)
else:
    distinct_count = 0
    truly_unique_count = 0
    unique_pct = 0
    duplicate_count = 0
    duplicate_pct = 0
    most_frequent_value = None
    top_5_dups = "No duplicates"

# Min/Max values (handle null dtype columns)
try:
    min_value = non_null_series.min() if non_null_count > 0 else None
    max_value = non_null_series.max() if non_null_count > 0 else None
except:
    min_value = None
    max_value = None

# Convert to string once for special chars and length calculations
str_series = non_null_series.cast(pl.Utf8) if non_null_count > 0 else None

# Special characters (reuse str_series)
if str_series is not None:
    def has_special_char(val):
        check_str = val[1:] if val.startswith('-') else val
        return any(not c.isalnum() for c in check_str)
    special_char_count = str_series.map_elements(has_special_char, return_dtype=pl.Boolean).sum()
else:
    special_char_count = 0

# Length statistics (reuse str_series)
if str_series is not None and str_series.len() > 0:
    lengths = str_series.str.len_chars()
    len_counts = lengths.value_counts().sort("count", descending=True)
    most_common_len = len_counts[0, 0] if len_counts.height > 0 else None
    median_len = lengths.median()
    min_len = lengths.min()
    max_len = lengths.max()
else:
    most_common_len = None
    median_len = None
    min_len = None
    max_len = None

# Top 10 value counts for ID columns (reuse value_counts)
top_10_values = None
if '_id' in column_name.lower() and value_counts is not None and value_counts.height > 0:
    top_10 = value_counts.head(10)
    top_10_parts = []
    for row in top_10.iter_rows():
        val, count = row
        top_10_parts.append(f"{val}({count})")
    top_10_values = " | ".join(top_10_parts)

return {
    'prod_data_src': prod_data_src,
    'schema': schema,
    'table': table,
    'column': column_name,
    'total_rows': total_rows,
    'null_count': null_count,
    'null_pct': round(null_pct, 2),
    'null_bucket': null_bucket,
    'distinct_count': distinct_count,
    'unique_count': truly_unique_count,
    'unique_pct': round(unique_pct, 2),
    'duplicate_count': duplicate_count,
    'duplicate_pct': round(duplicate_pct, 2),
    'special_char_count': special_char_count,
    'most_frequent_value': str(most_frequent_value) if most_frequent_value is not None else None,
    'top_5_dups': top_5_dups,
    'min_value': str(min_value) if min_value is not None else None,
    'max_value': str(max_value) if max_value is not None else None,
    'most_common_len': most_common_len,
    'median_len': median_len,
    'min_len': min_len,
    'max_len': max_len,
    'top_10_values': top_10_values
}
```

def parse_filename(filename):
“”“Parse filename in format hive.schema.table.parquet”””
name = Path(filename).stem  # Remove .parquet extension
parts = name.split(’.’)

```
if len(parts) >= 3:
    return parts[-2], parts[-1]  # schema, table
elif len(parts) == 2:
    return parts[0], parts[1]
else:
    return 'unknown', parts[0]
```

def profile_parquet_files(folder_path, output_csv=‘data_profile_results.csv’, breakdown_column=None):
“””
Profile all parquet files in a folder

```
Args:
    folder_path: Path to folder containing parquet files
    output_csv: Output CSV filename
    breakdown_column: Column name to break down analysis by (e.g., 'prod_data_src'). 
                     If None, analyzes entire dataset. This column will NOT be profiled itself.
"""

all_profiles = []

# Get all parquet files
parquet_files = list(Path(folder_path).glob('*.parquet'))

if not parquet_files:
    print(f"No .parquet files found in {folder_path}")
    return

print(f"Found {len(parquet_files)} parquet files to process")
if breakdown_column:
    print(f"Breaking down analysis by column: '{breakdown_column}'")
else:
    print("Analyzing entire dataset (no breakdown)")

# Progress bar for files
for file_path in tqdm(parquet_files, desc="Processing files", unit="file"):
    print(f"\nProcessing: {file_path.name}")
    
    try:
        # Read parquet file with Polars
        df = pl.read_parquet(file_path)
        
        # Parse schema and table from filename
        schema, table = parse_filename(file_path.name)
        
        # Check if breakdown column exists (if specified)
        if breakdown_column and breakdown_column not in df.columns:
            print(f"  Warning: breakdown column '{breakdown_column}' not found in {file_path.name}")
            print(f"  Processing entire file as single group")
            breakdown_column = None
        
        if breakdown_column is None:
            # Profile all columns for the entire file
            total_rows = df.height
            columns_to_profile = df.columns
            
            # Progress bar for columns
            for column in tqdm(columns_to_profile, desc=f"  Profiling columns", unit="col", leave=False):
                profile = profile_column(
                    df, 
                    column, 
                    total_rows, 
                    'N/A',
                    schema,
                    table
                )
                all_profiles.append(profile)
            
            print(f"  Profiled {len(columns_to_profile)} columns ({total_rows:,} rows)")
        else:
            # Get unique breakdown values
            breakdown_values = df[breakdown_column].unique().to_list()
            print(f"  Found {len(breakdown_values)} unique '{breakdown_column}' values")
            
            # Get columns to profile (exclude the breakdown column itself)
            columns_to_profile = [col for col in df.columns if col != breakdown_column]
            
            # Progress bar for breakdown values
            for breakdown_value in tqdm(breakdown_values, desc=f"  Processing breakdown values", unit="value", leave=False):
                # Filter for this breakdown value
                subset = df.filter(pl.col(breakdown_column) == breakdown_value)
                total_rows = subset.height
                
                # Profile each column in this subset (except breakdown column)
                for column in columns_to_profile:
                    profile = profile_column(
                        subset, 
                        column, 
                        total_rows, 
                        breakdown_value,
                        schema,
                        table
                    )
                    all_profiles.append(profile)
                
                print(f"  Profiled {breakdown_column}='{breakdown_value}': {len(columns_to_profile)} columns ({total_rows:,} rows)")
    
    except Exception as e:
        print(f"  Error processing {file_path.name}: {str(e)}")
        import traceback
        traceback.print_exc()
        continue

# Create results dataframe with Polars
results_df = pl.DataFrame(all_profiles)

# Reorder columns for better readability
column_order = [
    'prod_data_src', 'schema', 'table', 'column', 'total_rows',
    'null_count', 'null_pct', 'null_bucket', 
    'distinct_count', 'unique_count', 'unique_pct', 
    'duplicate_count', 'duplicate_pct',
    'special_char_count', 'most_frequent_value', 'top_5_dups',
    'min_value', 'max_value',
    'most_common_len', 'median_len', 'min_len', 'max_len',
    'top_10_values'
]

results_df = results_df.select(column_order)

# Save to CSV
results_df.write_csv(output_csv)
print(f"\n{'='*60}")
print(f"Profiling complete!")
print(f"Results saved to: {output_csv}")
print(f"Total profiles generated: {len(all_profiles):,}")
print(f"{'='*60}")

return results_df
```

# Example usage

if **name** == “**main**”:
# Specify your folder path here
folder_path = “/path/to/your/parquet/files”

```
# Define breakdown column (set to None to analyze entire dataset without breakdown)
breakdown_column = 'prod_data_src'  # Change this to your desired column or set to None

# Run the profiler
results = profile_parquet_files(
    folder_path, 
    output_csv='data_profile_results.csv',
    breakdown_column=breakdown_column
)

# Display first few rows
print("\nFirst 10 rows of results:")
print(results.head(10))
```




############################

import polars as pl
import os
import re
from pathlib import Path
from collections import Counter

def get_null_bucket(null_pct):
    """Categorize null percentage into buckets"""
    if null_pct == 0:
        return "0"
    elif null_pct < 25:
        return "0-24%"
    elif null_pct < 50:
        return "25-49%"
    elif null_pct < 75:
        return "50-74%"
    elif null_pct < 100:
        return "75-99%"
    else:
        return "100%"

def count_special_chars(series):
    """Count rows that contain special characters (excluding alphanumeric and handling negatives)"""
    # Filter nulls first, then convert to string
    non_null = series.filter(series.is_not_null())
    
    if non_null.len() == 0:
        return 0
    
    # Convert to string after filtering nulls
    str_series = non_null.cast(pl.Utf8)
    
    # Count rows where special characters exist (not counting leading minus for negatives)
    def has_special_char(val):
        # Remove leading minus if it exists
        check_str = val[1:] if val.startswith('-') else val
        # Check if any character is not alphanumeric
        return any(not c.isalnum() for c in check_str)
    
    count = str_series.map_elements(has_special_char, return_dtype=pl.Boolean).sum()
    return count

def get_top_5_duplicates(series):
    """Get top 5 duplicate values with counts in value(count) format"""
    # Get value counts, excluding nulls
    value_counts = series.filter(series.is_not_null()).value_counts().sort("count", descending=True)
    
    # Only consider values that appear more than once
    duplicates = value_counts.filter(pl.col("count") > 1)
    
    if duplicates.height == 0:
        return "No duplicates"
    
    # Get top 5
    top_5 = duplicates.head(5)
    
    # Format result as value(count)
    result_parts = []
    for row in top_5.iter_rows():
        val, count = row
        result_parts.append(f"{val}({count})")
    
    # Show how many additional duplicate groups exist
    if duplicates.height > 5:
        additional = duplicates.height - 5
        result_parts.append(f"...{additional}+ more")
    
    return " | ".join(result_parts)

def get_length_stats(series):
    """Calculate length statistics for the actual values in a column"""
    # Filter nulls first
    non_null = series.filter(series.is_not_null())
    
    if non_null.len() == 0:
        return None, None, None, None
    
    # Convert to string AFTER filtering nulls to get character length
    str_series = non_null.cast(pl.Utf8)
    lengths = str_series.str.len_chars()
    
    # Most common length
    len_counts = lengths.value_counts().sort("count", descending=True)
    most_common_len = len_counts[0, 0] if len_counts.height > 0 else None
    
    # Stats
    median_len = lengths.median()
    min_len = lengths.min()
    max_len = lengths.max()
    
    return most_common_len, median_len, min_len, max_len

def profile_column(df, column_name, total_rows, prod_data_src, schema, table):
    """Profile a single column with all required metrics"""
    
    series = df[column_name]
    
    # Null metrics
    null_count = series.null_count()
    null_pct = (null_count / total_rows * 100) if total_rows > 0 else 0
    null_bucket = get_null_bucket(null_pct)
    
    # Unique metrics
    non_null_count = total_rows - null_count
    distinct_count = series.n_unique()
    
    # For unique_pct, we want truly unique values (appear only once)
    if non_null_count > 0:
        value_counts = series.filter(series.is_not_null()).value_counts()
        truly_unique_count = value_counts.filter(pl.col("count") == 1).height
        unique_pct = (truly_unique_count / non_null_count * 100)
    else:
        truly_unique_count = 0
        unique_pct = 0
    
    # Duplicate metrics - percentage of rows that contain duplicate values
    if non_null_count > 0:
        value_counts = series.filter(series.is_not_null()).value_counts()
        # Get values that appear more than once
        duplicate_values = value_counts.filter(pl.col("count") > 1)
        # Sum the counts of all duplicate values to get total rows with duplicates
        duplicate_count = duplicate_values.select(pl.col("count").sum()).item() if duplicate_values.height > 0 else 0
        duplicate_pct = (duplicate_count / non_null_count * 100) if non_null_count > 0 else 0
    else:
        duplicate_count = 0
        duplicate_pct = 0
    
    # Min/Max values
    min_value = series.min()
    max_value = series.max()
    
    # Special characters
    special_char_count = count_special_chars(series)
    
    # Most frequent value
    if non_null_count > 0:
        value_counts = series.value_counts().sort("count", descending=True)
        most_frequent_value = value_counts[0, 0] if value_counts.height > 0 else None
    else:
        most_frequent_value = None
    
    # Top 5 duplicates
    top_5_dups = get_top_5_duplicates(series)
    
    # Length statistics
    most_common_len, median_len, min_len, max_len = get_length_stats(series)
    
    # Top 10 value counts for ID columns
    top_10_values = None
    if 'id' in column_name.lower():
        value_counts_top10 = series.value_counts().sort("count", descending=True).head(10)
        if value_counts_top10.height > 0:
            # Format as "value(count)"
            top_10_parts = []
            for row in value_counts_top10.iter_rows():
                val, count = row
                top_10_parts.append(f"{val}({count})")
            top_10_values = " | ".join(top_10_parts)
    
    # Date range analysis for date columns
    earliest_date = None
    latest_date = None
    date_range_days = None
    if '_dt' in column_name.lower():
        try:
            non_null_series = series.filter(series.is_not_null())
            
            # Debug: print column info
            print(f"    Analyzing date column: {column_name}, dtype: {series.dtype}, non-null count: {non_null_series.len()}")
            if non_null_series.len() > 0:
                print(f"    Sample values: {non_null_series.head(3).to_list()}")
            
            if non_null_series.len() == 0:
                pass
            elif series.dtype in [pl.Date, pl.Datetime]:
                # Already a date/datetime type
                date_series = non_null_series
                earliest_date = date_series.min()
                latest_date = date_series.max()
            else:
                # String column - try multiple date formats
                str_series = non_null_series.cast(pl.Utf8)
                
                # Try common date formats
                date_formats = [
                    "%Y-%m-%d",           # 2024-10-20
                    "%Y/%m/%d",           # 2024/10/20
                    "%m/%d/%Y",           # 10/20/2024
                    "%m-%d-%Y",           # 10-20-2024
                    "%d/%m/%Y",           # 20/10/2024
                    "%d-%m-%Y",           # 20-10-2024
                    "%Y-%m-%d %H:%M:%S",  # 2024-10-20 01:20:00
                    "%Y/%m/%d %H:%M:%S",  # 2024/10/20 01:20:00
                    "%m/%d/%Y %H:%M:%S",  # 10/20/2024 01:20:00
                ]
                
                date_series = None
                for fmt in date_formats:
                    try:
                        date_series = str_series.str.strptime(pl.Date, fmt)
                        print(f"    Successfully parsed with format: {fmt}")
                        break
                    except:
                        continue
                
                # If no format worked, try automatic parsing
                if date_series is None:
                    try:
                        date_series = str_series.str.to_date()
                        print(f"    Successfully parsed with str.to_date()")
                    except:
                        try:
                            date_series = str_series.str.to_datetime()
                            print(f"    Successfully parsed with str.to_datetime()")
                        except:
                            print(f"    Failed to parse dates")
                            pass
                
                if date_series is not None and date_series.len() > 0:
                    earliest_date = date_series.min()
                    latest_date = date_series.max()
            
            # Calculate date range
            if earliest_date and latest_date:
                print(f"    Found dates: {earliest_date} to {latest_date}")
                # Handle both date and datetime types
                if hasattr(earliest_date, 'date'):
                    earliest_date = earliest_date.date()
                if hasattr(latest_date, 'date'):
                    latest_date = latest_date.date()
                date_range_days = (latest_date - earliest_date).days
                
        except Exception as e:
            print(f"    Date parsing error for {column_name}: {str(e)}")
            import traceback
            traceback.print_exc()
    
    return {
        'prod_data_src': prod_data_src,
        'schema': schema,
        'table': table,
        'column': column_name,
        'total_rows': total_rows,
        'null_count': null_count,
        'null_pct': round(null_pct, 2),
        'null_bucket': null_bucket,
        'distinct_count': distinct_count,
        'unique_count': truly_unique_count,
        'unique_pct': round(unique_pct, 2),
        'duplicate_count': duplicate_count,
        'duplicate_pct': round(duplicate_pct, 2),
        'special_char_count': special_char_count,
        'most_frequent_value': str(most_frequent_value) if most_frequent_value is not None else None,
        'top_5_dups': top_5_dups,
        'min_value': str(min_value) if min_value is not None else None,
        'max_value': str(max_value) if max_value is not None else None,
        'most_common_len': most_common_len,
        'median_len': median_len,
        'min_len': min_len,
        'max_len': max_len,
        'top_10_values': top_10_values,
        'earliest_date': str(earliest_date) if earliest_date is not None else None,
        'latest_date': str(latest_date) if latest_date is not None else None,
        'date_range_days': date_range_days
    }

def parse_filename(filename):
    """Parse filename in format hive.schema.table.parquet"""
    name = Path(filename).stem  # Remove .parquet extension
    parts = name.split('.')
    
    if len(parts) >= 3:
        return parts[-2], parts[-1]  # schema, table
    elif len(parts) == 2:
        return parts[0], parts[1]
    else:
        return 'unknown', parts[0]

def profile_parquet_files(folder_path, output_csv='data_profile_results.csv', breakdown_column=None):
    """
    Profile all parquet files in a folder
    
    Args:
        folder_path: Path to folder containing parquet files
        output_csv: Output CSV filename
        breakdown_column: Column name to break down analysis by (e.g., 'prod_data_src'). 
                         If None, analyzes entire dataset. This column will NOT be profiled itself.
    """
    
    all_profiles = []
    
    # Get all parquet files
    parquet_files = list(Path(folder_path).glob('*.parquet'))
    
    if not parquet_files:
        print(f"No .parquet files found in {folder_path}")
        return
    
    print(f"Found {len(parquet_files)} parquet files to process")
    if breakdown_column:
        print(f"Breaking down analysis by column: '{breakdown_column}'")
    else:
        print("Analyzing entire dataset (no breakdown)")
    
    for file_path in parquet_files:
        print(f"\nProcessing: {file_path.name}")
        
        try:
            # Read parquet file with Polars
            df = pl.read_parquet(file_path)
            
            # Parse schema and table from filename
            schema, table = parse_filename(file_path.name)
            
            # Check if breakdown column exists (if specified)
            if breakdown_column and breakdown_column not in df.columns:
                print(f"  Warning: breakdown column '{breakdown_column}' not found in {file_path.name}")
                print(f"  Processing entire file as single group")
                breakdown_column = None
            
            if breakdown_column is None:
                # Profile all columns for the entire file
                total_rows = df.height
                columns_to_profile = df.columns
                
                for column in columns_to_profile:
                    profile = profile_column(
                        df, 
                        column, 
                        total_rows, 
                        'N/A',
                        schema,
                        table
                    )
                    all_profiles.append(profile)
                
                print(f"  Profiled {len(columns_to_profile)} columns ({total_rows:,} rows)")
            else:
                # Get unique breakdown values
                breakdown_values = df[breakdown_column].unique().to_list()
                print(f"  Found {len(breakdown_values)} unique '{breakdown_column}' values")
                
                # Get columns to profile (exclude the breakdown column itself)
                columns_to_profile = [col for col in df.columns if col != breakdown_column]
                
                for breakdown_value in breakdown_values:
                    # Filter for this breakdown value
                    subset = df.filter(pl.col(breakdown_column) == breakdown_value)
                    total_rows = subset.height
                    
                    # Profile each column in this subset (except breakdown column)
                    for column in columns_to_profile:
                        profile = profile_column(
                            subset, 
                            column, 
                            total_rows, 
                            breakdown_value,
                            schema,
                            table
                        )
                        all_profiles.append(profile)
                    
                    print(f"  Profiled {breakdown_column}='{breakdown_value}': {len(columns_to_profile)} columns ({total_rows:,} rows)")
        
        except Exception as e:
            print(f"  Error processing {file_path.name}: {str(e)}")
            import traceback
            traceback.print_exc()
            continue
    
    # Create results dataframe with Polars
    results_df = pl.DataFrame(all_profiles)
    
    # Reorder columns for better readability
    column_order = [
        'prod_data_src', 'schema', 'table', 'column', 'total_rows',
        'null_count', 'null_pct', 'null_bucket', 
        'distinct_count', 'unique_count', 'unique_pct', 
        'duplicate_count', 'duplicate_pct',
        'special_char_count', 'most_frequent_value', 'top_5_dups',
        'min_value', 'max_value',
        'most_common_len', 'median_len', 'min_len', 'max_len',
        'top_10_values'
    ]
    
    results_df = results_df.select(column_order)
    
    # Save to CSV
    results_df.write_csv(output_csv)
    print(f"\n{'='*60}")
    print(f"Profiling complete!")
    print(f"Results saved to: {output_csv}")
    print(f"{'='*60}")
    
    return results_df

# Example usage
if __name__ == "__main__":
    # Specify your folder path here
    folder_path = "C:/Users/NULL/Desktop/pdf codes/DD/v2"
    
    # Define breakdown column (set to None to analyze entire dataset without breakdown)
    breakdown_column = 'prod_data_src'  # Change this to your desired column or set to None
    
    # Run the profiler
    results = profile_parquet_files(
        folder_path, 
        output_csv='data_profile_results.csv',
        breakdown_column=breakdown_column
    )
    
