
import polars as pl
from pathlib import Path
import gc
import shutil


def get_null_bucket(null_pct):
    if null_pct == 0:
        return "0"
    elif null_pct < 25:
        return "0-24%"
    elif null_pct < 50:
        return "25-49%"
    elif null_pct < 75:
        return "50-74%"
    elif null_pct < 100:
        return "75-99%"
    else:
        return "100%"


def profile_series(series, total_rows, prod_data_src, schema, table, column_name):
    # Normalize prod_data_src so the column is always string
    prod_data_src = "N/A" if prod_data_src is None else str(prod_data_src)

    null_count = series.null_count()
    non_null_count = total_rows - null_count
    null_pct = (null_count / total_rows * 100) if total_rows > 0 else 0.0

    # Early out if everything is null
    if non_null_count == 0:
        return {
            "prod_data_src": prod_data_src,
            "schema": schema,
            "table": table,
            "column": column_name,
            "total_rows": int(total_rows),
            "null_count": int(null_count),
            "null_pct": float(round(null_pct, 2)),
            "null_bucket": get_null_bucket(null_pct),
            "distinct_count": 0,
            "unique_count": 0,
            "unique_pct": 0.0,
            "duplicate_count": 0,
            "duplicate_pct": 0.0,
            "special_char_count": 0,
            "most_frequent_value": None,
            "top_5_dups": "No duplicates",
            "min_value": None,
            "max_value": None,
            "most_common_len": None,
            "median_len": None,
            "min_len": None,
            "max_len": None,
        }

    distinct_count = series.n_unique()

    is_numeric = series.dtype in [
        pl.Int8,
        pl.Int16,
        pl.Int32,
        pl.Int64,
        pl.UInt8,
        pl.UInt16,
        pl.UInt32,
        pl.UInt64,
        pl.Float32,
        pl.Float64,
    ]

    if is_numeric:
        try:
            min_value = series.min()
            max_value = series.max()
        except Exception:
            min_value = None
            max_value = None
    else:
        min_value = None
        max_value = None

    # Work only on non null values for string based stats
    non_null_series = series.drop_nulls()
    if non_null_series.len() == 0:
        non_null_count = 0
    else:
        non_null_count = non_null_series.len()

    str_series = non_null_series.cast(pl.Utf8)

    grouped = str_series.to_frame("val").group_by("val").agg(
        pl.len().alias("count")
    )

    truly_unique_count = grouped.filter(pl.col("count") == 1).height
    unique_pct = (
        (truly_unique_count / non_null_count * 100) if non_null_count > 0 else 0.0
    )

    duplicate_rows = grouped.filter(pl.col("count") > 1)
    if duplicate_rows.height > 0:
        duplicate_count = duplicate_rows.select(pl.col("count").sum()).item()
        duplicate_pct = (
            (duplicate_count / non_null_count * 100) if non_null_count > 0 else 0.0
        )

        duplicate_sorted = duplicate_rows.sort("count", descending=True)
        top_5 = duplicate_sorted.head(5)
        result_parts = [
            f"{row[0]}({row[1]})"
            for row in top_5.select(["val", "count"]).iter_rows()
        ]
        if duplicate_rows.height > 5:
            result_parts.append(f"...{duplicate_rows.height - 5}+ more")
        top_5_dups = " | ".join(result_parts)
    else:
        duplicate_count = 0
        duplicate_pct = 0.0
        top_5_dups = "No duplicates"

    most_frequent_row = grouped.sort("count", descending=True).head(1)
    most_frequent_value = (
        most_frequent_row[0, 0] if most_frequent_row.height > 0 else None
    )

    special_char_count = (
        str_series.map_elements(
            lambda val: any(
                not c.isalnum() for c in (val[1:] if val.startswith("-") else val)
            ),
            return_dtype=pl.Boolean,
        ).sum()
    )

    lengths = str_series.str.len_chars()
    len_value_counts = lengths.value_counts().sort("count", descending=True)
    most_common_len = len_value_counts[0, 0] if len_value_counts.height > 0 else None
    median_raw = lengths.median()
    median_len = int(median_raw) if median_raw is not None else None
    min_len = lengths.min()
    max_len = lengths.max()

    del grouped
    del duplicate_rows
    del lengths
    gc.collect()

    return {
        "prod_data_src": prod_data_src,
        "schema": schema,
        "table": table,
        "column": column_name,
        "total_rows": int(total_rows),
        "null_count": int(null_count),
        "null_pct": float(round(null_pct, 2)),
        "null_bucket": get_null_bucket(null_pct),
        "distinct_count": int(distinct_count),
        "unique_count": int(truly_unique_count),
        "unique_pct": float(round(unique_pct, 2)),
        "duplicate_count": int(duplicate_count),
        "duplicate_pct": float(round(duplicate_pct, 2)),
        "special_char_count": int(special_char_count),
        "most_frequent_value": (
            str(most_frequent_value) if most_frequent_value is not None else None
        ),
        "top_5_dups": top_5_dups,
        "min_value": str(min_value) if min_value is not None else None,
        "max_value": str(max_value) if max_value is not None else None,
        "most_common_len": int(most_common_len) if most_common_len is not None else None,
        "median_len": int(median_len) if median_len is not None else None,
        "min_len": int(min_len) if min_len is not None else None,
        "max_len": int(max_len) if max_len is not None else None,
    }


def parse_filename(filename):
    name = Path(filename).stem
    parts = name.split(".")

    if len(parts) >= 3:
        return parts[-2], parts[-1]
    elif len(parts) == 2:
        return parts[0], parts[1]
    else:
        return "unknown", parts[0]


def profiles_to_df(profiles):
    schema = {
        "prod_data_src": pl.Utf8,
        "schema": pl.Utf8,
        "table": pl.Utf8,
        "column": pl.Utf8,
        "total_rows": pl.Int64,
        "null_count": pl.Int64,
        "null_pct": pl.Float64,
        "null_bucket": pl.Utf8,
        "distinct_count": pl.Int64,
        "unique_count": pl.Int64,
        "unique_pct": pl.Float64,
        "duplicate_count": pl.Int64,
        "duplicate_pct": pl.Float64,
        "special_char_count": pl.Int64,
        "most_frequent_value": pl.Utf8,
        "top_5_dups": pl.Utf8,
        "min_value": pl.Utf8,
        "max_value": pl.Utf8,
        "most_common_len": pl.Int64,
        "median_len": pl.Int64,
        "min_len": pl.Int64,
        "max_len": pl.Int64,
    }
    return pl.from_dicts(profiles, schema=schema)


def profile_parquet_files(
    folder_path,
    output_file="data_profile_results.parquet",
    breakdown_column=None,
    return_results_df=False,
):
    folder_path = Path(folder_path)
    parquet_files = list(folder_path.glob("*.parquet"))

    if not parquet_files:
        print("No parquet files found in folder")
        return None

    temp_dir = folder_path / "_temp_profiles"
    if temp_dir.exists():
        print(f"Removing existing temp directory: {temp_dir}")
        shutil.rmtree(temp_dir, ignore_errors=True)

    temp_dir.mkdir(exist_ok=True)
    print(f"Created temp directory: {temp_dir}")

    for file_path in parquet_files:
        try:
            print(f"\nProcessing {file_path.name}...")
            df = pl.read_parquet(file_path)
            schema, table = parse_filename(file_path.name)

            if breakdown_column is not None and breakdown_column not in df.columns:
                print(
                    f"Skipping {file_path.name} - "
                    f"breakdown column '{breakdown_column}' not found"
                )
                del df
                gc.collect()
                continue

            safe_file = file_path.name.replace(".", "_")

            if breakdown_column is None:
                # no breakdown, just profile whole file
                profiles = []
                total_rows = df.height

                for column in df.columns:
                    print(f"  Profiling column {column} (no breakdown)")
                    try:
                        series = df[column]
                        profile = profile_series(
                            series,
                            total_rows,
                            "N/A",
                            schema,
                            table,
                            column,
                        )
                        profiles.append(profile)
                    except Exception as e:
                        print(f"    Error profiling column {column}: {e}")

                if profiles:
                    temp_file = temp_dir / f"{safe_file}__NA.parquet"
                    profiles_df = profiles_to_df(profiles)
                    profiles_df.write_parquet(temp_file)
                    print(
                        f"Wrote {profiles_df.height} profiles to {temp_file.name}"
                    )
                    del profiles_df

                del profiles
                gc.collect()

            else:
                # breakdown per value in breakdown_column (prod_data_src style)
                breakdown_values = df[breakdown_column].unique().to_list()
                columns_to_profile = [
                    col for col in df.columns if col != breakdown_column
                ]

                print(
                    f"  Found {len(breakdown_values)} breakdown values, "
                    f"{len(columns_to_profile)} columns to profile"
                )

                for i, breakdown_value in enumerate(breakdown_values, 1):
                    print(f"  Breakdown {i}/{len(breakdown_values)}: {breakdown_value}")
                    mask = df[breakdown_column] == breakdown_value
                    total_rows = int(mask.sum())

                    if total_rows == 0:
                        del mask
                        gc.collect()
                        continue

                    profiles = []

                    for column in columns_to_profile:
                        try:
                            series = df[column].filter(mask)
                            profile = profile_series(
                                series,
                                total_rows,
                                breakdown_value,
                                schema,
                                table,
                                column,
                            )
                            profiles.append(profile)
                        except Exception as e:
                            print(
                                f"    Error profiling column {column} for "
                                f"{breakdown_column}={breakdown_value}: {e}"
                            )

                    if profiles:
                        safe_breakdown = (
                            str(breakdown_value)
                            .replace("/", "_")
                            .replace("\\", "_")
                            .replace(":", "_")
                        )
                        temp_file = temp_dir / f"{safe_file}__{safe_breakdown}.parquet"
                        profiles_df = profiles_to_df(profiles)
                        profiles_df.write_parquet(temp_file)
                        print(
                            f"    Wrote {profiles_df.height} profiles "
                            f"to {temp_file.name}"
                        )
                        del profiles_df

                    # clean up per prod_data_src
                    del profiles
                    del mask
                    gc.collect()

            del df
            gc.collect()
            print(f"Freed memory for {file_path.name}")

        except Exception as e:
            print(f"Error processing {file_path.name}: {e}")
            gc.collect()

    # final merge over all temp files
    temp_files = list(temp_dir.glob("*.parquet"))
    if not temp_files:
        print("No profiles generated")
        try:
            temp_dir.rmdir()
        except Exception:
            pass
        return None

    print("\n============================================================")
    print(f"Merging {len(temp_files)} temp files into {output_file} (streaming)...")

    column_order = [
        "prod_data_src",
        "schema",
        "table",
        "column",
        "total_rows",
        "null_count",
        "null_pct",
        "null_bucket",
        "distinct_count",
        "unique_count",
        "unique_pct",
        "duplicate_count",
        "duplicate_pct",
        "special_char_count",
        "most_frequent_value",
        "top_5_dups",
        "min_value",
        "max_value",
        "most_common_len",
        "median_len",
        "min_len",
        "max_len",
    ]

    temp_pattern = str(temp_dir / "*.parquet")

    lf = pl.scan_parquet(temp_pattern).select(column_order)
    lf.sink_parquet(output_file)

    row_count = (
        pl.scan_parquet(temp_pattern)
        .select(pl.count().alias("count"))
        .collect()
        .item()
    )

    print("Cleaning up temp files...")
    for temp_file in temp_files:
        try:
            temp_file.unlink()
        except Exception as e:
            print(f"Could not delete temp file {temp_file}: {e}")

    try:
        temp_dir.rmdir()
    except Exception as e:
        print(f"Could not remove temp directory {temp_dir}: {e}")

    gc.collect()

    print("============================================================")
    print(f"Profiling complete. Results saved to: {output_file}")
    print(f"Total profiles: {row_count:,}")

    if return_results_df:
        print("Loading results DataFrame into memory (may be large)...")
        results_df = pl.read_parquet(output_file)
        return results_df

    return None


if __name__ == "__main__":
    folder_path = "C:/Users/NULL/Desktop/pdf codes/DD/v2"
    breakdown_column = "prod_data_src"

    results = profile_parquet_files(
        folder_path,
        output_file="data_profile_results.parquet",
        breakdown_column=breakdown_column,
        return_results_df=False,
    )











#######################


import polars as pl
import os
import re
from pathlib import Path
from collections import Counter

def get_null_bucket(null_pct):
    """Categorize null percentage into buckets"""
    if null_pct == 0:
        return "0"
    elif null_pct < 25:
        return "0-24%"
    elif null_pct < 50:
        return "25-49%"
    elif null_pct < 75:
        return "50-74%"
    elif null_pct < 100:
        return "75-99%"
    else:
        return "100%"

def count_special_chars(series):
    """Count rows that contain special characters (excluding alphanumeric and handling negatives)"""
    # Filter nulls first, then convert to string
    non_null = series.filter(series.is_not_null())
    
    if non_null.len() == 0:
        return 0
    
    # Convert to string after filtering nulls
    str_series = non_null.cast(pl.Utf8)
    
    # Count rows where special characters exist (not counting leading minus for negatives)
    def has_special_char(val):
        # Remove leading minus if it exists
        check_str = val[1:] if val.startswith('-') else val
        # Check if any character is not alphanumeric
        return any(not c.isalnum() for c in check_str)
    
    count = str_series.map_elements(has_special_char, return_dtype=pl.Boolean).sum()
    return count

def get_top_5_duplicates(series):
    """Get top 5 duplicate values with counts in value(count) format"""
    # Get value counts, excluding nulls
    value_counts = series.filter(series.is_not_null()).value_counts().sort("count", descending=True)
    
    # Only consider values that appear more than once
    duplicates = value_counts.filter(pl.col("count") > 1)
    
    if duplicates.height == 0:
        return "No duplicates"
    
    # Get top 5
    top_5 = duplicates.head(5)
    
    # Format result as value(count)
    result_parts = []
    for row in top_5.iter_rows():
        val, count = row
        result_parts.append(f"{val}({count})")
    
    # Show how many additional duplicate groups exist
    if duplicates.height > 5:
        additional = duplicates.height - 5
        result_parts.append(f"...{additional}+ more")
    
    return " | ".join(result_parts)

def get_length_stats(series):
    """Calculate length statistics for the actual values in a column"""
    # Filter nulls first
    non_null = series.filter(series.is_not_null())
    
    if non_null.len() == 0:
        return None, None, None, None
    
    # Convert to string AFTER filtering nulls to get character length
    str_series = non_null.cast(pl.Utf8)
    lengths = str_series.str.len_chars()
    
    # Most common length
    len_counts = lengths.value_counts().sort("count", descending=True)
    most_common_len = len_counts[0, 0] if len_counts.height > 0 else None
    
    # Stats
    median_len = lengths.median()
    min_len = lengths.min()
    max_len = lengths.max()
    
    return most_common_len, median_len, min_len, max_len

def profile_column(df, column_name, total_rows, prod_data_src, schema, table):
    """Profile a single column with all required metrics"""
    
    series = df[column_name]
    
    # Null metrics
    null_count = series.null_count()
    null_pct = (null_count / total_rows * 100) if total_rows > 0 else 0
    null_bucket = get_null_bucket(null_pct)
    
    # Unique metrics
    non_null_count = total_rows - null_count
    distinct_count = series.n_unique()
    
    # For unique_pct, we want truly unique values (appear only once)
    if non_null_count > 0:
        value_counts = series.filter(series.is_not_null()).value_counts()
        truly_unique_count = value_counts.filter(pl.col("count") == 1).height
        unique_pct = (truly_unique_count / non_null_count * 100)
    else:
        truly_unique_count = 0
        unique_pct = 0
    
    # Duplicate metrics - percentage of rows that contain duplicate values
    if non_null_count > 0:
        value_counts = series.filter(series.is_not_null()).value_counts()
        # Get values that appear more than once
        duplicate_values = value_counts.filter(pl.col("count") > 1)
        # Sum the counts of all duplicate values to get total rows with duplicates
        duplicate_count = duplicate_values.select(pl.col("count").sum()).item() if duplicate_values.height > 0 else 0
        duplicate_pct = (duplicate_count / non_null_count * 100) if non_null_count > 0 else 0
    else:
        duplicate_count = 0
        duplicate_pct = 0
    
    # Min/Max values
    min_value = series.min()
    max_value = series.max()
    
    # Special characters
    special_char_count = count_special_chars(series)
    
    # Most frequent value
    if non_null_count > 0:
        value_counts = series.value_counts().sort("count", descending=True)
        most_frequent_value = value_counts[0, 0] if value_counts.height > 0 else None
    else:
        most_frequent_value = None
    
    # Top 5 duplicates
    top_5_dups = get_top_5_duplicates(series)
    
    # Length statistics
    most_common_len, median_len, min_len, max_len = get_length_stats(series)
    
    # Top 10 value counts for ID columns
    top_10_values = None
    if '_id' in column_name.lower():
        value_counts_top10 = series.value_counts().sort("count", descending=True).head(10)
        if value_counts_top10.height > 0:
            # Format as "value(count)"
            top_10_parts = []
            for row in value_counts_top10.iter_rows():
                val, count = row
                top_10_parts.append(f"{val}({count})")
            top_10_values = " | ".join(top_10_parts)
    
    return {
        'prod_data_src': prod_data_src,
        'schema': schema,
        'table': table,
        'column': column_name,
        'total_rows': total_rows,
        'null_count': null_count,
        'null_pct': round(null_pct, 2),
        'null_bucket': null_bucket,
        'distinct_count': distinct_count,
        'unique_count': truly_unique_count,
        'unique_pct': round(unique_pct, 2),
        'duplicate_count': duplicate_count,
        'duplicate_pct': round(duplicate_pct, 2),
        'special_char_count': special_char_count,
        'most_frequent_value': str(most_frequent_value) if most_frequent_value is not None else None,
        'top_5_dups': top_5_dups,
        'min_value': str(min_value) if min_value is not None else None,
        'max_value': str(max_value) if max_value is not None else None,
        'most_common_len': most_common_len,
        'median_len': median_len,
        'min_len': min_len,
        'max_len': max_len,
        'top_10_values': top_10_values
    }

def parse_filename(filename):
    """Parse filename in format hive.schema.table.parquet"""
    name = Path(filename).stem  # Remove .parquet extension
    parts = name.split('.')
    
    if len(parts) >= 3:
        return parts[-2], parts[-1]  # schema, table
    elif len(parts) == 2:
        return parts[0], parts[1]
    else:
        return 'unknown', parts[0]

def profile_parquet_files(folder_path, output_csv='data_profile_results.csv', breakdown_column=None):
    """
    Profile all parquet files in a folder
    
    Args:
        folder_path: Path to folder containing parquet files
        output_csv: Output CSV filename
        breakdown_column: Column name to break down analysis by (e.g., 'prod_data_src'). 
                         If None, analyzes entire dataset. This column will NOT be profiled itself.
    """
    
    all_profiles = []
    
    # Get all parquet files
    parquet_files = list(Path(folder_path).glob('*.parquet'))
    
    if not parquet_files:
        print(f"No .parquet files found in {folder_path}")
        return
    
    print(f"Found {len(parquet_files)} parquet files to process")
    if breakdown_column:
        print(f"Breaking down analysis by column: '{breakdown_column}'")
    else:
        print("Analyzing entire dataset (no breakdown)")
    
    for file_path in parquet_files:
        print(f"\nProcessing: {file_path.name}")
        
        try:
            # Read parquet file with Polars
            df = pl.read_parquet(file_path)
            
            # Parse schema and table from filename
            schema, table = parse_filename(file_path.name)
            
            # Check if breakdown column exists (if specified)
            if breakdown_column and breakdown_column not in df.columns:
                print(f"  Warning: breakdown column '{breakdown_column}' not found in {file_path.name}")
                print(f"  Processing entire file as single group")
                breakdown_column = None
            
            if breakdown_column is None:
                # Profile all columns for the entire file
                total_rows = df.height
                columns_to_profile = df.columns
                
                for column in columns_to_profile:
                    profile = profile_column(
                        df, 
                        column, 
                        total_rows, 
                        'N/A',
                        schema,
                        table
                    )
                    all_profiles.append(profile)
                
                print(f"  Profiled {len(columns_to_profile)} columns ({total_rows:,} rows)")
            else:
                # Get unique breakdown values
                breakdown_values = df[breakdown_column].unique().to_list()
                print(f"  Found {len(breakdown_values)} unique '{breakdown_column}' values")
                
                # Get columns to profile (exclude the breakdown column itself)
                columns_to_profile = [col for col in df.columns if col != breakdown_column]
                
                for breakdown_value in breakdown_values:
                    # Filter for this breakdown value
                    subset = df.filter(pl.col(breakdown_column) == breakdown_value)
                    total_rows = subset.height
                    
                    # Profile each column in this subset (except breakdown column)
                    for column in columns_to_profile:
                        profile = profile_column(
                            subset, 
                            column, 
                            total_rows, 
                            breakdown_value,
                            schema,
                            table
                        )
                        all_profiles.append(profile)
                    
                    print(f"  Profiled {breakdown_column}='{breakdown_value}': {len(columns_to_profile)} columns ({total_rows:,} rows)")
        
        except Exception as e:
            print(f"  Error processing {file_path.name}: {str(e)}")
            import traceback
            traceback.print_exc()
            continue
    
    # Create results dataframe with Polars
    results_df = pl.DataFrame(all_profiles)
    
    # Reorder columns for better readability
    column_order = [
        'prod_data_src', 'schema', 'table', 'column', 'total_rows',
        'null_count', 'null_pct', 'null_bucket', 
        'distinct_count', 'unique_count', 'unique_pct', 
        'duplicate_count', 'duplicate_pct',
        'special_char_count', 'most_frequent_value', 'top_5_dups',
        'min_value', 'max_value',
        'most_common_len', 'median_len', 'min_len', 'max_len',
        'top_10_values'
    ]
    
    results_df = results_df.select(column_order)
    
    # Save to CSV
    results_df.write_csv(output_csv)
    print(f"\n{'='*60}")
    print(f"Profiling complete!")
    print(f"Results saved to: {output_csv}")
    print(f"Total profiles generated: {len(all_profiles):,}")
    print(f"{'='*60}")
    
    return results_df

# Example usage
if __name__ == "__main__":
    # Specify your folder path here
    folder_path = "C:/Users/NULL/Desktop/pdf codes/DD/v2"
    
    # Define breakdown column (set to None to analyze entire dataset without breakdown)
    breakdown_column = 'prod_data_src'  # Change this to your desired column or set to None
    
    # Run the profiler
    results = profile_parquet_files(
        folder_path, 
        output_csv='data_profile_results.csv',
        breakdown_column=breakdown_column
    )
    
