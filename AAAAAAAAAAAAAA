

import pandas as pd
import pyarrow.parquet as pq
import os
import re
from glob import glob

def get_special_char_count(series):
    if series.dtype != object:
        return 0
    pattern = re.compile(r'[^a-zA-Z0-9\s.,:;!?@#\$%\^\&\*\(\)_\-\+=]')
    return series.dropna().astype(str).apply(lambda x: len(pattern.findall(x))).sum()

def get_null_bucket(null_pct):
    if null_pct == 0:
        return "0%"
    elif null_pct < 10:
        return "<10%"
    elif null_pct < 50:
        return "10-50%"
    else:
        return ">50%"

# --- NEW PART START ---
def analyze_series(series, top_n=3, top5_n=5):
    total = len(series)
    uniques = series.nunique(dropna=True)
    dups = total - uniques if total > 0 else 0
    
    dup_pct = round((dups / total) * 100, 2) if total > 0 else 0
    uniq_pct = round(100 - dup_pct, 2) if total > 0 else 0
    
    counts = series.value_counts(dropna=True)
    
    # Duplicates list (summary)
    dup_list = [f"{v}({c})" for v, c in counts.items() if c > 1]
    if len(dup_list) > top_n:
        dup_list = dup_list[:top_n] + [f"...+{len(dup_list) - top_n}"]
    dup_str = ", ".join(dup_list) if dup_list else "None"
    
    # Unique count only
    uniq_count = (counts == 1).sum()
    
    # Top 5 duplicates
    dup_top5 = counts[counts > 1].head(top5_n)
    dup_top5_str = ", ".join(f"{v}({c})" for v, c in dup_top5.items()) if not dup_top5.empty else "None"
    
    # Top 5 uniques
    uniq_top5 = counts[counts == 1].head(top5_n)
    uniq_top5_str = ", ".join(str(v) for v in uniq_top5.index) if not uniq_top5.empty else "None"
    
    return {
        'Dups_freq': dup_str,
        'Uniq_freq': uniq_count,
        'Dups_freq_5_max': dup_top5_str,
        'Uniq_freq_5_max': uniq_top5_str,
        'Dup%': dup_pct,
        'Uniq%': uniq_pct
    }
# --- NEW PART END ---

def analyze_parquet_file(file_path):
    try:
        table = pq.read_table(file_path)
        df = table.to_pandas()
    except Exception as e:
        print(f"Failed to read {file_path}: {e}")
        return []

    base_name = os.path.basename(file_path).replace('.parquet', '')
    parts = base_name.split('.')
    if len(parts) < 3:
        print(f"Filename format incorrect: {base_name}")
        return []

    table_label, schema, table_name = parts[-3:]
    total_rows = len(df)
    if total_rows == 0:
        return []

    prd_src = df['prd_src'].iloc[0] if 'prd_src' in df.columns else 'UNKNOWN'

    results = []
    for col in df.columns:
        if col == 'prd_src':
            continue

        series = df[col]
        null_count = series.isnull().sum()
        null_pct = (null_count / total_rows) * 100
        null_bucket = get_null_bucket(null_pct)
        special_char_count = get_special_char_count(series)
        most_freq_val = series.mode().iloc[0] if not series.mode().empty else None
        
        # --- NEW PART START ---
        # Get detailed duplicate and unique stats for this column
        freq_stats = analyze_series(series)
        # --- NEW PART END ---

        results.append({
            'prd_src': prd_src,
            'column': col,
            'data_type': str(series.dtype),
            'null_count': null_count,
            'null_pct': round(null_pct, 2),
            'null_bucket': null_bucket,
            'special_char_count': special_char_count,
            'most_frequent_value': most_freq_val,
            'total_rows': total_rows,
            'schema': schema,
            'table_label': table_label,
            'table': table_name,
            # --- NEW PART START ---
            # Add detailed freq stats:
            'Dup%': freq_stats['Dup%'],
            'Uniq%': freq_stats['Uniq%'],
            'Dups_freq': freq_stats['Dups_freq'],
            'Uniq_freq': freq_stats['Uniq_freq'],
            'Dups_freq_5_max': freq_stats['Dups_freq_5_max'],
            'Uniq_freq_5_max': freq_stats['Uniq_freq_5_max'],
            # --- NEW PART END ---
        })

    return results

# ---------- MAIN SCRIPT ----------
input_folder = r'/path/to/parquet/files'  # Replace with your path
parquet_files = glob(os.path.join(input_folder, '*.parquet'))

all_stats = []
for file in parquet_files:
    stats = analyze_parquet_file(file)
    if stats:
        all_stats.extend(stats)

df_stats = pd.DataFrame(all_stats)
df_stats.sort_values(by=['prd_src', 'schema', 'table'], inplace=True)

# Optional: Save to Excel or CSV
df_stats.to_excel('parquet_stats_report.xlsx', index=False)
# df_stats.to_csv('parquet_stats_report.csv', index=False)

print("Analysis complete. Report saved.")















import pandas as pd
import pyarrow.parquet as pq
import os
import re
from glob import glob

def get_special_char_count(series):
    if series.dtype != object:
        return 0
    pattern = re.compile(r'[^a-zA-Z0-9\s.,:;!?@#\$%\^\&\*\(\)_\-\+=]')
    return series.dropna().astype(str).apply(lambda x: len(pattern.findall(x))).sum()

def get_null_bucket(null_pct):
    if null_pct == 0:
        return "0%"
    elif null_pct < 10:
        return "<10%"
    elif null_pct < 50:
        return "10-50%"
    else:
        return ">50%"

def analyze_series(series, top_n=3, top5_n=5):
    total = len(series)
    uniques = series.nunique(dropna=True)
    dups = total - uniques if total > 0 else 0
    
    dup_pct = round((dups / total) * 100, 2) if total > 0 else 0
    uniq_pct = round(100 - dup_pct, 2) if total > 0 else 0
    
    counts = series.value_counts(dropna=True)
    
    # Duplicates list (summary)
    dup_list = [f"{v}({c})" for v, c in counts.items() if c > 1]
    if len(dup_list) > top_n:
        dup_list = dup_list[:top_n] + [f"...+{len(dup_list) - top_n}"]
    dup_str = ", ".join(dup_list) if dup_list else "None"
    
    # Unique count only
    uniq_count = (counts == 1).sum()
    
    # Top 5 duplicates
    dup_top5 = counts[counts > 1].head(top5_n)
    dup_top5_str = ", ".join(f"{v}({c})" for v, c in dup_top5.items()) if not dup_top5.empty else "None"
    
    # Top 5 uniques
    uniq_top5 = counts[counts == 1].head(top5_n)
    uniq_top5_str = ", ".join(str(v) for v in uniq_top5.index) if not uniq_top5.empty else "None"
    
    return {
        'Dups_freq': dup_str,
        'Uniq_freq': uniq_count,
        'Dups_freq_5_max': dup_top5_str,
        'Uniq_freq_5_max': uniq_top5_str,
        'Dup%': dup_pct,
        'Uniq%': uniq_pct
    }

def analyze_parquet_file(file_path):
    try:
        table = pq.read_table(file_path)
        df = table.to_pandas()
    except Exception as e:
        print(f"Failed to read {file_path}: {e}")
        return []

    base_name = os.path.basename(file_path).replace('.parquet', '')
    parts = base_name.split('.')
    if len(parts) < 3:
        print(f"Filename format incorrect: {base_name}")
        return []

    table_label, schema, table_name = parts[-3:]
    total_rows = len(df)
    if total_rows == 0:
        return []

    prd_src = df['prd_src'].iloc[0] if 'prd_src' in df.columns else 'UNKNOWN'

    results = []
    for col in df.columns:
        if col == 'prd_src':
            continue

        series = df[col]
        null_count = series.isnull().sum()
        null_pct = (null_count / total_rows) * 100
        null_bucket = get_null_bucket(null_pct)
        special_char_count = get_special_char_count(series)
        most_freq_val = series.mode().iloc[0] if not series.mode().empty else None
        
        # Get detailed duplicate and unique stats
        freq_stats = analyze_series(series)

        results.append({
            'prd_src': prd_src,
            'column': col,
            'data_type': str(series.dtype),
            'null_count': null_count,
            'null_pct': round(null_pct, 2),
            'null_bucket': null_bucket,
            'special_char_count': special_char_count,
            'most_frequent_value': most_freq_val,
            'total_rows': total_rows,
            'schema': schema,
            'table_label': table_label,
            'table': table_name,
            # Add detailed freq stats:
            'Dup%': freq_stats['Dup%'],
            'Uniq%': freq_stats['Uniq%'],
            'Dups_freq': freq_stats['Dups_freq'],
            'Uniq_freq': freq_stats['Uniq_freq'],
            'Dups_freq_5_max': freq_stats['Dups_freq_5_max'],
            'Uniq_freq_5_max': freq_stats['Uniq_freq_5_max'],
        })

    return results

# ---------- MAIN SCRIPT ----------
input_folder = r'/path/to/parquet/files'  # Replace with your path
parquet_files = glob(os.path.join(input_folder, '*.parquet'))

all_stats = []
for file in parquet_files:
    stats = analyze_parquet_file(file)
    if stats:
        all_stats.extend(stats)

df_stats = pd.DataFrame(all_stats)
df_stats.sort_values(by=['prd_src', 'schema', 'table'], inplace=True)

# Optional: Save to Excel or CSV
df_stats.to_excel('parquet_stats_report.xlsx', index=False)
# df_stats.to_csv('parquet_stats_report.csv', index=False)

print("Analysis complete. Report saved.")








from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, countDistinct, lit

# Initialize Spark
spark = SparkSession.builder.appName("ColumnStats").getOrCreate()

# Example DataFrame (replace this with your real DataFrame)
data = [
    (1, "A", None),
    (2, "B", "X"),
    (3, "B", "X"),
    (4, None, "Y"),
    (None, "A", "Z"),
    (2, "B", "X"),
]
columns = ["col1", "col2", "col3"]
df = spark.createDataFrame(data, columns)

# Total number of rows
total_rows = df.count()

# Loop through each column to compute stats
stats = []
for column in df.columns:
    null_count = df.filter(col(column).isNull()).count()
    unique_count = df.select(column).distinct().count()
    duplicate_count = total_rows - unique_count
    
    stats.append({
        "column": column,
        "null_%": round((null_count / total_rows) * 100, 2),
        "unique_%": round((unique_count / total_rows) * 100, 2),
        "duplicate_%": round((duplicate_count / total_rows) * 100, 2)
    })

# Convert to a DataFrame for display
stats_df = spark.createDataFrame(stats)
stats_df.show(truncate=False)







Control Filter Warning = 
VAR Selected = SELECTEDVALUE('ff'[Column2])
RETURN 
    SWITCH(
        TRUE(),
        ISBLANK(Selected), BLANK(),
        Selected = "Y", "‚ö†Ô∏è Warning: Filtering all dataset wtih QC ONLY",
        Selected = "N", "‚ö†Ô∏è Warning: Filtering all datasets without any QC",
        " "
    )






## Method 2: Using LOOKUPVALUE()

This method works regardless of active relationships:

```dax
Left Join Indicator = 
IF(
    ISBLANK(
        LOOKUPVALUE(
            Table2[PK_Check],
            Table2[PK_Check], Table1[PK_COMP]
        )
    ),
    "Not in Table2",
    "In Table2"
)
```

## Method 3: Using COUNTROWS() and FILTER()

For a more robust approach that handles duplicates:

```dax
Left Join Indicator = 
IF(
    COUNTROWS(
        FILTER(
            Table2,
            Table2[PK_Check] = Table1[PK_COMP]
        )
    ) = 0,
    "Not in Table2",
    "In Table2"
)


import datetime

def convert_datetime_objects(df):
    for col in df.columns:
        if df[col].apply(lambda x: isinstance(x, (datetime.date, datetime.time))).any():
            df[col] = df[col].astype(str)
    return df

df_stats = convert_datetime_objects(df_stats)






# Convert datetime.date columns to pandas datetime or string
for col in df_stats.select_dtypes(include=['object']).columns:
    if df_stats[col].apply(lambda x: isinstance(x, pd.Timestamp) or isinstance(x, pd.NaT)).any():
        continue
    if df_stats[col].apply(lambda x: isinstance(x, (datetime.date, datetime.time))).any():
        df_stats[col] = df_stats[col].astype(str)





import pandas as pd
import pyarrow.parquet as pq
import os
import re
from collections import Counter
from glob import glob

def get_special_char_count(series):
    if series.dtype != object:
        return 0
    pattern = re.compile(r'[^a-zA-Z0-9\s.,:;!?@#\$%\^\&\*\(\)_\-\+=]')
    return series.dropna().astype(str).apply(lambda x: len(pattern.findall(x))).sum()

def get_null_bucket(null_pct):
    if null_pct == 0:
        return "0%"
    elif null_pct < 10:
        return "<10%"
    elif null_pct < 50:
        return "10-50%"
    else:
        return ">50%"

def analyze_parquet_file(file_path):
    try:
        table = pq.read_table(file_path)
        df = table.to_pandas()
    except Exception as e:
        print(f"Failed to read {file_path}: {e}")
        return []

    # Extract table, schema, label from filename
    base_name = os.path.basename(file_path).replace('.parquet', '')
    parts = base_name.split('.')
    if len(parts) < 3:
        print(f"Filename format incorrect: {base_name}")
        return []
    table_label, schema, table_name = parts[-3:]

    total_rows = len(df)
    if total_rows == 0:
        return []

    prd_src = df['prd_src'].iloc[0] if 'prd_src' in df.columns else 'UNKNOWN'

    results = []
    for col in df.columns:
        if col == 'prd_src':
            continue

        series = df[col]
        null_count = series.isnull().sum()
        null_pct = (null_count / total_rows) * 100
        null_bucket = get_null_bucket(null_pct)
        unique_pct = (series.nunique(dropna=True) / total_rows) * 100
        duplicated_pct = ((total_rows - series.nunique(dropna=True)) / total_rows) * 100
        special_char_count = get_special_char_count(series)
        most_freq_val = series.mode().iloc[0] if not series.mode().empty else None

        results.append({
            'prd_src': prd_src,
            'column': col,
            'data_type': str(series.dtype),
            'null_count': null_count,
            'null_pct': round(null_pct, 2),
            'null_bucket': null_bucket,
            'unique_pct': round(unique_pct, 2),
            'duplicated_pct': round(duplicated_pct, 2),
            'special_char_count': special_char_count,
            'most_frequent_value': most_freq_val,
            'total_rows': total_rows,
            'schema': schema,
            'table_label': table_label,
            'table': table_name
        })

    return results

# ---------- MAIN SCRIPT ----------
input_folder = r'/path/to/parquet/files'  # Replace with your path
parquet_files = glob(os.path.join(input_folder, '*.parquet'))

all_stats = []
for file in parquet_files:
    stats = analyze_parquet_file(file)
    if stats:
        all_stats.extend(stats)

df_stats = pd.DataFrame(all_stats)
df_stats.sort_values(by=['prd_src', 'schema', 'table'], inplace=True)

# Optional: Save to Excel or CSV
df_stats.to_excel('parquet_stats_report.xlsx', index=False)
# df_stats.to_csv('parquet_stats_report.csv', index=False)

print("Analysis complete. Report saved.")










import pyarrow as pa
import pyarrow.flight as flight
import pandas as pd

def query_flight_server(host, port, username, password, dataset_name=None, sql_query=None):
    """
    Connect to Arrow Flight server and query data
    """
    
    # Connect to server
    uri = f"grpc://{host}:{port}"
    client = flight.connect(uri)
    
    # Authenticate
    client.authenticate_basic_token(username, password)
    print(f"Connected and authenticated to {uri}")
    
    # Query data
    if sql_query:
        # SQL query approach
        descriptor = flight.FlightDescriptor.for_command(sql_query.encode('utf-8'))
    elif dataset_name:
        # Dataset path approach
        descriptor = flight.FlightDescriptor.for_path(dataset_name)
    else:
        # List available datasets first
        print("Listing available datasets:")
        flights = list(client.list_flights())
        for i, flight_info in enumerate(flights):
            print(f"{i+1}. {flight_info.descriptor}")
        return None
    
    # Get flight info and data
    flight_info = client.get_flight_info(descriptor)
    ticket = flight_info.endpoints[0].ticket
    reader = client.do_get(ticket)
    table = reader.read_all()
    
    client.close()
    return table

# Usage
if __name__ == "__main__":
    # Your server details
    HOST = "your_host"
    PORT = 8080
    USERNAME = "your_username" 
    PASSWORD = "your_password"
    
    try:
        # Option 1: List datasets first
        print("Available datasets:")
        query_flight_server(HOST, PORT, USERNAME, PASSWORD)
        
        # Option 2: Query specific dataset
        # table = query_flight_server(HOST, PORT, USERNAME, PASSWORD, dataset_name="your_dataset")
        
        # Option 3: Run SQL query
        # table = query_flight_server(HOST, PORT, USERNAME, PASSWORD, sql_query="SELECT * FROM your_table LIMIT 10")
        
        # Process results
        # if table:
        #     df = table.to_pandas()
        #     print(f"Retrieved {len(df)} rows")
        #     print(df.head())
        
    except Exception as e:
        print(f"Error: {e}")









import pandas as pd
import re

# Sample data
data = {
    '1gg': [10],
    '2gg': [20],
    'pro': ['rpefj = ("1gg", \'2gg\')']
}
df = pd.DataFrame(data)

# Extract quoted items
def extract_items(value):
    if not isinstance(value, str):
        return []
    return re.findall(r"'([^']+)'|\"([^\"]+)\"", value)

# Apply extraction
df['extracted'] = df['pro'].apply(lambda x: [i[0] or i[1] for i in extract_items(x)])

# Explode into separate rows
df_exp = df.explode('extracted').reset_index(drop=True)

# Rename for clarity
df_exp = df_exp.rename(columns={'extracted': 'extracted_item'})

print(df_exp[['extracted_item']])





















def get_column_stats_by_group(df: pd.DataFrame, group_col: str = 'prod_data_src') -> pd.DataFrame:
    if group_col not in df.columns:
        raise ValueError(f"'{group_col}' column not found in dataframe")

    all_stats = []
    special_char_pattern = re.compile(r'[^a-zA-Z0-9\s.,:;!?@#\$%\^\&\*\(\)_\-\+=]')

    grouped = df.groupby(group_col)

    for group_value, group_df in grouped:
        total_rows = len(group_df)

        for col in group_df.columns:
            if col == group_col:
                continue  # Skip the group column itself in the stats

            null_count = group_df[col].isna().sum()
            unique_count = group_df[col].nunique(dropna=True)
            duplicate_count = group_df.duplicated(subset=[col]).sum()
            data_type = str(group_df[col].dtype)

            special_char_count = 0
            if group_df[col].dtype == 'object':
                series_str = group_df[col].dropna().astype(str)
                special_char_count = series_str.str.count(special_char_pattern).sum()

            try:
                most_freq = group_df[col].mode(dropna=True).iloc[0]
            except Exception:
                most_freq = None

            all_stats.append({
                'prod_data_src': group_value,
                'Column': col,
                'Data Type': data_type,
                'Total Rows': total_rows,
                'Null %': round((null_count / total_rows) * 100, 2) if total_rows > 0 else None,
                'Unique %': round((unique_count / total_rows) * 100, 2) if total_rows > 0 else None,
                'Duplicate %': round((duplicate_count / total_rows) * 100, 2) if total_rows > 0 else None,
                'Special Char Count': int(special_char_count),
                'Most Frequent Value': most_freq
            })

    return pd.DataFrame(all_stats)

stats_df = get_column_stats_by_group(df, group_col='prod_data_src')














import pandas as pd
import numpy as np
import pyodbc
import os
import re
from datetime import datetime
from tqdm import tqdm
import traceback

# ------------------ User Configuration ------------------

uid = open(r"C:\Users\YourUsername\gg\Shan.txt").read().strip()
pwd = 'your_password'  # Replace with your actual password

input_path = r'C:\Users\YourUsername\gg\schema_table_list.xlsx'
output_dir = r"C:\Users\YourUsername\gg\outputs"
as_of_dt = '2025-07-06'
sample_limit = 10000

# ------------------ Setup ------------------

os.makedirs(output_dir, exist_ok=True)

conn_str = (
    "DRIVER={Dremio Connector};"
    "ConnectionType=Direct;"
    "HOST=your.dremio.server.com;"
    "PORT=31010;"
    "AuthenticationType=Plain;"
    f"UID={uid};"
    f"PWD={pwd};"
    "SSL=1;"
    "SSLVerifyServer=1;"
    "UseSystemTrustStore=1;"
    "TLSMinVersion=TLSv1.2;"
)

conn = pyodbc.connect(conn_str, autocommit=True)

table_list_df = pd.read_excel(input_path)
required_cols = {'schema', 'table'}
if not required_cols.issubset(table_list_df.columns):
    raise ValueError(f"Excel must contain the following columns: {required_cols}")

# ------------------ Stats Function ------------------

def get_column_stats(df: pd.DataFrame) -> pd.DataFrame:
    stats = []
    total_rows = len(df)
    special_char_pattern = re.compile(r'[^a-zA-Z0-9\s.,:;!?@#\$%\^\&\*\(\)_\-\+=]')

    for col in df.columns:
        null_count = df[col].isna().sum()
        unique_count = df[col].nunique(dropna=True)
        duplicate_count = df.duplicated(subset=[col]).sum()
        data_type = str(df[col].dtype)

        special_char_count = 0
        if df[col].dtype == 'object':
            series_str = df[col].dropna().astype(str)
            special_char_count = series_str.str.count(special_char_pattern).sum()

        try:
            most_freq = df[col].mode(dropna=True).iloc[0]
        except Exception:
            most_freq = None

        stats.append({
            'Column': col,
            'Data Type': data_type,
            'Total Rows': total_rows,
            'Null %': round((null_count / total_rows) * 100, 2) if total_rows > 0 else None,
            'Unique %': round((unique_count / total_rows) * 100, 2) if total_rows > 0 else None,
            'Duplicate %': round((duplicate_count / total_rows) * 100, 2) if total_rows > 0 else None,
            'Special Char Count': int(special_char_count),
            'Most Frequent Value': most_freq
        })

    return pd.DataFrame(stats)

# ------------------ Main Loop ------------------

error_log_path = os.path.join(output_dir, 'error_log.txt')
error_log = []

today_str = datetime.now().strftime("%Y%m%d")

for idx, row in tqdm(table_list_df.iterrows(), total=len(table_list_df)):
    schema = row['schema']
    table = row['table']
    full_table = f'"hive"."{schema}"."{table}"'
    sheet_name = f"{schema}_{table}"[:31]

    try:
        query = (
            f"SELECT * FROM {full_table} "
            f"WHERE as_of_dt = '{as_of_dt}' "
            f"LIMIT {sample_limit}"
        )

        df = pd.read_sql(query, conn)

        if df.empty:
            msg = f"[WARNING] No rows in {full_table} for as_of_dt = '{as_of_dt}'"
            print(msg)
            error_log.append(msg)
            continue

        stats_df = get_column_stats(df)

        # Add schema and table names columns for joins
        stats_df['Schema'] = schema
        stats_df['Table'] = table

        output_filename = f"{schema}.{table}.{today_str}.xlsx"
        output_path = os.path.join(output_dir, output_filename)

        with pd.ExcelWriter(output_path, engine='xlsxwriter') as writer:
            stats_df.to_excel(writer, sheet_name=sheet_name, index=False)

        print(f"[SUCCESS] Output saved for {schema}.{table} at {output_path}")

    except Exception as e:
        err_msg = f"[ERROR] {schema}.{table} - {str(e)}"
        print(err_msg)
        traceback_text = traceback.format_exc()
        error_log.append(f"{err_msg}\n{traceback_text}\n")

if error_log:
    with open(error_log_path, 'w') as f:
        f.writelines(line + '\n' for line in error_log)
    print(f"\n‚ö†Ô∏è Errors encountered. See log at: {error_log_path}")



###########################################################











import pandas as pd
import numpy as np
import pyodbc
import os
import re
from datetime import datetime
from tqdm import tqdm
import traceback

# ------------------ User Configuration ------------------

uid = open(r"C:\Users\YourUsername\gg\Shan.txt").read().strip()
pwd = 'your_password'  # Replace with your actual password

input_path = r'C:\Users\YourUsername\gg\schema_table_list.xlsx'
output_dir = r"C:\Users\YourUsername\gg\outputs"
as_of_dt = '2025-07-06'
sample_limit = 10000

# ------------------ Setup ------------------

os.makedirs(output_dir, exist_ok=True)

conn_str = (
    "DRIVER={Dremio Connector};"
    "ConnectionType=Direct;"
    "HOST=your.dremio.server.com;"
    "PORT=31010;"
    "AuthenticationType=Plain;"
    f"UID={uid};"
    f"PWD={pwd};"
    "SSL=1;"
    "SSLVerifyServer=1;"
    "UseSystemTrustStore=1;"
    "TLSMinVersion=TLSv1.2;"
)

conn = pyodbc.connect(conn_str, autocommit=True)

table_list_df = pd.read_excel(input_path)
required_cols = {'schema', 'table'}
if not required_cols.issubset(table_list_df.columns):
    raise ValueError(f"Excel must contain the following columns: {required_cols}")

# ------------------ Stats Function ------------------

def get_column_stats(df: pd.DataFrame) -> pd.DataFrame:
    stats = []
    total_rows = len(df)
    special_char_pattern = re.compile(r'[^a-zA-Z0-9\s.,:;!?@#\$%\^\&\*\(\)_\-\+=]')

    for col in df.columns:
        null_count = df[col].isna().sum()
        unique_count = df[col].nunique(dropna=True)
        duplicate_count = df.duplicated(subset=[col]).sum()
        data_type = str(df[col].dtype)

        # Special character count (only for string-like columns)
        special_char_count = 0
        if df[col].dtype == 'object':
            series_str = df[col].dropna().astype(str)
            special_char_count = series_str.str.count(special_char_pattern).sum()

        # Most frequent value
        try:
            most_freq = df[col].mode(dropna=True).iloc[0]
        except Exception:
            most_freq = None

        stats.append({
            'Column': col,
            'Data Type': data_type,
            'Total Rows': total_rows,
            'Null %': round((null_count / total_rows) * 100, 2) if total_rows > 0 else None,
            'Unique %': round((unique_count / total_rows) * 100, 2) if total_rows > 0 else None,
            'Duplicate %': round((duplicate_count / total_rows) * 100, 2) if total_rows > 0 else None,
            'Special Char Count': int(special_char_count),
            'Most Frequent Value': most_freq
        })

    return pd.DataFrame(stats)

# ------------------ Main Loop ------------------

error_log_path = os.path.join(output_dir, 'error_log.txt')
error_log = []

today_str = datetime.now().strftime("%Y%m%d")

for idx, row in tqdm(table_list_df.iterrows(), total=len(table_list_df)):
    schema = row['schema']
    table = row['table']
    full_table = f'"hive"."{schema}"."{table}"'
    sheet_name = f"{schema}_{table}"[:31]

    try:
        query = (
            f"SELECT * FROM {full_table} "
            f"WHERE as_of_dt = '{as_of_dt}' "
            f"LIMIT {sample_limit}"
        )

        df = pd.read_sql(query, conn)

        if df.empty:
            msg = f"[WARNING] No rows in {full_table} for as_of_dt = '{as_of_dt}'"
            print(msg)
            error_log.append(msg)
            continue

        stats_df = get_column_stats(df)

        output_filename = f"{schema}.{table}.{today_str}.xlsx"
        output_path = os.path.join(output_dir, output_filename)

        with pd.ExcelWriter(output_path, engine='xlsxwriter') as writer:
            stats_df.to_excel(writer, sheet_name=sheet_name, index=False)

        print(f"[SUCCESS] Output saved for {schema}.{table} at {output_path}")

    except Exception as e:
        err_msg = f"[ERROR] {schema}.{table} - {str(e)}"
        print(err_msg)
        traceback_text = traceback.format_exc()
        error_log.append(f"{err_msg}\n{traceback_text}\n")

# Save error log if any
if error_log:
    with open(error_log_path, 'w') as f:
        f.writelines(line + '\n' for line in error_log)
    print(f"\n‚ö†Ô∏è Errors encountered. See log at: {error_log_path}")


































os.makedirs(output_dir, exist_ok=True)

# Connect to Dremio
conn = pyodbc.connect(conn_str, autocommit=True)

# Loop through all tables
for _, row in table_list_df.iterrows():
    schema = row['schema']
    table = row['table']
    full_path = f"hive.{schema}.{table}"
    print(f"\nüîç Profiling {full_path}...")

    # Safe WHERE clause
    where_clause = f"WHERE as_of_date = DATE '{as_of_date}'" if use_as_of_date else ""

    try:
        # Get column names
        meta_query = f"SELECT * FROM {full_path} {where_clause} LIMIT 0"
        df_empty = pd.read_sql(meta_query, conn)
        columns = df_empty.columns.tolist()

        # Get total rows
        row_count_query = f"SELECT COUNT(*) AS total_rows FROM {full_path} {where_clause}"
        total_rows_result = pd.read_sql(row_count_query, conn)
        total_rows = total_rows_result.iloc[0]['total_rows']

        print(f"üëâ Total rows: {total_rows}")

        # Stats per column
        stats = []
        for col in tqdm(columns, desc=f"{schema}.{table}"):
            try:
                stat_query = (
                    f"SELECT "
                    f"  COUNT(*) FILTER (WHERE {col} IS NULL) AS null_count, "
                    f"  COUNT(DISTINCT {col}) AS unique_count "
                    f"FROM {full_path} {where_clause}"
                )
                result = pd.read_sql(stat_query, conn).iloc[0]

                null_count = result.get('null_count') or 0
                unique_count = result.get('unique_count') or 0

                if total_rows == 0:
                    null_pct = unique_pct = duplicate_pct = 0
                else:
                    null_pct = (null_count / total_rows) * 100
                    unique_pct = (unique_count / total_rows) * 100
                    non_null = total_rows - null_count
                    duplicate_pct = ((non_null - unique_count) / total_rows) * 100

                stats.append({
                    'Column': col,
                    'Null %': round(null_pct, 2),
                    'Unique %': round(unique_pct, 2),
                    'Duplicate %': round(duplicate_pct, 2),
                    'Total Rows': total_rows
                })

            except Exception as col_err:
                stats.append({
                    'Column': col,
                    'Null %': 'Error',
                    'Unique %': 'Error',
                    'Duplicate %': 'Error',
                    'Total Rows': total_rows,
                    'Error': str(col_err)
                })

        # Save to Excel
        df_stats = pd.DataFrame(stats)
        output_path = os.path.join(output_dir, f"{schema}_{table}_profile.xlsx")
        df_stats.to_excel(output_path, index=False)
        print(f"‚úÖ Saved: {output_path}")

    except Exception as e:
        print(f"‚ùå Failed for {full_path}: {e}")

# Close connection
conn.close()
print("\nüéâ All done.")





























import pyodbc
import pandas as pd
from tqdm import tqdm
import os

# Read credentials
uid = open(r"C:\Users\YourUsername\gg\Shan.txt").read().strip()
pwd = 'your_password'  # Replace with your actual password

# Read Excel with schema/table list (must have columns: schema, table)
input_path = r'C:\Users\YourUsername\gg\schema_table_list.xlsx'
table_list_df = pd.read_excel(input_path)

# Optional: Only used if your table has an 'as_of_date' column
use_as_of_date = True
as_of_date = '2025-07-30'

# Dremio connection string
conn_str = (
    "DRIVER={Dremio Connector};"
    "ConnectionType=Direct;"
    "HOST=your.dremio.server.com;"
    "PORT=31010;"
    "AuthenticationType=Plain;"
    f"UID={uid};"
    f"PWD={pwd};"
    "SSL=1;"
    "SSLVerifyServer=1;"
    "UseSystemTrustStore=1;"
    "TLSMinVersion=TLSv1.2;"
)

# Output directory
output_dir = r"C:\Users\YourUsername\gg\outputs"
os.makedirs(output_dir, exist_ok=True)

# Connect to Dremio
conn = pyodbc.connect(conn_str, autocommit=True)

# Loop through schema/table combinations
for _, row in table_list_df.iterrows():
    schema = row['schema']
    table = row['table']
    full_path = f"hive.{schema}.{table}"
    print(f"\nüîç Profiling {full_path}...")

    # Build WHERE clause safely
    where_clause = f"WHERE as_of_date = DATE '{as_of_date}'" if use_as_of_date else ""

    try:
        # Step 1: Get column names
        meta_query = f"SELECT * FROM {full_path} {where_clause} LIMIT 0"
        df_empty = pd.read_sql(meta_query, conn)
        columns = df_empty.columns.tolist()

        # Step 2: Get total row count
        row_count_query = f"SELECT COUNT(*) AS total_rows FROM {full_path} {where_clause}"
        total_rows = pd.read_sql(row_count_query, conn).iloc[0]['total_rows']

        # Step 3: Collect stats per column
        stats = []
        for col in tqdm(columns, desc=f"{schema}.{table}"):
            try:
                stat_query = (
                    f"SELECT "
                    f"  COUNT(*) FILTER (WHERE {col} IS NULL) AS null_count, "
                    f"  COUNT(DISTINCT {col}) AS unique_count "
                    f"FROM {full_path} {where_clause}"
                )
                result = pd.read_sql(stat_query, conn).iloc[0]
                null_count = result['null_count']
                unique_count = result['unique_count']
                non_null = total_rows - null_count
                duplicate_pct = ((non_null - unique_count) / total_rows) * 100 if total_rows else 0

                stats.append({
                    'Column': col,
                    'Null %': round((null_count / total_rows) * 100, 2),
                    'Unique %': round((unique_count / total_rows) * 100, 2),
                    'Duplicate %': round(duplicate_pct, 2),
                    'Total Rows': total_rows
                })
            except Exception as col_err:
                stats.append({
                    'Column': col,
                    'Null %': 'Error',
                    'Unique %': 'Error',
                    'Duplicate %': 'Error',
                    'Total Rows': total_rows,
                    'Error': str(col_err)
                })

        # Step 4: Save to Excel
        df_stats = pd.DataFrame(stats)
        output_path = os.path.join(output_dir, f"{schema}_{table}_profile.xlsx")
        df_stats.to_excel(output_path, index=False)
        print(f"‚úÖ Saved: {output_path}")

    except Exception as e:
        print(f"‚ùå Failed for {full_path}: {e}")

# Close connection
conn.close()
print("\nüéâ All done.")






















import pyodbc
import pandas as pd
from tqdm import tqdm
import os

# Read credentials
uid = open(r"C:\Users\YourUsername\gg\Shan.txt").read().strip()
pwd = 'your_password'

# Read Excel with schema and table list
table_list_df = pd.read_excel(r'C:\Users\YourUsername\gg\schema_table_list.xlsx')  # <-- UPDATE THIS PATH
as_of_date = '2025-07-30'

# Dremio connection
conn_str = f"""
DRIVER={{Dremio Connector}};
ConnectionType=Direct;
HOST=your.dremio.server.com;
PORT=31010;
AuthenticationType=Plain;
UID={uid};
PWD={pwd};
SSL=1;
SSLVerifyServer=1;
UseSystemTrustStore=1;
TLSMinVersion=TLSv1.2;
"""

conn = pyodbc.connect(conn_str, autocommit=True)

# Output folder
output_dir = r"C:\Users\YourUsername\gg\outputs"
os.makedirs(output_dir, exist_ok=True)

# Loop through each schema/table
for _, row in table_list_df.iterrows():
    schema = row['schema']
    table = row['table']
    full_path = f"hive.{schema}.{table}"
    print(f"üîç Profiling {full_path}...")

    try:
        # Get column names
        meta_query = f"""
        SELECT * FROM {full_path}
        WHERE as_of_date = DATE '{as_of_date}'
        LIMIT 0
        """
        df_empty = pd.read_sql(meta_query, conn)
        columns = df_empty.columns.tolist()

        # Get total rows
        row_count_query = f"""
        SELECT COUNT(*) AS total_rows
        FROM {full_path}
        WHERE as_of_date = DATE '{as_of_date}'
        """
        total_rows = pd.read_sql(row_count_query, conn).iloc[0]['total_rows']

        # Stats
        stats = []
        for col in tqdm(columns, desc=f"{schema}.{table}"):
            stat_query = f"""
            SELECT
                COUNT(*) FILTER (WHERE {col} IS NULL) AS null_count,
                COUNT(DISTINCT {col}) AS unique_count
            FROM {full_path}
            WHERE as_of_date = DATE '{as_of_date}'
            """
            try:
                result = pd.read_sql(stat_query, conn).iloc[0]
                null_count = result['null_count']
                unique_count = result['unique_count']
                non_null = total_rows - null_count
                duplicate_pct = ((non_null - unique_count) / total_rows) * 100 if total_rows else 0

                stats.append({
                    'Column': col,
                    'Null %': round((null_count / total_rows) * 100, 2),
                    'Unique %': round((unique_count / total_rows) * 100, 2),
                    'Duplicate %': round(duplicate_pct, 2),
                    'Total Rows': total_rows
                })
            except Exception as col_err:
                stats.append({
                    'Column': col,
                    'Null %': 'Error',
                    'Unique %': 'Error',
                    'Duplicate %': 'Error',
                    'Error': str(col_err)
                })

        # Save result
        df_stats = pd.DataFrame(stats)
        output_path = os.path.join(output_dir, f"{schema}_{table}_profile.xlsx")
        df_stats.to_excel(output_path, index=False)
        print(f"‚úÖ Saved: {output_path}")

    except Exception as e:
        print(f"‚ùå Failed for {full_path}: {e}")

conn.close()
print("\nüéâ All done.")








################################
################################


import pyodbc
import pandas as pd
from tqdm import tqdm

uid = open(r"C:\Users\YourUsername\gg\Shan.txt").read().strip()
pwd = 'your_password'

schema = 'hive.dsfsdf'
table = 'sfdsdffsdf'
as_of_date = '2025-07-30'  # Optional

conn_str = f"""
DRIVER={{Dremio Connector}};
ConnectionType=Direct;
HOST=your.dremio.server.com;
PORT=31010;
AuthenticationType=Plain;
UID={uid};
PWD={pwd};
SSL=1;
SSLVerifyServer=1;
UseSystemTrustStore=1;
TLSMinVersion=TLSv1.2;
"""

conn = pyodbc.connect(conn_str, autocommit=True)

# Get column list dynamically (zero-row fetch)
meta_query = f"""
SELECT *
FROM {schema}.{table}
WHERE as_of_date = DATE '{as_of_date}'
LIMIT 0
"""

df_empty = pd.read_sql(meta_query, conn)
columns = df_empty.columns.tolist()

# Total rows
row_count_query = f"""
SELECT COUNT(*) AS total_rows
FROM {schema}.{table}
WHERE as_of_date = DATE '{as_of_date}'
"""
total_rows = pd.read_sql(row_count_query, conn).iloc[0]['total_rows']

# Stats per column
stats = []

for col in tqdm(columns, desc="Analyzing columns"):
    query = f"""
    SELECT
        COUNT(*) FILTER (WHERE {col} IS NULL) AS null_count,
        COUNT(DISTINCT {col}) AS unique_count
    FROM {schema}.{table}
    WHERE as_of_date = DATE '{as_of_date}'
    """
    try:
        result = pd.read_sql(query, conn).iloc[0]
        null_count = result['null_count']
        unique_count = result['unique_count']
        non_null = total_rows - null_count
        duplicate_pct = ((non_null - unique_count) / total_rows) * 100 if total_rows else 0

        stats.append({
            'Column': col,
            'Null %': round((null_count / total_rows) * 100, 2),
            'Unique %': round((unique_count / total_rows) * 100, 2),
            'Duplicate %': round(duplicate_pct, 2)
        })
    except Exception as e:
        stats.append({'Column': col, 'Null %': 'Error', 'Unique %': 'Error', 'Duplicate %': 'Error', 'Error': str(e)})

df_stats = pd.DataFrame(stats)
df_stats.to_excel(r'C:\Users\YourUsername\gg\profile_output.xlsx', index=False)
print("‚úÖ Done. Excel written.")

conn.close()
