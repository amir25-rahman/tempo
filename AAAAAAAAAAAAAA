import pandas as pd
import re

# Sample data
data = {
    '1gg': [10],
    '2gg': [20],
    'pro': ['rpefj = ("1gg", \'2gg\')']
}
df = pd.DataFrame(data)

# Extract quoted items
def extract_items(value):
    if not isinstance(value, str):
        return []
    return re.findall(r"'([^']+)'|\"([^\"]+)\"", value)

# Apply extraction
df['extracted'] = df['pro'].apply(lambda x: [i[0] or i[1] for i in extract_items(x)])

# Explode into separate rows
df_exp = df.explode('extracted').reset_index(drop=True)

# Rename for clarity
df_exp = df_exp.rename(columns={'extracted': 'extracted_item'})

print(df_exp[['extracted_item']])





















def get_column_stats_by_group(df: pd.DataFrame, group_col: str = 'prod_data_src') -> pd.DataFrame:
    if group_col not in df.columns:
        raise ValueError(f"'{group_col}' column not found in dataframe")

    all_stats = []
    special_char_pattern = re.compile(r'[^a-zA-Z0-9\s.,:;!?@#\$%\^\&\*\(\)_\-\+=]')

    grouped = df.groupby(group_col)

    for group_value, group_df in grouped:
        total_rows = len(group_df)

        for col in group_df.columns:
            if col == group_col:
                continue  # Skip the group column itself in the stats

            null_count = group_df[col].isna().sum()
            unique_count = group_df[col].nunique(dropna=True)
            duplicate_count = group_df.duplicated(subset=[col]).sum()
            data_type = str(group_df[col].dtype)

            special_char_count = 0
            if group_df[col].dtype == 'object':
                series_str = group_df[col].dropna().astype(str)
                special_char_count = series_str.str.count(special_char_pattern).sum()

            try:
                most_freq = group_df[col].mode(dropna=True).iloc[0]
            except Exception:
                most_freq = None

            all_stats.append({
                'prod_data_src': group_value,
                'Column': col,
                'Data Type': data_type,
                'Total Rows': total_rows,
                'Null %': round((null_count / total_rows) * 100, 2) if total_rows > 0 else None,
                'Unique %': round((unique_count / total_rows) * 100, 2) if total_rows > 0 else None,
                'Duplicate %': round((duplicate_count / total_rows) * 100, 2) if total_rows > 0 else None,
                'Special Char Count': int(special_char_count),
                'Most Frequent Value': most_freq
            })

    return pd.DataFrame(all_stats)

stats_df = get_column_stats_by_group(df, group_col='prod_data_src')














import pandas as pd
import numpy as np
import pyodbc
import os
import re
from datetime import datetime
from tqdm import tqdm
import traceback

# ------------------ User Configuration ------------------

uid = open(r"C:\Users\YourUsername\gg\Shan.txt").read().strip()
pwd = 'your_password'  # Replace with your actual password

input_path = r'C:\Users\YourUsername\gg\schema_table_list.xlsx'
output_dir = r"C:\Users\YourUsername\gg\outputs"
as_of_dt = '2025-07-06'
sample_limit = 10000

# ------------------ Setup ------------------

os.makedirs(output_dir, exist_ok=True)

conn_str = (
    "DRIVER={Dremio Connector};"
    "ConnectionType=Direct;"
    "HOST=your.dremio.server.com;"
    "PORT=31010;"
    "AuthenticationType=Plain;"
    f"UID={uid};"
    f"PWD={pwd};"
    "SSL=1;"
    "SSLVerifyServer=1;"
    "UseSystemTrustStore=1;"
    "TLSMinVersion=TLSv1.2;"
)

conn = pyodbc.connect(conn_str, autocommit=True)

table_list_df = pd.read_excel(input_path)
required_cols = {'schema', 'table'}
if not required_cols.issubset(table_list_df.columns):
    raise ValueError(f"Excel must contain the following columns: {required_cols}")

# ------------------ Stats Function ------------------

def get_column_stats(df: pd.DataFrame) -> pd.DataFrame:
    stats = []
    total_rows = len(df)
    special_char_pattern = re.compile(r'[^a-zA-Z0-9\s.,:;!?@#\$%\^\&\*\(\)_\-\+=]')

    for col in df.columns:
        null_count = df[col].isna().sum()
        unique_count = df[col].nunique(dropna=True)
        duplicate_count = df.duplicated(subset=[col]).sum()
        data_type = str(df[col].dtype)

        special_char_count = 0
        if df[col].dtype == 'object':
            series_str = df[col].dropna().astype(str)
            special_char_count = series_str.str.count(special_char_pattern).sum()

        try:
            most_freq = df[col].mode(dropna=True).iloc[0]
        except Exception:
            most_freq = None

        stats.append({
            'Column': col,
            'Data Type': data_type,
            'Total Rows': total_rows,
            'Null %': round((null_count / total_rows) * 100, 2) if total_rows > 0 else None,
            'Unique %': round((unique_count / total_rows) * 100, 2) if total_rows > 0 else None,
            'Duplicate %': round((duplicate_count / total_rows) * 100, 2) if total_rows > 0 else None,
            'Special Char Count': int(special_char_count),
            'Most Frequent Value': most_freq
        })

    return pd.DataFrame(stats)

# ------------------ Main Loop ------------------

error_log_path = os.path.join(output_dir, 'error_log.txt')
error_log = []

today_str = datetime.now().strftime("%Y%m%d")

for idx, row in tqdm(table_list_df.iterrows(), total=len(table_list_df)):
    schema = row['schema']
    table = row['table']
    full_table = f'"hive"."{schema}"."{table}"'
    sheet_name = f"{schema}_{table}"[:31]

    try:
        query = (
            f"SELECT * FROM {full_table} "
            f"WHERE as_of_dt = '{as_of_dt}' "
            f"LIMIT {sample_limit}"
        )

        df = pd.read_sql(query, conn)

        if df.empty:
            msg = f"[WARNING] No rows in {full_table} for as_of_dt = '{as_of_dt}'"
            print(msg)
            error_log.append(msg)
            continue

        stats_df = get_column_stats(df)

        # Add schema and table names columns for joins
        stats_df['Schema'] = schema
        stats_df['Table'] = table

        output_filename = f"{schema}.{table}.{today_str}.xlsx"
        output_path = os.path.join(output_dir, output_filename)

        with pd.ExcelWriter(output_path, engine='xlsxwriter') as writer:
            stats_df.to_excel(writer, sheet_name=sheet_name, index=False)

        print(f"[SUCCESS] Output saved for {schema}.{table} at {output_path}")

    except Exception as e:
        err_msg = f"[ERROR] {schema}.{table} - {str(e)}"
        print(err_msg)
        traceback_text = traceback.format_exc()
        error_log.append(f"{err_msg}\n{traceback_text}\n")

if error_log:
    with open(error_log_path, 'w') as f:
        f.writelines(line + '\n' for line in error_log)
    print(f"\n⚠️ Errors encountered. See log at: {error_log_path}")



###########################################################











import pandas as pd
import numpy as np
import pyodbc
import os
import re
from datetime import datetime
from tqdm import tqdm
import traceback

# ------------------ User Configuration ------------------

uid = open(r"C:\Users\YourUsername\gg\Shan.txt").read().strip()
pwd = 'your_password'  # Replace with your actual password

input_path = r'C:\Users\YourUsername\gg\schema_table_list.xlsx'
output_dir = r"C:\Users\YourUsername\gg\outputs"
as_of_dt = '2025-07-06'
sample_limit = 10000

# ------------------ Setup ------------------

os.makedirs(output_dir, exist_ok=True)

conn_str = (
    "DRIVER={Dremio Connector};"
    "ConnectionType=Direct;"
    "HOST=your.dremio.server.com;"
    "PORT=31010;"
    "AuthenticationType=Plain;"
    f"UID={uid};"
    f"PWD={pwd};"
    "SSL=1;"
    "SSLVerifyServer=1;"
    "UseSystemTrustStore=1;"
    "TLSMinVersion=TLSv1.2;"
)

conn = pyodbc.connect(conn_str, autocommit=True)

table_list_df = pd.read_excel(input_path)
required_cols = {'schema', 'table'}
if not required_cols.issubset(table_list_df.columns):
    raise ValueError(f"Excel must contain the following columns: {required_cols}")

# ------------------ Stats Function ------------------

def get_column_stats(df: pd.DataFrame) -> pd.DataFrame:
    stats = []
    total_rows = len(df)
    special_char_pattern = re.compile(r'[^a-zA-Z0-9\s.,:;!?@#\$%\^\&\*\(\)_\-\+=]')

    for col in df.columns:
        null_count = df[col].isna().sum()
        unique_count = df[col].nunique(dropna=True)
        duplicate_count = df.duplicated(subset=[col]).sum()
        data_type = str(df[col].dtype)

        # Special character count (only for string-like columns)
        special_char_count = 0
        if df[col].dtype == 'object':
            series_str = df[col].dropna().astype(str)
            special_char_count = series_str.str.count(special_char_pattern).sum()

        # Most frequent value
        try:
            most_freq = df[col].mode(dropna=True).iloc[0]
        except Exception:
            most_freq = None

        stats.append({
            'Column': col,
            'Data Type': data_type,
            'Total Rows': total_rows,
            'Null %': round((null_count / total_rows) * 100, 2) if total_rows > 0 else None,
            'Unique %': round((unique_count / total_rows) * 100, 2) if total_rows > 0 else None,
            'Duplicate %': round((duplicate_count / total_rows) * 100, 2) if total_rows > 0 else None,
            'Special Char Count': int(special_char_count),
            'Most Frequent Value': most_freq
        })

    return pd.DataFrame(stats)

# ------------------ Main Loop ------------------

error_log_path = os.path.join(output_dir, 'error_log.txt')
error_log = []

today_str = datetime.now().strftime("%Y%m%d")

for idx, row in tqdm(table_list_df.iterrows(), total=len(table_list_df)):
    schema = row['schema']
    table = row['table']
    full_table = f'"hive"."{schema}"."{table}"'
    sheet_name = f"{schema}_{table}"[:31]

    try:
        query = (
            f"SELECT * FROM {full_table} "
            f"WHERE as_of_dt = '{as_of_dt}' "
            f"LIMIT {sample_limit}"
        )

        df = pd.read_sql(query, conn)

        if df.empty:
            msg = f"[WARNING] No rows in {full_table} for as_of_dt = '{as_of_dt}'"
            print(msg)
            error_log.append(msg)
            continue

        stats_df = get_column_stats(df)

        output_filename = f"{schema}.{table}.{today_str}.xlsx"
        output_path = os.path.join(output_dir, output_filename)

        with pd.ExcelWriter(output_path, engine='xlsxwriter') as writer:
            stats_df.to_excel(writer, sheet_name=sheet_name, index=False)

        print(f"[SUCCESS] Output saved for {schema}.{table} at {output_path}")

    except Exception as e:
        err_msg = f"[ERROR] {schema}.{table} - {str(e)}"
        print(err_msg)
        traceback_text = traceback.format_exc()
        error_log.append(f"{err_msg}\n{traceback_text}\n")

# Save error log if any
if error_log:
    with open(error_log_path, 'w') as f:
        f.writelines(line + '\n' for line in error_log)
    print(f"\n⚠️ Errors encountered. See log at: {error_log_path}")


































os.makedirs(output_dir, exist_ok=True)

# Connect to Dremio
conn = pyodbc.connect(conn_str, autocommit=True)

# Loop through all tables
for _, row in table_list_df.iterrows():
    schema = row['schema']
    table = row['table']
    full_path = f"hive.{schema}.{table}"
    print(f"\n🔍 Profiling {full_path}...")

    # Safe WHERE clause
    where_clause = f"WHERE as_of_date = DATE '{as_of_date}'" if use_as_of_date else ""

    try:
        # Get column names
        meta_query = f"SELECT * FROM {full_path} {where_clause} LIMIT 0"
        df_empty = pd.read_sql(meta_query, conn)
        columns = df_empty.columns.tolist()

        # Get total rows
        row_count_query = f"SELECT COUNT(*) AS total_rows FROM {full_path} {where_clause}"
        total_rows_result = pd.read_sql(row_count_query, conn)
        total_rows = total_rows_result.iloc[0]['total_rows']

        print(f"👉 Total rows: {total_rows}")

        # Stats per column
        stats = []
        for col in tqdm(columns, desc=f"{schema}.{table}"):
            try:
                stat_query = (
                    f"SELECT "
                    f"  COUNT(*) FILTER (WHERE {col} IS NULL) AS null_count, "
                    f"  COUNT(DISTINCT {col}) AS unique_count "
                    f"FROM {full_path} {where_clause}"
                )
                result = pd.read_sql(stat_query, conn).iloc[0]

                null_count = result.get('null_count') or 0
                unique_count = result.get('unique_count') or 0

                if total_rows == 0:
                    null_pct = unique_pct = duplicate_pct = 0
                else:
                    null_pct = (null_count / total_rows) * 100
                    unique_pct = (unique_count / total_rows) * 100
                    non_null = total_rows - null_count
                    duplicate_pct = ((non_null - unique_count) / total_rows) * 100

                stats.append({
                    'Column': col,
                    'Null %': round(null_pct, 2),
                    'Unique %': round(unique_pct, 2),
                    'Duplicate %': round(duplicate_pct, 2),
                    'Total Rows': total_rows
                })

            except Exception as col_err:
                stats.append({
                    'Column': col,
                    'Null %': 'Error',
                    'Unique %': 'Error',
                    'Duplicate %': 'Error',
                    'Total Rows': total_rows,
                    'Error': str(col_err)
                })

        # Save to Excel
        df_stats = pd.DataFrame(stats)
        output_path = os.path.join(output_dir, f"{schema}_{table}_profile.xlsx")
        df_stats.to_excel(output_path, index=False)
        print(f"✅ Saved: {output_path}")

    except Exception as e:
        print(f"❌ Failed for {full_path}: {e}")

# Close connection
conn.close()
print("\n🎉 All done.")





























import pyodbc
import pandas as pd
from tqdm import tqdm
import os

# Read credentials
uid = open(r"C:\Users\YourUsername\gg\Shan.txt").read().strip()
pwd = 'your_password'  # Replace with your actual password

# Read Excel with schema/table list (must have columns: schema, table)
input_path = r'C:\Users\YourUsername\gg\schema_table_list.xlsx'
table_list_df = pd.read_excel(input_path)

# Optional: Only used if your table has an 'as_of_date' column
use_as_of_date = True
as_of_date = '2025-07-30'

# Dremio connection string
conn_str = (
    "DRIVER={Dremio Connector};"
    "ConnectionType=Direct;"
    "HOST=your.dremio.server.com;"
    "PORT=31010;"
    "AuthenticationType=Plain;"
    f"UID={uid};"
    f"PWD={pwd};"
    "SSL=1;"
    "SSLVerifyServer=1;"
    "UseSystemTrustStore=1;"
    "TLSMinVersion=TLSv1.2;"
)

# Output directory
output_dir = r"C:\Users\YourUsername\gg\outputs"
os.makedirs(output_dir, exist_ok=True)

# Connect to Dremio
conn = pyodbc.connect(conn_str, autocommit=True)

# Loop through schema/table combinations
for _, row in table_list_df.iterrows():
    schema = row['schema']
    table = row['table']
    full_path = f"hive.{schema}.{table}"
    print(f"\n🔍 Profiling {full_path}...")

    # Build WHERE clause safely
    where_clause = f"WHERE as_of_date = DATE '{as_of_date}'" if use_as_of_date else ""

    try:
        # Step 1: Get column names
        meta_query = f"SELECT * FROM {full_path} {where_clause} LIMIT 0"
        df_empty = pd.read_sql(meta_query, conn)
        columns = df_empty.columns.tolist()

        # Step 2: Get total row count
        row_count_query = f"SELECT COUNT(*) AS total_rows FROM {full_path} {where_clause}"
        total_rows = pd.read_sql(row_count_query, conn).iloc[0]['total_rows']

        # Step 3: Collect stats per column
        stats = []
        for col in tqdm(columns, desc=f"{schema}.{table}"):
            try:
                stat_query = (
                    f"SELECT "
                    f"  COUNT(*) FILTER (WHERE {col} IS NULL) AS null_count, "
                    f"  COUNT(DISTINCT {col}) AS unique_count "
                    f"FROM {full_path} {where_clause}"
                )
                result = pd.read_sql(stat_query, conn).iloc[0]
                null_count = result['null_count']
                unique_count = result['unique_count']
                non_null = total_rows - null_count
                duplicate_pct = ((non_null - unique_count) / total_rows) * 100 if total_rows else 0

                stats.append({
                    'Column': col,
                    'Null %': round((null_count / total_rows) * 100, 2),
                    'Unique %': round((unique_count / total_rows) * 100, 2),
                    'Duplicate %': round(duplicate_pct, 2),
                    'Total Rows': total_rows
                })
            except Exception as col_err:
                stats.append({
                    'Column': col,
                    'Null %': 'Error',
                    'Unique %': 'Error',
                    'Duplicate %': 'Error',
                    'Total Rows': total_rows,
                    'Error': str(col_err)
                })

        # Step 4: Save to Excel
        df_stats = pd.DataFrame(stats)
        output_path = os.path.join(output_dir, f"{schema}_{table}_profile.xlsx")
        df_stats.to_excel(output_path, index=False)
        print(f"✅ Saved: {output_path}")

    except Exception as e:
        print(f"❌ Failed for {full_path}: {e}")

# Close connection
conn.close()
print("\n🎉 All done.")






















import pyodbc
import pandas as pd
from tqdm import tqdm
import os

# Read credentials
uid = open(r"C:\Users\YourUsername\gg\Shan.txt").read().strip()
pwd = 'your_password'

# Read Excel with schema and table list
table_list_df = pd.read_excel(r'C:\Users\YourUsername\gg\schema_table_list.xlsx')  # <-- UPDATE THIS PATH
as_of_date = '2025-07-30'

# Dremio connection
conn_str = f"""
DRIVER={{Dremio Connector}};
ConnectionType=Direct;
HOST=your.dremio.server.com;
PORT=31010;
AuthenticationType=Plain;
UID={uid};
PWD={pwd};
SSL=1;
SSLVerifyServer=1;
UseSystemTrustStore=1;
TLSMinVersion=TLSv1.2;
"""

conn = pyodbc.connect(conn_str, autocommit=True)

# Output folder
output_dir = r"C:\Users\YourUsername\gg\outputs"
os.makedirs(output_dir, exist_ok=True)

# Loop through each schema/table
for _, row in table_list_df.iterrows():
    schema = row['schema']
    table = row['table']
    full_path = f"hive.{schema}.{table}"
    print(f"🔍 Profiling {full_path}...")

    try:
        # Get column names
        meta_query = f"""
        SELECT * FROM {full_path}
        WHERE as_of_date = DATE '{as_of_date}'
        LIMIT 0
        """
        df_empty = pd.read_sql(meta_query, conn)
        columns = df_empty.columns.tolist()

        # Get total rows
        row_count_query = f"""
        SELECT COUNT(*) AS total_rows
        FROM {full_path}
        WHERE as_of_date = DATE '{as_of_date}'
        """
        total_rows = pd.read_sql(row_count_query, conn).iloc[0]['total_rows']

        # Stats
        stats = []
        for col in tqdm(columns, desc=f"{schema}.{table}"):
            stat_query = f"""
            SELECT
                COUNT(*) FILTER (WHERE {col} IS NULL) AS null_count,
                COUNT(DISTINCT {col}) AS unique_count
            FROM {full_path}
            WHERE as_of_date = DATE '{as_of_date}'
            """
            try:
                result = pd.read_sql(stat_query, conn).iloc[0]
                null_count = result['null_count']
                unique_count = result['unique_count']
                non_null = total_rows - null_count
                duplicate_pct = ((non_null - unique_count) / total_rows) * 100 if total_rows else 0

                stats.append({
                    'Column': col,
                    'Null %': round((null_count / total_rows) * 100, 2),
                    'Unique %': round((unique_count / total_rows) * 100, 2),
                    'Duplicate %': round(duplicate_pct, 2),
                    'Total Rows': total_rows
                })
            except Exception as col_err:
                stats.append({
                    'Column': col,
                    'Null %': 'Error',
                    'Unique %': 'Error',
                    'Duplicate %': 'Error',
                    'Error': str(col_err)
                })

        # Save result
        df_stats = pd.DataFrame(stats)
        output_path = os.path.join(output_dir, f"{schema}_{table}_profile.xlsx")
        df_stats.to_excel(output_path, index=False)
        print(f"✅ Saved: {output_path}")

    except Exception as e:
        print(f"❌ Failed for {full_path}: {e}")

conn.close()
print("\n🎉 All done.")








################################
################################


import pyodbc
import pandas as pd
from tqdm import tqdm

uid = open(r"C:\Users\YourUsername\gg\Shan.txt").read().strip()
pwd = 'your_password'

schema = 'hive.dsfsdf'
table = 'sfdsdffsdf'
as_of_date = '2025-07-30'  # Optional

conn_str = f"""
DRIVER={{Dremio Connector}};
ConnectionType=Direct;
HOST=your.dremio.server.com;
PORT=31010;
AuthenticationType=Plain;
UID={uid};
PWD={pwd};
SSL=1;
SSLVerifyServer=1;
UseSystemTrustStore=1;
TLSMinVersion=TLSv1.2;
"""

conn = pyodbc.connect(conn_str, autocommit=True)

# Get column list dynamically (zero-row fetch)
meta_query = f"""
SELECT *
FROM {schema}.{table}
WHERE as_of_date = DATE '{as_of_date}'
LIMIT 0
"""

df_empty = pd.read_sql(meta_query, conn)
columns = df_empty.columns.tolist()

# Total rows
row_count_query = f"""
SELECT COUNT(*) AS total_rows
FROM {schema}.{table}
WHERE as_of_date = DATE '{as_of_date}'
"""
total_rows = pd.read_sql(row_count_query, conn).iloc[0]['total_rows']

# Stats per column
stats = []

for col in tqdm(columns, desc="Analyzing columns"):
    query = f"""
    SELECT
        COUNT(*) FILTER (WHERE {col} IS NULL) AS null_count,
        COUNT(DISTINCT {col}) AS unique_count
    FROM {schema}.{table}
    WHERE as_of_date = DATE '{as_of_date}'
    """
    try:
        result = pd.read_sql(query, conn).iloc[0]
        null_count = result['null_count']
        unique_count = result['unique_count']
        non_null = total_rows - null_count
        duplicate_pct = ((non_null - unique_count) / total_rows) * 100 if total_rows else 0

        stats.append({
            'Column': col,
            'Null %': round((null_count / total_rows) * 100, 2),
            'Unique %': round((unique_count / total_rows) * 100, 2),
            'Duplicate %': round(duplicate_pct, 2)
        })
    except Exception as e:
        stats.append({'Column': col, 'Null %': 'Error', 'Unique %': 'Error', 'Duplicate %': 'Error', 'Error': str(e)})

df_stats = pd.DataFrame(stats)
df_stats.to_excel(r'C:\Users\YourUsername\gg\profile_output.xlsx', index=False)
print("✅ Done. Excel written.")

conn.close()
